title,author,content,url,crawl_time
探秘Transformer系列之（1）：注意力机制,未知作者,"因为各种事情，好久没有写博客了，之前写得一些草稿也没有时间整理（都没有时间登录博客和微信，导致最近才发现好多未读消息和私信，在这里和各位朋友说下万分抱歉）。现在恢复更新，是因为最近有些从非AI领域转过来的新同学来找我询问是否有比较好的学习资料，他们希望在短期内迅速上手 Transformer。我在网上找了下，但是没有找到非常合适的系统的学习资料，于是就萌发了自己写一个系列的想法，遂有此系列。在整理过程中，我也发现了自己很多似是而非的错误理解，因此这个系列也是自己一个整理、学习和提高的过程。 本文为系列第一篇，主要目的是引入Transformer概念和其相关背景。在2017年，Google Brain的Vaswani等人在论文”Attention is All You Need“中发布了Transformer。原始论文中给出Transformer的定义如下： Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution。 其中提到了sequence，RNN，convolution，self-attention等概念，所以我们接下来就从这些概念入手进行分析。我们先开始从Seq2Seq介绍，然后逐渐切换到注意力机制，最后再导出Transformer模型架构。 seq2seq（Sequence to Sequence/序列到序列）概念最早由Bengio在2014年的论文“Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation”中提出，其代表从一个源序列生成一个目标序列的操作。因为机器翻译是大家较熟悉且容易理解的领域，因此后续我们主要使用机器翻译来进行讲解，避免引入过多概念。 机器翻译其实就是文本生成。语言模型将文本看作是时间序列。在此视角下，每个单词都和它之前的单词相关，通过学习前面单词序列的统计规律就可以预测下一个单词。因此，机器翻译会从概率角度对语言建模，让新预测的单词和之前单词连成整个句子后最合理，即原有句子加上新预测单词后，成为整个句子的概率最大。这就涉及到自回归模型。 自回归（Autoregressive）模型是一种生成模型，其语言建模目标是根据给定的上下文来预测下一个单词。遵循因果原则（当前单词只受到其前面单词的影响），自回归模型的核心思想是利用一个变量的历史值来预测其未来的值，其将""序列数据的生成""建模为一个逐步预测每个新元素的条件概率的过程。在每个时间步，模型根据之前生成的元素预测当前元素的概率分布。 下图给出了自回归模型的示例。模型每次推理只会预测输出一个 token，当前轮输出token 与历史输入 token 拼接，作为下一轮的输入 token，这样逐次生成后面的预测token，直到新输出一个结束符号或者句子长度达到预设的最大阈值。就下图来说，模型执行序列如下： 将预测出的第一个单词“wear”结合原输入一起提供给模型，即第二次模型的输入是“You should wear”。 将预测出的第二个单词“shoes”结合原输入一起提供给模型，即第三次模型的输入是“You should wear shoes”。 该过程中的每一步预测都需要依赖上一步预测的结果，且从第二轮开始，前后两轮的输入只相差一个 token。 隐变量模型是一种引入隐变量来表示过去信息的模型。自回归模型在预测时会把过去观测到的信息总结起来记作\(h_t\)，并且更新预测\(x_t\)。即总结\(h_t = g(h_{t-1}, x_{t-1})\)，然后基于\(x_t = P(x_t \mid h_{t})\)来估计\(x_t\)。由于\(h_t\)从未被观测到，因此\(h_t\)就是隐变量，这类模型也被称为隐变量自回归模型（latent autoregressive models）。 有了\(ℎ_t\)之后，其实预测变换为两个子问题。一个问题是如何根据之前的隐变量 \(ℎ_{t−1}\) 和之前的输入信息\(x_{t-1}\)得到现在的隐变量\(ℎ_t\)，另一个问题是如何根据当前的隐变量\(ℎ_t\)和之前的输入\(x_{t-1}\)得到当前的\(x_t\) 。其实，这就是编码器-解码器模型要面对的问题。 目前，处理序列转换的神经网络模型大多是编码器-解码器（Encoder-Decoder）模型。传统的RNN架构仅适用于输入和输出等长的任务。然而，大多数情况下，机器翻译的输出和输入都不是等长的，因此，对于输入输出都是变长的序列，研究人员决定使用一个定长的状态机来作为输入和输出之间的桥梁。于是人们使用了一种新的架构：前半部分的RNN只有输入，后半部分的RNN只有输出（上一轮的输出会当作下一轮的输入以补充信息），两个部分通过一个隐状态（hidden state）来传递信息。把隐状态看成对输入信息的一种编码的话，前半部分可以叫做编码器（Encoder），后半部分可以叫做解码器（Decoder）。这种架构因而被称为编码器-解码器架构，所用到的模型就是编码器-解码器模型，具体如下图所示，图中编码器和解码器通过一个中间隐状态C来完成信息交互。 如何压缩？人们很容易想到马尔可夫假设，即系统的未来状态只与当前状态有关。这也被称为近因效应：从文本生成角度看，就是当前词只与距离它比较近的个词更加相关。如果考虑前面n个单词，这就得到了N-gram模型，即当前单词的概率取决于前n个单词。但是基于马尔科夫假设的模型难以处理句子中的长距离依赖关系（某个单词依赖句子中很早的单词），也没有考虑深层语义关系。而且N-gram模型的大小几乎是n的指数倍，如果n数目太大，则n个单词组合数可能性太多，对资源消耗过大，因此需要新的模型。新模型应该不仅仅简单的关注单词的频率和顺序，还可以考虑更长距离的依赖关系，却又不必显式考虑那么多单词组合可能性，于是人们想到使用神经网络来拟合。 MLP 是最基本的神经网络模型之一，它可以将词向量序列映射到一个固定长度的向量表示，然后将这个向量输入到一个softmax层中，计算出下一个词的概率分布。MLP虽然理论上没有距离长依赖问题，但是很难训练好。CNN/RNN/Transformer等网络结构都可以看作是在MLP上添加的约束条件。通过这些先验约束，同等参数量下的优化难度被降低，模型更容易找到最优解。前面提到的“使用神经网络来拟合”就是使用CNN，RNN或者Transformer来实现编码器和解码器。 因为我们本系列介绍的主角是Transformer，就说明Transformer在实现编码器和解码器上是存在优势的，因此我们先来看看CNN和RNN方案的问题所在。 注：本节只是从普遍意义或者说是在典型问题上进行阐释，并非定论。因为CNN和RNN方案也是在不停的发展，其某一个阶段的方案可能解决（或者缓解）了下面提到的问题。 当面临冗长且信息密集的输入序列时，编码器-解码器模型在整个解码过程中保持相关性的能力可能会减弱。为了更好的说明，我们先看看序列转换面对的主要技术挑战：对齐问题和长依赖问题（或者说是遗忘问题）。 我们来看看为什么要对齐。首先，在某些领域（比如语音识别）中，虽然输入与输出的顺序相同，但是没有一一对应的关系。其次，在某些领域（比如机器翻译）中，在处理完整个输入序列后，模型的输出序列可能和输入序列的顺序不一致。以机器翻译为例，假如我们要让模型将英语 ""how are you"" 翻译为中文 ""你好吗""，或者将“Where are you”翻译成“你在哪里”，我们会发现翻译结果中的语序和原来句子的语序并不相同，同时，一些翻译结果并不能与英语中的词汇一一对应到。 不对齐问题带来的最大困境是：在时间序列的 t 时刻，我们并不能确定此时的模型是否已经获得了输出正确结果所需要的所有信息。因此，人们往往先把所有输入编码到一个隐状态，然后逐步对这个隐状态进行解码，这样才能确保在解码过程中模型一定收到了所需的全部信息。虽然此方案可以保证输入信息的完整性，但却有一个明显缺陷，即在解码过程中，无法确定贡献度。比如，当把”I love you""翻译成“我爱你”时，“我”应该与“I”对齐（因为其贡献最大），但是该方案中，""I"",""love"",""you""这三个词对“我”的贡献都是一致的。 我们以下面句子为例来进行分析。“秋蝉的衰弱的残声，更是北国的特产，因为北平处处全长着树，屋子又低，所以无论在什么地方，都听得见它们的啼唱。” 将例句从英文翻译成中文时，英文和中文明显是有对齐关系的，因此需要知道哪个英文单词对应到哪个中文。比如将”它们“翻译成”They“，但是“它们”代表什么呢？是“树”？“屋子“？还是”秋蝉“？通过”啼唱“和知识，我们知道“秋蝉”和“它们”指代是同一个对象，但是如果把”听得见它们的啼唱“ 修改为“看见它们的树荫”，则“它们”指代的就是“树”了。 人类可以很容易的同时看到“秋蝉”和“它们”这两个词然后把这两个词关联起来，即人们知道“它们”和“秋蝉”有长距离的依赖关系，从而理解整个句子。但是对于计算机或者对于模型来说，”秋蝉“和”它们“在例句中的距离太长了，很容易被两个词中间的其它词干扰。为了准确给出最终的答案，神经网络需要对前面”秋蝉的衰弱的残声“和”它们“之间的交互关系进行建模。然而，某些神经网络很难处理长距离依赖关系，因为处理这种依赖关系的关键因素之一是信号在网络中穿越路径的长度，两个位置之间路径越短，神经网络就越容易学习到这种长距离依赖关系，两个位置之间距离越远，建模难度就越大。如果模型无法处理长距离依赖，则会出现长时信息丢失，也就是产生了遗忘问题。 CNN的本质是学习空间数据中的局部依赖关系。CNN的卷积操作可以提取重要特征，但是因为单个卷积核的长度一般比较小，所以CNN的卷积感受视野是局部的，提取的是局部特征，进行局部信息计算。即CNN对相对位置敏感，对绝对位置不敏感，难以提取序列的长距离依赖关系。 为了让CNN可以处理长序列，人们一般会堆叠更多的卷积，通过叠加多层卷积区去把局部感受野进行扩大，让卷积网络依靠深度来弥补全局信息的缺失，从而捕捉长距离依赖。在这种方案中，不同卷积层会提供不同层次的特征，进而在上层完成长距离信息依赖计算，把长序列的信息压缩到一个卷积窗口内部，以此来让模型有机会捕捉到长距离依赖和复杂的结构关系。 比如下图所示，最下层CNN使用滑动窗口作用于这个文本序列，每个窗口都对其中的数据进行处理。A1窗口获取到了“秋蝉”这个信息，A3窗口获取到了“它们”这个信息。但是，因为“秋蝉”和“它们”距离太远，所以没有一个单一窗口可以把这两个词建立起依赖关系，即没有单一窗口可以同时看到这两个词。因此，模型只能持续堆叠卷积网络，不断加深整个网络，这样C1这个窗口才可以同时包含“秋蝉”和“它们”的信息。 但是深度就代表间接，间接就代表着流失。在CNN方案中，因为信息被“逐级加工，层层抽象”，而且信息传递过程不够透明，信息会在过深的网络传播过程之后只有部分保留，导致模型性能下降。所以CNN一般在长依赖关系建模场景使用较少，更适合短文本计算。 从表象上看，RNN是时序结构，后面的时刻天然就依赖于前面时刻的输出。从本质上来说，RNN是一种连接主义模型，具有选择性地在序列步骤间传递信息的能力，可以对序列上下文信息以及不同范围元素间的局部依赖关系进行捕捉。RNN的独到之处在于引入了“记忆”功能，让网络能记住之前输入的信息。随着数据在RNN中的流动，之前时间步的记忆会作为输入参与到当前数据的处理中，让模型能够动态地融合时间上下文和序列的历史信息。因为可以有效地处理变长的序列数据，理论上，RNN可以预测无限长句子，利用前面所有信息，所以非常适合翻译场景。 事实上，在Transformer出现之前，编码器和解码器通常是由RNN或其变体（如 LSTM 或 GRU）组成的。我们先看看如何用RNN实现编码器。以下图为例，编码器需要把“北国的特产”这个句子编码成隐状态。图中的每个方块都是一个简单的RNN单元。每个RNN单元接收两个输入（输入词和隐状态），输出一个隐状态。 模型在第一步得到输入h0和“北”，调用函数f()进行计算，得到输出h1=f(h0, 北)。h0是第一个隐状态（通常内容是数值0或者随机值）。模型在第二步得到输入h1和“国”，同样调用函数f()进行计算，得到h2。以此类推，模型最终输出h5。在计算过程的每一步t，前面所有节点得到的信息都保存在上一步计算出来的中间隐状态ht中，因此，后一个词的计算都会用到前面所有词的输出结果。 可以把隐藏状态ht看作是信息循环的载体，其承载的信息可以跨时间步在RNN中进行传递。随着数据在RNN中的流动，之前时间步的激活状态会作为输入参与到当前数据的处理中，让模型能够动态地融合时间上下文和序列的历史信息。因此从理论上来说，RNN可以通过隐状态得到任意两个词的依赖，无论这两个词距离多远，它们的信息一定会在某一步计算之中汇聚。 我们再给出编码器-解码器的结构图，其中si是解码器在时刻\(i\)的隐状态。编码器读入输入的token xi，并对每个token xi生成一个隐状态hi。从第一个h1到最后一个hm，这些隐状态不断将前面信息积累。最后的携带整个输入序列信息的hm会作为解码器的输入s0。 RNN的缺点同样鲜明。在RNN方案中，在每个时间步，RNN都会将序列中之前的所有信息压缩到一个固定长度的隐向量，最终编码器和解码器之间只通过这个固定长度的隐状态来传递信息。固定长度的隐状态或者说有限的记忆能力在处理长序列时会导致接下来的几个问题，比如信息遗失、信息瓶颈等。 RNN需要对序列内容进行逐步处理， 每一步的输出取决于先前的隐藏状态和当前的输入。RNN这种串行计算在本质上是一种递归，其阻碍了训练时的并行计算，导致训练效率较低，训练时间过长。 RNN的网络结构特点导致难以训练。RNN 用于信息传输通路只有一条，并且该通路上的计算包含多次非线性激活操作，当 RNN 处理长序列时，因为时间步增加带来的多层激活函数的嵌套会导致梯度反传时指数级地衰减（消失）或增长（爆炸），这就是梯度消失问题或者梯度爆炸问题。当梯度消失时，前面的梯度信息无法有效地传递 到后面，也就导致词之间距离越远，前面对后面的影响越弱，所以RNN网络难以学习远距离依赖关系。当梯度爆炸时，网络的权重会变得极大，导致网络不稳定。而且当面对长序列时，RNN需要大量内存来维持长序列的隐状态，比如需要完整理解整个句子乃至整篇文章才能做出判断，这些内存负担对训练也造成了很大挑战。 我们再仔细看看”关系距离问题“。对于CNN方案，序列的第一个词和最后一个词要通过多层卷积后才可以建立联系。下图中的粗线为CNN结构中两个词建立关系所需的最长距离。 而RNN方案需要对序列“从头看到尾”才能确定给这两个词建立关系所需的距离。下图中的粗线是RNN结构中两个词建立关系所需的最长距离。 因此我们要面对的问题是：如何把大量的token压缩到一个隐藏状态中，而且该状态还可以有效的捕捉它们的底层结构和关系。如果想从根本上解决问题，我们有如下几步可以选择： 注意力（Attention）机制由Bengio团队2015年在论文“NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE”中提出，其主要思路为通过模仿人类观察事物的行为来降低算法复杂度、提高性能。人类在感知、认知和行为决策过程中会选择性地关注和处理相关信息，从而提高认知效率和精度，比如人类可以依据兴趣和需求选择关注某些信息而忽略或抑制其它信息，并且可以在多任务上分配注意力从而达到同时处理多个信息的目的。大家最熟悉的例子就是，当一个人看图片时，他会先快速通览，然后对重点区域进行特殊关注。而在文本生成的每个阶段，并非输入上下文的所有片段都同样重要。比如在机器翻译中，句子“A boy is eating the banana”中的“boy”一词并不需要了解整个句子的上下文之后，再来进行准确翻译。 注意力机制的本质可以用一句话来总结：上下文决定一切（context is everything）。一个字 / 词在一篇文本中表达的意思通常与它的上下文有关。比如下面两个句子中都有“transformer”这个单词，但是第一个“transformer”应该翻译成“变压器”，第二个“Transformer”最好不做翻译。 如何才能对“transformer”这个多义词进行语义区分？我们必须考虑单词的上下文才能更好的把单词的语义识别出来，即不仅仅要考虑到词本身，还要考虑其他词对这个词的影响，也就是语境的影响。比如第一个句子的“pole”、”fallen”和“wires”这几个邻近单词暗示了此处的“transformer”和真实物理环境相关。第二个句子的“model”和“natural language processing and image classification”则直接告诉我们此处的“Transformer”是深度学习相关概念。最终我们通过上下文语境可以推断出“Transformer”的准确含义，从而把这两个英文句子翻译成如下： 这就是注意力机制的作用：将每个词与序列中的其他词联系起来，通过句子中的其它词对我们关注词的语义进行推断。 注意力机制也是一种资源分配方式。目前我们知道了上下文的重要性，但是这还不够，因为一个单词的上下文包括很多其它单词，不同单词对目标单词的影响力往往不同。以翻译为例，因为输入语句是一个连贯的整体，所以每个输入词\(X_i\)都会给每个输出词\(Y_i\)造成影响。因此在考虑单词上下文时候，也需要考虑出上下文中每一个元素应该考虑多少。比如第二个英文句子中，“model”对“Transformer”的影响力必然是最大的。所以我们需要一种机制来依据不同的上下文聚焦于不同的信息，这样，一来可以使得序列中的重要元素被投射以较高注意力，重要信息不会被淹没；二来可以将有限的计算资源用来处理更重要的信息，从而做到提纲挈领。 注意力机制就是这种资源分配机制。其在学习的过程中自适应的给予输入的不同词赋予不同的注意力权重，从而区分输入的不同部分对输出的影响，自适应的学习应该关注的重点位置，做出准确判断，即注意力赋予了模型分辨的能力。 其实，论文“Recurrent Models of Visual Attention”中有一段话就深刻的印证了资源分配这个角度。具体如下：人类感知的一个重要特性是，人们不会一次处理整个场景。相反，人类有选择地将注意力集中在视觉空间的某些部分上，以在需要的时间和地点获取信息，并随着时间的推移将不同注视点的信息结合起来，建立场景的内部表示，指导未来的眼球运动和决策。将计算资源集中在场景的各个部分可以节省“带宽”，因为需要处理的“像素”更少。但它也大大降低了任务的复杂性，因为感兴趣的对象可以放置在注视的中心，而注视区域外的视觉环境的无关特征（“杂乱”）自然会被忽略。其英文原文如下： 确定了资源分配的原则之后就可以进行信息交换。注意力机制计算过程就是序列中元素交换信息的过程。注意力机制的输入是一个序列或者集合，注意力机制会从输入中选择性提取信息，据此计算出一组权重。这组权重代表了各个信息的重要程度，使用这种权重与原始信息相乘就得到了注意力处理后的加权信息。 信息交换在一定程度上起到了RNN中记忆力模块的作用，使得注意力编码器也能像RNN一样对一个复杂的语句或者场景进行理解和解析，比如： 将注意力机制应用在一个序列内部（这就是人们经常提到的自注意力机制），可以让序列中每个词都和序列中其他词关联起来，这样序列中每一个元素都有机会根据自身特征，有选择性地依据词之间的相关性来吸取整个序列中每一个其他元素的信息，对本身进行动态调整。这使得模型能够捕捉长距离依赖关系，不受距离的影响。考虑以下两个句子，第一句中的“it”就指代猫，因此“it”更多吸收了“cat”的信息。第二个句子的“it”指代牛奶，因此“it”更多吸收了“milk”的信息。 自注意力机制的目的是为当前单词创建抽象的、丰富的表征。这个表征是本单词被相同序列中的其它单词所影响的结果。经过自注意力机制处理之后，现在每个新单词都融入了其它单词的部分信息，这是一种数据依赖的加权平均，这种表征更加丰富。如果非要将自注意力机制和之前的注意力机制进行对比的话： 通过上面的原理分析，我们可以知道，注意力的核心思想就是帮助模型为输入的不同部分分配不同的权重，这样可以提取关键信息，让模型判断更精准，更加节省算力和存储。但是如何实现注意力机制？这就要解决两个问题： 论文”A General Survey on Attention Mechanisms in Deep Learning“用下图总结了注意力模型的通用结构，论文作者把这种通用架构叫做任务模型。任务模型包含四个部分： 我们用一个句子“北国的特产”为例来解释下图。图上X就是“北国的特产”这个完整的句子，F是从“北国的特产”提取到的特征向量列表，Z是“特”，q是从“特”提取出来的特征向量，我们目的就是要获取特征向量列表中哪个特征向量包含对于”特“最重要的信息。 因为此处提到了Q、K、V这三个术语，我们先用文本翻译为例来做初步介绍，后续章节会对Q、K、V进行深入分析。从上文的分析可知，序列中的每个单词都需要了解序列中其它单词的信息，从而才能确定彼此间的相互关系。因此每个单词都要去询问其它单词：咱俩之间是否密切？其它单词回复关系密切与否。拿到相互关系后，每个单词会据此把其它单词的信息吸收进来做信息融合。上述操作其实是一种搜索+合并的运算，我们需要找到一个适合的机制将这种运算进行落地。而上图注意力模型中，有两个输入：q（正在处理的序列）和F（被关注的序列），F又分别转换为K和V，这三个变量综合起来使用就可以满足我们的需求。 从词典的角度来看也许可以促进理解。query是你要找的内容，key是字典的索引（字典里面有什么样的信息），value是对应的信息。普通的字典查找是精确匹配，即依据匹配的键来返回其对应的值。而注意力机制是向量化+模糊匹配+信息合并。注意力机制不仅查找最佳匹配，还要依据匹配程度做加权求和。源序列每个元素转化为 对，这就构成了源序列的字典。目标序列每个元素提出了query，这就是要查询的内容。在查找中，目标序列中每个元素会用自己的query去和目标序列每个元素的key计算得到对齐系数。这个对齐系数就是元素之间的相似度或者相关性。query和key越相似就代表value对query的影响力越大，query越需要吸收value的信息。随后query会根据两个词之间的亲密关系来决定从V中提取出多少信息出来融入到自身。 通过 query、key、value 这三个向量的相互作用，模型得以衡量每个单词对其他单词的关注度。最终，源序列的每个元素会把自己融合其它单词提供的信息之后得到的真实数据放到一个向量中。 我们假设源序列和目标序列为同一个序列，下图中给出了序列中”一个“和其它单词之间的相似度。虚线代表key与query的相关度，线条的粗细分布就叫做”注意力分布“。或者说，线条的粗细就是权重，线条越粗，表示key与query越相关，对理解query越重要，value的权重就越大。 我们首先看看总体思路。自注意力层是一种和循环层和卷积层等效的计算单元。它们的目的都是把一个向量序列映射成另一个向量序列，比如说编码器把x映射成中间表示z。我们先回想翻译场景。如果是RNN方案，则最终编码器会生成一个隐向量，然后把这个隐向量传递给解码器进行解码。前面已经分析了这种方案的弊端，比如隐向量是固定的。为了克服这种弊端，我们应该在每一个时间步t都会产生一个隐向量\(h_t\)，把这些\(h_t\) 保存起来。当产生新输出时，我们让模型回顾之前保存的所有隐状态，发现隐状态中的关键信息就用上，这样就摆脱了RNN中隐向量是固定长度的弊端。但是如何判断某个隐状态对当前生成词是否重要？这就需要模型采用某种机制进行学习，从而才能了解需要给这个隐状态多少注意力。总而言之，注意力机制的任务应该是找到解码器当前隐向量和编码器所有隐向量之间的相互关系。按照上述思路，注意力机制的计算总体可以分为两步： 得到的注意力权重为 $$w_{3,1},w_{3,2},w_{3,3},w_{3,4}=softmax(g(Y2, C1, C2, C3, C4))$$。 当模型需要决定给予序列中某个单词以多大的“注意力”时，它会计算该单词与其他单词之间的注意力分数。注意力分数是衡量序列中不同单词对当前单词的重要性程度的指标，或者说是目标单词和输入中某单词对齐的可能性大小。可能性越大就应该赋予更大的权重。大权重代表在生成输出时，当下预测单词应该更加关注源文本中其对应单词的信息。 注意力分数通过相似度计算函数得到。该函数一般接受key和value向量作为输入，输出key和value向量之间的相关性，即注意力分数。下图提供了这些函数的概述，其中\(q\)是查询向量，\(k_l\)是矩阵\(K\)的第\(l\)列。如果对应到seq2seq，则q可以认为是解码器输出的隐向量，k可以认为是编码器内部的隐向量。现实中，相似度计算函数是通过矩阵形式来计算，而非单独计算某一列。 得到注意力分数之后，模型会使用softmax操作将注意力分数进行归一化得到注意力权重，这样可以使得所有权重总和为1，确保所有源元素贡献的特征总量保持一定，也可以更加突出重要的权重。 得到注意力权重之后，每个query都能从对应的key上获取相应的信息，此时，就需要由输出函数（Output Function）来将各个部分组合成最终的高级特征向量进行输出。在本阶段，注意力机制会以加权求和的模式对数据进行加工。这意味着每个单词在新的表示中不只是自身的信息，还包含了其他单词的信息，这帮助模型捕捉输入序列中的依赖关系。 回忆下之前提到的RNN方案和CNN方案遇到的问题：对齐问题、信息遗失问题和长依赖问题。以及为了从根本上解决这些问题，我们提出的几种改进思路： 我们接下来就以自注意力机制为例，看看其如何通过自己的优点来解决之前提到的问题，顺便也和这两个方案做下对比分析。 与RNN、CNN方案相比，自注意力机制可以增大信息含量，从而有效的解决RNN中的信息遗失问题。从某种程度上来看，所有的序列建模都在做如下操作： RNN方案中，解码器把过去所有的上下文信息压缩到一个固定大小低维向量（隐状态）中。解码器的不同阶段都使用这个隐状态。该方案的优势是在长上下文中的线性（相对于二次）复杂性。但是在长上下文时，RNN受限于固定大小的隐状态（表达能力有限），很难利用额外的条件信息。 TTT（Test-Time Training）则把上下文压缩到模型的权重中，其优势是既能在时间上保持固定大小，又能大大增强表达能力。 自注意力机制使用一个列表（从后续文章中我们会知道这其实是KV Cache）作为隐状态。所有的上下文都存储在列表中，没有被压缩。列表中所有的上下文共同构成了统一的隐状态（每个阶段的隐状态是列表的一项），这样编码器可以将更多数据传递给解码器。 因此，我们可以看到自注意力机制（Transformer）的优势：不只像RNN那样只传递一个编码器最终的隐状态，而是传入所有的隐藏状态（对应处理过的所有token）到解码器。这样新的token就可以和过去所有上下文进行交互。当然，随着上下文长度的增加，使用列表的成本也会越来越高，其处理时间随上下文长度急剧增长，列表的内存也急剧增加。 自注意力机制中，当某单词获取其它单词信息时，其它单词和它的位置距离都是固定常数。这样，两个词之间的紧密程度仅取决于它们的真实相关性，而非取决于距离。或者说对绝对位置不敏感，任意两个词都可以直接建模。该特点可以解决长距离依赖问题。我们具体分析下。 首先，CNN虽然可以借助增加卷积层数来扩大视野，融合信息，但是信息在过深的网络传播过程之中容易丢失，导致模型性能下降。自注意力机制摒弃了 CNN 的局部假设，把感知域（perceptive field）调整到了整个序列。RNN方案因为无法弥补长距离依赖问题会导致梯度消失和梯度爆炸问题。在处理任何一个单词时，自注意力可以注意到整个句子的所有单词，这样可以将序列中的任意两个位置之间的距离缩小为一个常量，能在常数时间内有效地捕捉到它们之间的依赖关系，直接建立联系，从而消除了距离概念。因为此特点，自注意力机制的信息传播损失小。下图为三个方案构建词语之间关系所需距离的对比。 另外，常数距离特点也让自注意力机制没有“有序假设”。让自注意力机制在处理每个位置时，并非像RNN那样更容易注意到靠后的内容，忽略靠前的输入。而是平等看待序列中每个单词的顺序，这样能够有效得考虑到输入序列中的其他位置，可以更好的将对其他单词的“理解”融入到当前处理的单词中，信息融合效率高。 加权求和可以分为加权和求和两方面来看，前者是对数据进行有差别的对待，而后者是做数据融合，综合在一起就是用提纲挈领（对于不同信息量的元素给予不同的关注度）的方式对数据进行加工。我们接下来从不同角度对加权求和与CNN、全连接层进行比对分析。 首先，注意力机制是动态产生权重。CNN或者全连接层的权重是静态权重，这些权重在训练时候固定下来，在推理时使用相同权重进行推理，并没有建立权重和输入实体本身的关系。注意力机制是动态权重，用输入的query、key通过相似度计算来得到注意力权重。注意力机制根据输入来计算出输出应该更关注哪些输入，权重的产生随着输入变化，这是一种自适应操作。 其次，从另一个角度来看，注意力机制是站在某个输入对象的立场上看待上下文和它之间的关系，并以此关系为权重来确定如何吸取其它对象的信息。因此，注意力机制是一种相对关系建模。而这种操作大多采用全局注意力以建模更广范围的依赖关系，是一种全局操作。 将注意力应用在序列转换的源序列和目标序列之间就是对齐机制。在机器翻译中，源序列中的每个单词都可能会对输出造成影响，不仅仅是待翻译词之前的输入，其后续的输入也可能对输出产生影响，因此需要结合源序列和目标序列进行综合考虑。比如下面中译英的例子，当翻译到”一个“时，我们无法确定英文应该翻译成”a“还是”an“，必须往后看到”苹果“才能确定，因此需要在翻译时把”a or an“的操作和”苹果“进行对齐。 注意力允许依据输入序列和输出序列的不同部分进行相关度计算，这样就可以决定在输出某个单词时，模型将重点关注输入数据的哪些部分。从而在输入序列和输出序列进行对齐，更准确地学习序列之间的关系，提高模型的泛化能力和性能。下图中虚线代表key与query的相关度，线条越粗，表示越相关。我们可以看出来，“一个”和“苹果”等词对“an”的编码更重要（或更相关或更相似），应该承担更多的对“an”的预测，需要赋予更多的权重。因此，在模型生成“an“的时候，不但要从”一个“提取语义，也要从”苹果“提取语义，才能判断出来应该是”an“还是”a“。最终”一个“和”苹果“共同决定了对应的英文单词应该是""an""。 编码器可以同时从左到右和从右到左读取输入序列，并将每个时间步的隐状态拼接起来作为输出。这样做的好处是，可以让编码器同时考虑输入序列中每个单词的前后上下文信息，从而生成更丰富和完整的表示。 综上所述，注意力机制确保每个解码步骤都由最相关的上下文片段提供信息，为长期存在的长距离依赖问题提供了稳健的解决方案，从而重新定义了序列建模的格局。而且，注意力机制的提出不仅是技术上的突破，也为机器学习领域开辟了新的视野，更反映了人类认知的深刻影响。正如 Andrej Karpathy 所评价的：“注意力是一种重大解锁，是神经网络架构设计的一次飞跃。” 注意力机制只是一种思想，可以用到很多任务上。我们接下来看看几个经典的使用注意力机制对Encoder-Decoder进行改进的案例，也梳理一下历史上的重要节点。用范伟的话说：我们要知道Transformer是怎么来的。当然，我们也希望可以看到Transformer是怎么没的，毕竟如果有新方案取代了Transformer，那就说明AI领域将迎来新的历史性突破。 我们首先用一个图来展示下注意力发展历史。大家可以看到， Transformer是建立在很多巨人肩膀之上的集大成者。 论文Recurrent Continuous Translation Models 被认为是神经网络机器翻译(NMT)的开篇之作，其特点如下： 论文""Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation""这篇是Bengio 团队的成果，其特点如下： 没有把编码器-解码器做成一个端到端的模型，只是把解码器的输出概率作为特征喂给了统计机器翻译（SMT）模型。 论文“Sequence to Sequence Learning with Neural Networks”也起到了重要作用，其特点如下： 论文“Neural Machine Translation by Learning to Jointly Align and Translate”提出的Bahdanau Attention是Attention的开创者之一。其特点如下： 作者在论文中使用注意力机制解决了如何建模“与距离无关的依赖”，如何把编码器产生的信息更有效的传递给解码器的问题。这样注意力机制就和RNN一起配合，解决了遗忘和对齐问题。 !Andrej Karpathy分享了Dzmitry Bahdanau 的私人邮件，从中我们可以看到，为了克服编码器和解码器之间的瓶颈，Dzmitry Bahdanau的思路发展历程。 可以看到，突破性的想法往往来自那些在实践中寻求解决问题的创新者，而非空想理论家。Dzmitry Bahdanau在邮件中说得非常好： 我的AI宏愿是启动更多像机器翻译项目那样的惊人应用项目。优秀的研发工作对基础技术的进步贡献远胜过那些我们常认为是“真正”AI研究的复杂理论。 论文“Effective Approaches to Attention-based Neural Machine Translation”基于Bahdanau Attention对注意力机制的计算方式进行多样性探索。 ResNet是何凯明大神的经典之作，其可以有效解决梯度消失/爆炸问题，让神经网络层数的瓶颈得以突破。后面有文章详细讲解，这里不再赘述。 之前的注意力机制都关注不同序列之间的注意力，即交叉注意力。而论文Long Short-Term Memory-Networks for Machine Reading在交叉注意力的基础上提出了self-attenton（自注意力机制），或者叫 intra-attention。作者认为我们在顺序地阅读每个单词的时候，每个单词和前面单词的关联度是不一样的，因此使用了注意力机制来完成这个分析工作。 论文的动机如下。传统的LSTM在处理输入时把句子视为单词序列，并递归的将每个词与之前的记忆结合起来，直至得到整个句子的语义表示。这种方案面临两个问题： 论文QKV-Attention: FRUSTRATINGLY SHORT ATTENTION SPANS IN NEURAL LANGUAGE MODELING这可能是最早提出QKV概念的论文。 作者认为目前注意力机制的隐向量承担了太多功能，这样会分身乏术，影响模型效果。因为同一个词，它在背景向量中的意义和它作为查询词时候代表的意义不同，所以要区分开来。因此该论文对每个时间步产生的输出向量进行分割成三个向量key, value, predict，各自有不同的分工。以下是修改逻辑和历程。 原始模型结构如下图所示，神经网络语言模型只有一个输出向量，同时用来计算注意力向量，编码历史上下文信息和预测下一个词的分布。 论文作者将原始模型的输出划分成两部分：key和value，用key来计算注意力向量，用value来编码下一个词的分布和上下文信息。 但是value仍然同时被用来编码上下文信息和下一个词的分布表示（既要对查询的结果负责，又要对聚合的结果负责），一心二用容易出错。因此作者做了进一步改进。 新模型结构如下图所示，作者对原始模型的输出做进一步划分为三部分：key，value和predict。用key来计算注意力向量，用value来编码上下文信息，用predict来编码下一个词的分布。 作者又继续进行改进，将原始模型的输出划分成N－1部分，用前N－1个时间步的输出向量的一部分来计算下一个词的分布表示。模型结构如下图所示。 论文Self Attention & MultiHead Attention : A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING 把注意力机制用到了文本表示学习中，让注意力机制完成多项工作。论文的注意力计算过程如下： 论文[Convolutional Sequence to Sequence Learning](Convolutional Sequence to Sequence Learning) 使用CNN模型结合注意力机制进行操作，从而解决RNN不能并行的问题。 与RNN相比，CNN存在一定的优势。一来CNN可以并行处理，因此训练速度快，二则RNN并不能很好地处理句子中的结构化信息。因此，作者使用CNN来做编码器和解码器，利用层次结构来捕获词之间远程的依赖关系，也可以更好地捕捉更复杂的关系。在解码过程中，每一个卷积层都会进行注意力操作，称为multi-step attention。 我们可以看到，在上述的演进过程中，Transformer所包括或者说所需要的组件一步步的被实现出来。然而上述注意力方案中依然有RNN、CNN的存在，这拖了后腿。比如RNN无法并行训练，不利于大规模快速训练和部署，也不利于整个算法领域发展。 因此，Transformer作者彻底摒弃了RNN和CNN，构建了一个全新的序列转换架构。Transformer整个网络结构完全是由注意力机制组成，通过把序列元素两两直接比较，Transformer可以学习到输入序列里所有词的相关性，一步到位捕捉到全局的联系。同时，因为Transformer不是按顺序分析，这样它可以并行操作，也使得它比 RNN 具有更高的计算效率和可扩展性。最终，Transformer就是这样一个融汇贯通的集大成者。 当然我们也要看到，RNN和CNN一直没有放弃努力，各自的发展也是有声有色。尤其在强化学习领域，RNN更是在发挥巨大的作用。我们也期望未来会有更多创新的模型和方法来让Transformer在强化学习领域发挥更大的作用。 A General Survey on Attention Mechanisms in Deep Learning Gianni Brauwers and Flavius Frasincar Andrej Karpathy 首次公开私人邮件：揭秘Transformer 注意力机制真相 AI寒武纪 Attention is All your Need Bahdanau 和 Luong Attention 直观介绍 Honoria [Convolutional Sequence to Sequence Learning](Convolutional Sequence to Sequence Learning) Effective Approaches to Attention-based Neural Machine Translation Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation Long Short-Term Memory-Networks for Machine Reading NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE QKV-Attention: FRUSTRATINGLY SHORT ATTENTION SPANS IN NEURAL LANGUAGE MODELING Recurrent Continuous Translation Models Recurrent Continuous Translation Models Self Attention & MultiHead Attention : A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING Self Attention & MultiHead Attention : A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING Self Attention 1: Long Short-Term Memory-Networks for Machine Reading seq2seq中的两种attention机制（图+公式） 胡文星 Sequence to Sequence Learning with Neural Networks Thang Luong's Thesis on Neural Machine Translation Minh-Thang Luong Transformer自下而上理解(4) Attention without RNN marsggbo 《FRUSTRATINGLY SHORT ATTENTION SPANS IN NEURAL LANGUAGE MODELING》阅读笔记 Simple 一文看懂 Bahdanau 和 Luong 两种 Attention 机制的区别 Flitter 注意力机制 OnlyInfo 理解Attention:从起源到MHA,MQA和GQA Linsight 看图学大模型：Transformers 的前生今世(上） 看图学 第四篇：一文搞懂Transformer架构的三种注意力机制 AIwithGary 综述：图像处理中的注意力机制 极市平台 非Transformer架构的进展与猜想 StormBlafe",https://www.cnblogs.com/rossiXYZ/p/18705809,2025-03-13 10:53:59
感谢「河南图奕网络」赞助园子，成为第一家创始赞助商,未知作者,在8月1日发布救援行动-赞助商计划后，我们并没有抱什么奢望，更没有妄想很快能找到赞助商，只是为救园多一点可能的希望，万一找到一家赞助商，就会多一份救园力量。 没想到第2天就有幸遇到一家有意向的企业，中午加微信开始沟通赞助商计划的细节，晚上快7点的时候就收到了赞助款，万一很快成真。 相信这样非同寻常的公司做产品也会很用心，而且他们的产品目标用户群也是开发者，欢迎大家关注一下他们旗下的两款产品。 IShell现在有个小福利，加入IShell用户交流QQ群时备注「博客园」（QQ群号：936123255），可以免费领取三个月VIP。,https://www.cnblogs.com/cmt/p/18341478,2025-03-13 10:54:31
老周,未知作者,"由于两个月的奋战，导致很久没更新了。就是上回老周说的那个产线和机械手搬货的项目，好不容易等到工厂放假了，我就偷偷乐了。当然也过年了，老周先给大伙伴们拜年了，P话不多讲，就祝大家身体健康、生活愉快。其实生活和健康是密不可分的，想活得好，就得健康。包括身体健康、思想健康、心理健康、精神健康。不能以为我无病无痛就很健康，你起码要全方位健康。 不管你的工作是什么，忙或者不忙，报酬高或低，但是，人，总得活，总得过日子。咱们最好多给自己点福利，多整点可以自娱自乐的东西，这就是生活。下棋、打游戏、绘画、书法、钓鱼、飙车、唢呐……不管玩点啥，只要积极正向的就好，可以大大降低得抑郁症、高血压的机率；可以减少70%无意义的烦恼；可以降低跳楼风险；在这个礼崩乐坏的社会环境中，可以抵御精神污染……总之，益处是大大的有。 然后老周再说一件事，一月份的时候常去工厂调试，也认识了机械臂厂商派的技术支持——吴大工程师。由于工厂所处地段非常繁华，因此每次出差，午饭只能在附近一家四川小吃店解决。毕竟这方圆百十里也仅此一家。不去那里吃饭除非自带面包蹲马路边啃，工厂不供食也不供午休场所。刚开始几次出差还真的像个傻子似的蹲马路边午休。后来去多了，直接钻进工厂的会议室睡午觉。 4、做过许多项目的一线开发者。我强调的项目数量多，而不是长年只维护一个项目的。只有数量多你学到的才多； 5、社区贡献较多者，这个和3差不多。不过，老周认为的社区贡献就是不仅提供代码，还提供文档、思路、技巧等； ========================================================================================================== 下面进入主题，咱们今天聊聊 IChangeToken。它的主要功能是提供更改通知。比如你的配置源发生改变了，要通知配置的使用者重新加载。你可能会疑惑，这货跟使用事件有啥区别？这个老周也不好下结论，应该是为异步代码准备的吧。 这个 Change Token 思路很清奇，实际功能类似事件，就是更改通知。咱们可以了解一下其原理，但如果你觉得太绕，不想了解也没关系的。在自定义配置源时，咱们是不需要自己写 Change Token 的，框架已有现成的。我们只要知道要触发更改通知时调用相关成员就行。 1、runtime-main\src\libraries\Common\src\Extensions\ChangeCallbackRegistrar.cs：这个主要是 UnsafeRegisterChangeCallback 方法，用于注册回调委托； 2、runtime-main\src\libraries\Microsoft.Extensions.Primitives\src\ChangeToken.cs：这个类主要是提供静态的辅助方法，用于注册回调委托。它的好处是可以循环——注册回调后，触发后委托被调用；调用完又自动重新注册，使得 Change Token 可以多次触发； 3、runtime-main\src\libraries\Microsoft.Extensions.Primitives\src\CancellationChangeToken.cs：这个类是真正实现 IChangeToken 接口的； 4、runtime-main\src\libraries\Microsoft.Extensions.Configuration\src\ConfigurationReloadToken.cs：这个也是实现 IChangeToken 接口，而且它才是咱们今天的主角，该类就是为重新加载配置数据而提供的。调用它的 OnReload 方法可以触发更改通知。 CancellationTokenSource 类表示一个取消任务的标记，访问它的 Token 属性可以获得一个 CancellationToken 结构体实例，可以检索它的 IsCancellationRequested 属性以明确是否有取消请求（有则true，无则false）。 还有更重要的，CancellationToken 结构体的 Register 方法可以注册一个委托作为回调，当收到取消请求后会触发这个委托。对的，这个就是 Change Token 灵魂所在了。一旦回调被触发后，CancellationTokenSource 就处于取消状态了，你无法再次触发，除非重置或重新实例化。这就是回调只能触发一次的原因。 下面，咱们完成一个简单的演示——用数据库做配置源。在 SQL Server 里面随便建个数据库，然后添加一个表，名为 tb_configdata。它有四个字段： ID和config_key设为主键，config_value 是配置的值，remark 是备注。备注字段其实可以不用，但实际应用的时候，可以用来给配置项写点注释。 然后，在程序里面咱们用到 EF Core，故要先生成与表对应的实体类。这里老周就不用工具了，直接手写更有效率。 上述代码的情况特殊，实体类的名称和成员名称与数据表并不一致，所以在重写 OnModelCreating 方法时，需要进行映射。 1、ToTable(""tb_configdata"") 告诉 EF 实体类对应的数据表是 tb_configdata； 2、HasKey(c => new { c.ID, c.ConfigKey })：表明该实体有两个主键——ID和ConfigKey。这里指定的是实体类的属性，而不是数据表的字段名，因为后面咱们会进行列映射； 3、HasColumnName(""config_key"")：告诉 EF，实体的 ConfigKey 属性对应的是数据表中 config_key。后面的几个属性的道理一样，都是列映射。 做映射就类似于填坑，如果你不想挖坑，那就直接让实体类名与表名一样，属性名与表字段（列）一样，这样就省事多了。不过，在实际使用中真没有那么美好。很多时候数据库是小李负责的，人家早就建好了，存储过程都写了几万个了。后面前台程序是老张来开发，对老张来说，要么把实体的命名与数据库的一致，要么就做一下映射。多数情况下是要映射的，毕竟很多时候数据库对象的命名都比较奇葩。尤其有上千个表的时候，为了看得顺眼，很多人喜欢这样给数据表命名：ta_XXX、ta_YYY、tb_ZZZ、tc_FFF、tx_PPP、ty_EEE、tz_WWW。还有这样命名的：m1_Report、m2_ReportDetails…… m105_TMD、m106_WNM、m107_DOUBI。 此处咱们不用直接实现 IConfigurationProvider 接口，而是从 ConfigurationProvider 类派生就行了。自定义配置源的东东老周以前写过，只是当时没有实现更改通知。 由于老周不知道怎么监控数据库更新，最简单的办法就是用定时器循环检查。重点是重写 Load 方法，完成加载配置的逻辑。Load 方法覆写后不需要调用 base 的 Load 方法，因为基类的方法是空的，调用了也没毛用。 在 Timer 对象调用的方法（OnTimer）中，先调用 Load 方法，再调用 OnReload 方法。这样就可以在加载数据后触发更改通知。 然后实现 IConfigurationSource 接口，提供 MyConfigurationProvider 实例。 默认的配置源有JSON文件、命令行、环境变量等，为了排除干扰，便于查看效果，在 Main 方法中咱们先把配置源列表清空，再添加咱们自定义的配置源。 这里你可能会有个疑问：连接字符串硬编码了不太好，要不写在配置文件中，可是，写在JSON文件中咱们怎么获取呢？毕竟 ConfigurationProvider 不使用依赖注入。 IConfigurationSource 不是有个 Build 方法吗？Build 方法不是有个参数是 IConfigurationBuilder 吗？用它，用它，狠狠地用它。 ----------------------------------------------------------------------------------------------------- 接下来，咱们弄个一对多的例子。逻辑是这样的：启动程序显示主窗口，接着创建五个子窗口。主窗口上有个大大的按钮，点击后，五个子窗口会收到通知。大概就这个样子： 窗口上只放了一个文本框。上面代码中，使用了 ChangeToken.OnChange 静态方法，为 Change Token 注册回调委托，本例中回调委托绑定的是 OnCallback 方法，也就是说：当 Change Token 触发后会在文本框中追加文本。OnChange 静态方法有两个重载： 上述例子用的是第一个，其实里面调用的也是第二个重载，只是把咱们传递的 OnCallback 方法当作 TState 传进去了。 请大伙伴暂时记住 changeTokenProducer 和 changeTokenConsumer 这两参数。changeTokenProducer 也是一个委托，返回 IChangeToken。用的时候一定要注意，每次触发之前，Change Token 要先创建新实例。注意是先创建新实例再触发，否则会导致无限。尽管内部会判断 HasChanged 属性，可问题是这个判断是在注册回调之后的。这个是跟 Change Token 的清奇逻辑有关，咱们看看 OnChage 的源代码就明白了。 简单来说，就是返回一个 ChangeTokenRegistration 实例，这是个私有类，咱们是访问不到的，以 IDisposable 接口公开。其中，它有两个方法是递归调用的： 在 ChangeTokenRegistration 类的构造函数中，先调用 RegisterChangeTokenCallback 方法，开始了整个递归套娃的过程。在 RegisterChangeTokenCallback 方法中，为 token 注册的回调就是调用 OnChangeTokenFired 方法。 而 OnChangeTokenFired 方法中，是先获取新的 Change Token，再触发旧 token。最后，又调用 RegisterChangeTokenCallback 方法，实现了无限套娃的逻辑。 因此，咱们在用的时候，必须先创建新的 Change Token 实例，然后再调用 RegisterChangeTokenCallback 实例的 Cancel 方法。不然这无限套娃会一直进行到栈溢出，除非你提前把 ChangeTokenRegistration 实例 Dispose 掉（由 OnChange 静态方法返回）。可是那样的话，你就不能多次接收更改了。 下面就是主窗口部分，也是最危险的部分——必须按照咱们上面分析的顺序进行，不然会 Stack Overflow。 按钮1的单击事件处理方法就是触发点，所以，CancellationTokenSource、CancellationChangeToken 要先换成新的实例，然后再用旧的实例去 Cancel。这里用 Interlocked 类会好一些，毕竟要考虑异步的情况，虽然咱这里都是在UI线程上传递的，但还是遵守这个习惯好一些。 这样处理就能避免栈溢出了。运行后，先打开五个子窗口（多点击一次就能创建十个子窗口）。接着点击大大按钮，五个子窗口就能收到通知了。",https://www.cnblogs.com/tcjiaan/p/18012397,2025-03-13 10:54:25
liyongqiang-cc,未知作者,在构建高性能的 .NET Core 应用时，缓存是提升系统响应速度、减轻数据库压力的利器。然而，缓存并非一蹴而就，它也需要“热身”才能发挥最佳性能。这就是缓存预热的意义所在。 缓存预热，顾名思义，就是在系统启动或缓存失效后，主动将热点数据加载到缓存中，避免用户首次请求时直接访问数据库，造成性能瓶颈。 .NET Core 提供了多种缓存机制，例如内存缓存、分布式缓存等。我们可以根据不同的缓存类型，选择不同的预热方式。 缓存预热是提升 .NET Core 应用性能的重要手段。通过合理的预热策略，可以有效提升用户体验、降低数据库压力、提高系统稳定性。希望本文能帮助你更好地理解和应用缓存预热技术。,https://www.cnblogs.com/liyongqiang-cc/p/18737994,2025-03-13 10:54:34
.NET研究社,未知作者,"目前国产化系统浪潮下，适配鸿蒙是中国软件大势所趋，.NET作为最适合开发客户端语言之一，适配鸿蒙系统(HarmonyOS Next)是目前.NET开发者最关心的事情。我目前业余时间正在移植Avalonia到HarmonyOS，去年在.NET Conf CN上分享过，目前又取得一点进展，所以本文把所有问题进行整合与大家进行分享。 自HarmonyOS 5.0.0(12)起，禁止匿名内存申请可执行权限，除系统内置的JavaScript引擎外，其他虚拟机不能使用Jit功能，所以无法将CoreCLR接入到鸿蒙系统中，而最新版的Mono虽然支持解释执行，但是由于性能问题也不会接入Mono到鸿蒙系统，最终只能选择接入NativeAOT运行时。 支撑鸿蒙可以接入NativeAOT的原理是鸿蒙系统兼容libc是musl的Linux系统的动态库(.so)。而.NET的RID支持linux-musl-arm64/linux-musl-x64，所以理论上可以将.NET程序编译为原生的Linux动态库(.so)，然后在鸿蒙的原生项目中，通过dlopen以及dlsym等函数调用C#中的入口函数。 而C#调用鸿蒙api则通过P/Invoke调用鸿蒙的NDK，而ArkUI的TypeScript api则通过NDK中的napi调用。 具体做法可以参考我正在做的Avalonia移植项目: https://github.com/OpenHarmony-NET/OpenHarmony.Avalonia 鸿蒙系统使用了seccomp限制危险的syscall调用。标准posix下，如果系统不支持某个syscall则返回错误码，而seccomp非常激进，如果调用了非法的sycall则直接杀掉进程。.NET的运行时初始化时，会调用__NR_get_mempolicy系统调用对numa支持进行检查，而这个系统调用不在鸿蒙的seccomp白名单中，所以导致直接宕机。 鸿蒙系统中seccomp的系统调用白名单如下：https://gitee.com/openharmony/startup_init/blob/master/services/modules/seccomp/seccomp_policy/app.seccomp.policy 其实安卓中也有类似的限制，.NET的NativeAOT之所以能在安卓平台下运行是因为.NET中对安卓进行了特殊处理，而在鸿蒙平台我们使用的是Linux平台的代码，所以没有对这些系统调用进行处理。 解决上个问题后，.NET运行时初始化依然不能成功，导致程序崩溃，经过排查发现是GC初始化时会申请256G左右的虚拟内存，导致mmap返回Out Of Memory错误。 解决办法1：设置环境变量“DOTNET_GCHeapHardLimit”，将虚拟内存申请控制在约180G以下即可。 解决方案1：从Alpine上偷包 ，因为Alpine的libc是musl，所以理论上Alpine的库在鸿蒙上大部分都能使用。 阿里云Alpine软件包镜像地址: arm64架构：https://mirrors.aliyun.com/alpine/edge/main/aarch64/ amd64架构：https://mirrors.aliyun.com/alpine/edge/main/x86_64/ 鸿蒙的ICU配置文件路径与默认路径不同，需要调用修改环境变量API，将ICU_DATA修改为/system/usr/ohos_icu 且鸿蒙平台上libICU的大版本是72，要使用这个版本的库。 NativeAOT众所周知不支持跨平台编译，而我的方案需要发布到linux-musl平台，所以无法在Windows上发布，影响开发效率。 解决方案：在项目中引入项目https://github.com/OpenHarmony-NET/PublishAotCross Marshal.GetDelegateForFunctionPointer的实现依赖动态生成汇编，而HarmonyOS不支持动态生成汇编代码执行(Jit)，使用该函数会导致崩溃。 前文中提到部分问题的解决方案是修改源码，具体操作步骤如下： 修改完代码，执行以下命令进行编译(linux平台下，需要有编译环境)： 编译成功后，打开目录 运行时/artifacts/bin/coreclr/linux.arm64.Release/aotsdk，将这里所有的替换到自己电脑nuget的缓存目录, 例如C:\Users\用户名\.nuget\packages\runtime.linux-musl-arm64.microsoft.dotnet.ilcompiler\dotnet版本\sdk https://github.com/dotnet/runtime/issues/110074 https://github.com/dotnet/runtime/issues/111649",https://www.cnblogs.com/CeSun/p/18706813,2025-03-13 10:54:42
开发中对象命名的一点思考,未知作者,在我所见过的项目中，大多数团队都倾向于“功能堆砌式”开发：需求来了就加逻辑或函数，却很少有人愿意花时间在设计上，尤其是在对象命名花费时间。这看似“快速实现需求”的方式，通常会对代码的可读性产生坏的影响，进而影响可维护性。 一个好的对象命名并非只是让代码表面看起来整洁；它背后关系到人类和 AI 对系统的认知方式，也会在后续维护和迭代中形塑程序员以及 AI 工具的行为。换句话说，合适的命名不但决定了当前代码的优雅程度，也会潜移默化地影响未来对代码进行修改、扩展和重构时的思维路径与决策过程。 下面，我们将通过几个示例，探讨如何通过合理的命名让对象真正体现业务含义与自主决策能力，而不是简单地扮演一个“被动执行者”。 想象你走进一家餐厅。你会如何点餐？是要一份宫保鸡丁，还是告诉厨师：“你先放油，然后大火爆炒，然后再加调料”？如果你选择后者，那么对应到程序设计中，大概就是下面的写法： FoodMaker ，它只是一个“做饭的人”或“执行器”，缺乏更多的“主观能动性”。这不仅增加了使用者在思维层面的负担，也显得对专业厨师的经验和技能不够尊重。 相比之下，更贴近现实的方式是直接告诉对方“我需要一份宫保鸡丁”，厨师会根据自身经验和对食材的理解来完成这道菜。例如： 这样，Chef 完整体现了专业性与自主性： 值得注意的是，很多以 “-er” 或 “-or” 结尾的对象（如 Manager、Processor、Controller、Validator 等）常常会呈现出类似的“过程化”倾向：它们更多体现的是过程集合而非业务主体。当我们把命名改为更能传达专业身份或业务角色的名词时，往往会看到对象的“自我意识”和“主观能动性”随之提升，从而让整个系统的抽象层次更高、可维护性更好。 在 AI 辅助编程工具的普及下，命名的清晰度在大型项目中显得尤为重要。模糊或过于抽象的名字不仅会增加团队成员、甚至让3个月后的你自己的认知负担，也会让 AI 在大量上下文中难以理解该对象的真实意图，甚至产生误导性补全。我们先来看一个不恰当的示例： 要让代码更易读、更具可维护性，也让 AI 分析更准确，我们可以将“餐厅运营”按照现实场景拆分成多个专门对象，让每个类名更能“自证其职”： 现实生活中，若宫保鸡丁的必需食材（例如花生）突然缺货，真正的专业厨师会主动寻找替代食材，而不会要求顾客重新“下指令”或“换个点餐方式”。同理，在软件中，一个设计得足够“智能”的对象，也应该能在外部条件或业务需求变化时，自行调整内部逻辑，而不影响调用者的使用方式。 局限： 下面的 Chef 内部自行决定花生是否可用，如果缺货就用其他食材替代。这样，即使后续有更多类似变化（换新调料、临时供应商等），也能集中在 Chef 内部调整，无需改动客户端调用代码。 好处： 无论花生库存如何，客户端调用方式始终一致：一个智能的 Chef 能在内部完成相应的“自适应”处理。 对象的命名，许多人往往只把它当作“好看”或“顺口”的问题，却忽视了它所暗含的业务理解深度与系统思维。当我们刻意避开 “-er” 结尾或“Service”、“utility”后缀等含糊标签，并让对象名真实反映其专业角色与业务职责时，能够有下述好处： 在软件工程中，命名的重要性通常被忽视，但命名本身潜移默化影响我们的编码时的思维。只有当我们真正将对象视为拥有“尊严”与“自主决策能力”的实体时，才能更容易构建出一个高内聚、易扩展、符合业务本质的系统。,https://www.cnblogs.com/CareySon/p/18711135,2025-03-13 10:54:39
apocelipes,未知作者,"这里给定一个可以概率均等地生成0到6的随机数生成器，要求用这个生成器创造出能概率均等地生成0到4的随机数生成器。 然而这个答案只满足了输出的范围在0到4，不满足概率均等，所以不正确。这种时候列表法的作用就显现出来了： 通过列表法我们其实也发现了这个问题真正的解法：除掉5和6的话剩下的输出不仅符合要求概率也是均等的，所以代码会变成这样： 上面的代码其实就是随机采样算法中非常重要的一种：拒绝采样。同样上面的rand7生成rand5也可以归类成一大类问题：给定一组满足规律或者特征是g(x)的样本，现在需要从这些样本中筛选出或者生成另一组满足特征是f(x)的样本。解决这类问题的算法很多，而拒绝采样是比较直观的：判断g(x)的样本是否符合要求，不符合的就排除取下一个样本，符合要求的就归类到满足f(x)的样本集合中。 拒绝采样在大多数时间都能获得理想的结果，但还有采样率需要注意。采样率就是g(x)的样本中有多少可以被接受，采样率太低的时候意味着算法的效率也会非常低。所以我们简单算算rand5的采样率，是七分之五，大约70%。这个概率不大不小，勉强合适。 标准库里当然不会有rand5和rand7，但它提供了一个叫Int63n的函数，它解决的问题是如何从均匀分布在[0, 2⁶⁴)范围上的随机整数中生成均匀分布在范围[0, n)上的随机整数。换句话说，虽然范围不一样了，但还是同一个问题。 我们肯定不能像上面那样把大于等于n的样本全丢了，因为2⁶⁴包含至少1844京（1E16）个整数样本，这会带来低得无法接受的采样率。 但因为我们是用mod来选择范围内的随机数的，因此我们可以选择n的倍数，这个证明太简单了，列表法加归纳就行。或者还可以这么想，有一个整数常数C，x % n和Cx % n能产生的输出的种类和它们的数量都是完全相同的，所以如果样本能均匀分布在[0, n)的范围上，那么范围[0, C·n]只不过是[0, n)重复了C次，所以样本在每一段上都是均匀分布的，整个合起来的区间上也是均匀的。 C其实也很好选择，取一个2⁶⁴内的n的最大的倍数就行，如果C本身能被2⁶⁴整除，那么C就是2⁶⁴/n。 代码还是很简单的，超过C·n的样本全部拒绝采样，剩下的样本就能保证在mod n的时候获得分布均匀的随机整数了。 采样率是多少？我们可以利用拒绝率来反推，这里拒绝率还挺好算的，就是(1 > 64来避免了x % n带来的除法运算。而且新算法不用一开始就算余数，因此运气好的时候可以一次除法都不做。 还有一个小疑问，128位乘法够了吗？肯定够了，因为n最大也只能取到2⁶⁴，这意味这x*n的范围最大也只到[0, 2⁶⁴·2⁶⁴)，128位乘法刚好够用。 最后做下性能测试，标准库里已经提供了，在我的10代i5上旧算法一次调用需要18ns，新算法只需要5ns，两者使用的随机数发生器是一样的，因此可以说新算法快了3倍，提升还是很可观的。 上一节讨论了从更大的样本空间里筛选出特定特征的子集，这一节我们反过来：从范围更小的样本空间里派生出有某一特征的超集。 首先要考虑的是如何把受限的样本空间尽量扩张。上一节我们用乘法来扩展了样本分布的范围，然而乘法尤其是乘以常数是没法增加样本数量的，因此这个做法只能pass。加法可以平移样本的范围，但也不能增加样本总量，而且我们需要样本空间是[0, x)平移之后起点都变了，因此也不行。 那剩下的可行的也最稳定的办法是rand5() * rand5()。它像乘法一样能扩张样本的范围，同时因为不是乘以常数因此还有机会产生新的样本。我们列个表看看： 没错，正好产生了均等分布的0到24的整数。很神奇吧，其实想明白为什么不难。我们先看5 * rand5，这样或产生0、5、10、15、20这五个数字，我们要想有机会生成连续的整数，就一定需要把缺少的1到4，11到14这些数字补上。这时候正巧一个+ rand5就可以把这些缺的空洞全部填上。当然用进位来理解会更简单。 总结: n * randn + randn可以产生连续的范围在[0, n*n)的均匀分布的整数。注意这里没有结合律，因为randn每次的结果都是不一样的 这个样本空间是远超rand7的要求的，因此现在问题回到了第一节：如何从rand25生成rand7？现在大家都知道了： rand.Uint64()返回的随机数有足足64bits，而我们通常不需要这么大的随机数，举个例子，假如我们只需要0到15的随机数，这个数字只需要4bits就够了，如果用rand.Uint64N(16)来生成，我们会浪费掉60bits的数据。 为啥这么说？因为rand.Uint64()保证能概率均等的生成[0, 2⁶⁴)范围内的整数，这其实说明了两件事，第一是这个随机数的每一个bit也都是随机的，这个很明显；第二个是每个bits是0还是1的概率也是均等的，这个可以靠列表加归纳法得出。我们来看看第二点这么证明。 不难注意到，三个bit各有八种输出，0占四种，1占剩下四种，0和1的概率均等。这个结论能推广到任意的[0, 2^n)范围上。 同样，基于这个结论，我们还能得到这样一个结论，任意连续的n个bit，都能产生均匀分布在[0, 2^n)上的随机数，这个证明太简单了，所以我们集中注意力就行了。 现在回头看看为什么我说会浪费60bits，因为根据上面的结论，我们64位的随机整数完全可以按每四位划分一次，这样可以分成16组，而每组正好能产生[0, 16)范围内的随机数，且概率同样的均等的。也就是说一次rand.Uint64()理论上应该可以产生16个我们需要的随机数，但实际上我们只生成了一个。这里就有很大的提升空间了。 到目前为止还不错，如果你不在乎生成的随机数的概率分布或者你只想生成[0, 2^n)范围的随机数且这个n可以整除64，那么可以直接跳到下一节继续看了。 接着往下看的人肯定是希望不管在什么范围内都能生成概率均匀的随机数且尽量多利用已生成的随机bits的。但事情往往不尽人意，比如，[0, 13)和[0, 7)就是两个例子。前者右边界不是2的幂，后者虽然是2的幂但3不能整除64。 我们先从简单的问题开始解决，[0, 13)。受先表示数字12至少得用4个bit，4能整除64，所以我们还可以每4个连续的bit分割成一组，但这时概率分布均匀的条件是满足不了的。无法保证的原因很简单，和我们第一节里说的“rand7生成rand5”的情况一样，每个均匀分割出来的一组连续的bits的组合里有我们不需要的样本存在。处理这个情况的方法在第一节里已经有表述了，那就是拒绝采样。确定拒绝的范围也使用第一节说到的办法，注意到每一组bits能生成[0, 16)的随机数，再考虑到13本身是素数，这里只需要简单地把≥13的样本全部剔除即可。 如果一组不满足采样要求，我们就跳过直接去下一组，因此有可能16组里无法获得足够的随机数，因此我们得重新获取一次64位的随机数，然后再次进入分割计算。这么做会对性能产生一点点负面影响，但依旧很快： 上面那种情况还是最简单的，但[0, 7)就不好办了。首先表示6需要至少3个bit，而3不能整除64，其次6也不是2的幂。这个怎么处理呢？ 第一个想法是，既然3不能整除64，那我们选个能被3整除的，这里是63，也就是说超过2⁶³-1的样本全部丢弃，然后把符合要求的样本按每连续的3bits进行分割。这样我们先保证了3bits分割出来的每一组都能均等的生成[0, 8)范围内的随机整数。现在问题转化成了“rand8怎么生成rand7”，这题我们会做而且做了好多回了，最终代码会是这样： 代码变得很长也很复杂，而且需要两步拒绝采样，相对的我们一次也能分割出21组，比4bits的时候多了5组，所以难说性能下降还是不变，因此我们看看测试： 第二种想法只需要一步拒绝采样，既然3不能整除64，那么就找到一个离3最近的可以整除64且大于3的整数。在这里我们可以直接注意到4符合条件，实际开发中如果要找到任意符合条件的数，可以依赖一下线性探测。现在我们按连续的4位把64位随机整数分割，这样分割出来的每一组可以生成均匀分布在[0, 16)上的整数。然后问题变成了“从rand16生成rand7”。代码这样写： 代码简单了，也只需要一步拒绝采样就行，但问题在于每一组的生成范围变大导致我们不得不使用取模操作。看看性能怎么样： 想法2比想法1快了近20%，看来两步拒绝采样成了硬伤，不仅仅是因为多获取几次64位随机数更慢，多出来的一个if还可能会影响分支预测，即便最后我们多了5组可以采样也无济于事了。 讨论了半天概率均匀分布的情况，但业务中还有一种常见场景：一组样本进行采样，既要有随机性，又要样本之间在统计上尽量满足某些比例关系。 这个场景我想大家最先想到的应该是抽奖。是的没错，这是带权重随机数的常见应用。但还有一个场景，一个负载均衡器连接着一组权重不同的服务器硬件，现在想要尽量按权重来分配链接，这时候带权重随机数就有用武之地了。 假设我们有样本(1, 2, 3, 4)四个整数，然后要按1:2:3:4的比例来随机生成样本，该怎么做呢？ 按比例，我们可以得到1，2，3，4生成的概率是0.1，0.2，0.3，0.4，这些概率加起来是一定等于1的，所以我们不妨来想象有一个数轴上的0到1的区间，然后我们把这些比例“塞进”数轴里： 我故意打乱了顺序，实际上顺序并不影响结果。每个样本可以有一个数轴上的范围，范围的长度之间也是符合比重的，因此当存在一个可以均匀生成[0, 1)之间所有实数的随机数生成器时，这个生成器生成的数落在哪个范围里，我们就选择生成这个范围对应的样本，举个例子，如果生成的实数落在[0.0, 0.4)这个区间里，那么就生成样本“4”，如果落在[0.8, 1.0)这个区间，就生成“2”。这样带权重的随机数就生成了。这个看上面那个图还是挺好理解的，我就不费笔墨去证明了。 但我不是很想用这个方法。为啥呢，因为你看到了，区间的左边是闭合的，这意味着要做浮点数的等值比较，虽然很简单，但我很懒不想多写，而且浮点数没法精确表示所有情况的比例导致我们区间的两端都有精度损失，这就需要考虑误差率，虽然通常这点精度损失带来的误差可以忽略不记（尤其是在统计意义上）但只是考虑到这点我就浑身难受了。 所以我要找一种不需要浮点数的解决方案。想出来其实不难，假设我们有0到9的整数，正好10个，现在样本“1”按比例需要占用其中1个样本，样本“4”按比例要占用其中四个样本，现在我们获得了一个能均匀生成0到9的整数的随机数生成器，那只要生成的随机数正好是样本“4”占用的那几个随机数我们就生成“4”，生成的随机数是样本“1”占用的那就生成“1”。可以看到只要占够一定数量的不同的样本，那么我们一样能生成带权重的随机数。 下面有几个问题，一是样本总数怎么确定，这个简单，每个比例当成整数相加即可，比如1:2:3:4就是1+2+3+4=10，2:2:3:5就是2+2+3+5=12，依此类推。如果比例是实数呢？2.3 : 3.4 : 4.7怎么办？这就要用到比例的性质了，等比扩大后比例不变，所以我们每个实数都乘以10，然后去掉小数点后的0全部当成整数，所以23+34+47=104，理论上任意比例都能这么整，不过整数最终有大小限制的，你总不能生成个随机数还用big.Int吧，所以注意总和别超过整数范围限制。 二是样本的范围怎么算，虽然我们只需要不相同的满足1里总数的离散的点就行，但为了方便计算我们还是选择连续的整数比较好，所以范围限定为[0, sum-1]。这样我们能直接利用rand.Uint64N()来生成需要的随机数生成器。 最后我们只要让样本按比例随机占领一些连续的整数就行了。而且我们只需要记录右边界就够了，我们从范围是[0, n]的第一个样本开始比较，如果生成器给出的随机数小于等于某个右边界，那它一定落在边界代表的样本上（因为是从最小的边界开始比较的，所以随机数必然不可能落在前一个范围的样本上）。 首先是数据结构，WeightRandom是我们的带权重随机数生成器，upperBoundary是样本数量的总和，entries则是各个样本和样本占领的连续整数的右边界。 lowerBoundary用来统计有多少样本，我们最终选择了左闭右开的区间，这样方便算。rndMap的key是样本，value则是比例。当样本的范围计算并保存结束之后，我们需要按照右边界从小到大排序这些样本，因为后面的查找范围到样本的对应关系需要右边界满足从小到大的顺序。 查找时先生成一个范围在[0, upperBoundary)之间的随机数，然后我们从最小的边界开始逐一比较，一旦发现比自己大的边界，那么就说明需要生成边界对应的样本。底部那句panic如字面意思，理论上是执行不到的，但go不知道，我们要么返回个空值要么panic，考虑到能走到这里那说明我们的程序或者标准库的代码有重大bug，panic了去调试才是比较好的办法。 根据upperBoundary的大小，实际上我们还能复用上一节充分利用每一个bit的办法，不需要每次都生成新的随机数，等分割出来的分组都消耗完了再生成，这样可以大大加速这个函数。不过为了通用性和尽量简化代码，我就不这样写了。 我们按权重生成“abcd”四个字母，比例是15:30:45:60，简化一下就是1:2:3:4，所以理论上概率应该接近10%，20%，30%和40%。不过统计上的概率总是有点误差的，只要大致趋势接近于这个比例就行了。我们运行100亿次来看看结果： 非常符合预期。作为一项优化措施，我们可以利用类似二分查找的办法来定位样本，因为右边界本身是有序的，这可以显著改善在有大量样本时的性能表现。不过如果你的样本不超过10个的话我觉得现在这样的线性查找就足够了。 因为限制更少，所以在通用的工具库里用浮点数方案的人更多，但业务场景和通用工具库是不一样的，很多时候选整数方案也没啥问题，最终你应该选择一个符合业务需求并且自己和同事都看得懂的方案。 至于性能怎么样，浮点数方案的查找过程和整数方案是一样的，性能需要做一次完整的测试才能看出孰高孰低，我不好在这凭空幻想。当然测试我就不做了，我偷个懒。 “种子”其实是指一些伪随机数生成算法需要的一些初始状态，这些算法会根据初始状态来生成一系列有随机性的数值序列。 所以相同的种子通常会带来相同的序列，这时候虽然每个序列都有随机性，但两个序列之间是没有随机性的——有了其中一个序列就可以精准预测另一个序列的排列。具体表现很可能会是你编写了一个游戏，其中敌人会随机采取一些行动，然而因为种子没设置好，导致每次见到这个敌人的时候它都会采取一模一样的行动步骤，这样的游戏是极其无聊的。 种子通常只用设置一次，并且在程序第一次需要随机数的地方设置——理想情况是这样的，然而总是有倒霉蛋忘记了这一点，于是随机算法经常只能使用默认的低质量且相同的种子。所以比较现代的语言，比如go1.22和python3都选择了在程序刚开始运行的时候帮你自动设置种子。 那作为开发者我用啥呢？一般来说系统时间是够用了，自己写着完或者做些简单工具可以用这个，不过要记住系统时间是可修改的不可靠的。如果是库或者对随机性依赖比较重的比如游戏，/dev/urandom是个比较理想的选择。追求极致性能并且对种子质量要求没那么高时，像谷歌那样利用ASLR带来的随机值也是可以的。 golang实际上是先利用auxv，如果系统不支持，就回退到从urandom之类的随机设备读取随机值，这个也出问题了就使用系统时间： 这个是全局函数的设置，go还能自己创建rand.Source，这个的种子只能显式传进去，这时候传什么go就没法管了，灵活的同时牺牲了一定的安全性。 Python3则是先读取urandom，失败后会结合系统时间加当前进程pid来生成种子，这样比单使用系统时间要强： 然后这个函数会被Random对象的__init__方法调用，如果初始化一个Random对象但不传seed参数，那么就会进行默认设置。而random模块里所有的方法其实都是由一个全局的Random()对象提供的，因为没传seed进去，所以代码里会自动设置seed： 概率和统计对程序设计的影响是很大的，所以我觉得与其花时间看最近比较火的微分方程，稍微抽出点时间看看概率统计对自己的帮助可能更大。 最后，其实标准库还有各种第三方库已经贴心准备了几乎全套功能了，看懂文档就能放心用，而且我也更推荐用这些库，开源的且久经检验的代码始终是比自己闭门造车来的强。 https://stackoverflow.com/questions/18394733/generating-a-random-number-between-1-7-by-rand5 https://www.linuxquestions.org/questions/linux-kernel-70/what-does-proc-pid-auxv-mean-exactly-4175421876/",https://www.cnblogs.com/apocelipes/p/18281363,2025-03-13 10:54:19
Chat2DB 3.0 发布：AI 能力脱胎换骨，全方位的 SQL 开发助手,未知作者,AI 助手不仅支持生成 SQL，还能直接执行展示查询到的结果，所见即所得，进一步提升开发与数据分析效率。 这次真不是简单的 SQL 工具了！而是升级为全方位的开发助手。它可以使用代码模型，基于数据库表结构直接生成 Python、Java 等多种语言的代码，还能通过 推理模型 提供各种技术难题的解决思路，助你轻松应对复杂的技术挑战。 ✅ SQL 准确性评估：通过高级算法预测并评估SQL语句的正确性，防止跨库关联或权限问题导致的执行失败 新版本中，Chat2DB 正式接入了 DeepSeek-V3 和 DeepSeek-R1，同时新增了对 CodeGeeX、通义千问 Qwen、GLM-4 等模型的支持，涵盖代码模型、推理模型和混合模型等 10+ 先进模型。 对于数据库管理者来说，清晰的数据结构是高效工作的基础。本次升级中，Chat2DB 新增了 ER 图功能，用户只需一键操作，即可直观查看数据库的结构和关系。 Chat2DB 的 AI 功能已全面覆盖 所有类型的数据库，包括之前未支持的 MongoDB、Redis 等。无论你使用的是关系型数据库还是非关系型数据库，现在都可以通过 AI 快速生成 SQL。 查询结果集优化：支持 Cmd/Ctrl+S 修改数据、Cmd/Ctrl+R 刷新数据，滚动条体验更流畅。 数据库兼容性提升：修复 Oracle、SQL Server、DM 等数据库的语法和展示问题，确保操作精准无误。 修复 SQL Server 的 TIME、DATETIME2、DATETIMEOFFSET 类型在修改表操作时生成的 SQL 错误。 在过去的不到一年里，我们支持了数百个用户需求，平均每周发布一个版本，持续优化迭代，始终将用户体验放在首位。感谢每一位用户的信任与支持！！！ 最后的最后，小编再告诉大家一个好消息：为了满足企业对数据安全和定制化的需求，我们现已正式推出 Chat2DB 企业私有化部署方案，如果有兴趣了解或咨询详情，欢迎扫描下方二维码添加客服微信获取报价！,https://www.cnblogs.com/cmt/p/18765612,2025-03-13 10:54:36
apocelipes,未知作者,"这里给定一个可以概率均等地生成0到6的随机数生成器，要求用这个生成器创造出能概率均等地生成0到4的随机数生成器。 然而这个答案只满足了输出的范围在0到4，不满足概率均等，所以不正确。这种时候列表法的作用就显现出来了： 通过列表法我们其实也发现了这个问题真正的解法：除掉5和6的话剩下的输出不仅符合要求概率也是均等的，所以代码会变成这样： 上面的代码其实就是随机采样算法中非常重要的一种：拒绝采样。同样上面的rand7生成rand5也可以归类成一大类问题：给定一组满足规律或者特征是g(x)的样本，现在需要从这些样本中筛选出或者生成另一组满足特征是f(x)的样本。解决这类问题的算法很多，而拒绝采样是比较直观的：判断g(x)的样本是否符合要求，不符合的就排除取下一个样本，符合要求的就归类到满足f(x)的样本集合中。 拒绝采样在大多数时间都能获得理想的结果，但还有采样率需要注意。采样率就是g(x)的样本中有多少可以被接受，采样率太低的时候意味着算法的效率也会非常低。所以我们简单算算rand5的采样率，是七分之五，大约70%。这个概率不大不小，勉强合适。 标准库里当然不会有rand5和rand7，但它提供了一个叫Int63n的函数，它解决的问题是如何从均匀分布在[0, 2⁶⁴)范围上的随机整数中生成均匀分布在范围[0, n)上的随机整数。换句话说，虽然范围不一样了，但还是同一个问题。 我们肯定不能像上面那样把大于等于n的样本全丢了，因为2⁶⁴包含至少1844京（1E16）个整数样本，这会带来低得无法接受的采样率。 但因为我们是用mod来选择范围内的随机数的，因此我们可以选择n的倍数，这个证明太简单了，列表法加归纳就行。或者还可以这么想，有一个整数常数C，x % n和Cx % n能产生的输出的种类和它们的数量都是完全相同的，所以如果样本能均匀分布在[0, n)的范围上，那么范围[0, C·n]只不过是[0, n)重复了C次，所以样本在每一段上都是均匀分布的，整个合起来的区间上也是均匀的。 C其实也很好选择，取一个2⁶⁴内的n的最大的倍数就行，如果C本身能被2⁶⁴整除，那么C就是2⁶⁴/n。 代码还是很简单的，超过C·n的样本全部拒绝采样，剩下的样本就能保证在mod n的时候获得分布均匀的随机整数了。 采样率是多少？我们可以利用拒绝率来反推，这里拒绝率还挺好算的，就是(1 > 64来避免了x % n带来的除法运算。而且新算法不用一开始就算余数，因此运气好的时候可以一次除法都不做。 还有一个小疑问，128位乘法够了吗？肯定够了，因为n最大也只能取到2⁶⁴，这意味这x*n的范围最大也只到[0, 2⁶⁴·2⁶⁴)，128位乘法刚好够用。 最后做下性能测试，标准库里已经提供了，在我的10代i5上旧算法一次调用需要18ns，新算法只需要5ns，两者使用的随机数发生器是一样的，因此可以说新算法快了3倍，提升还是很可观的。 上一节讨论了从更大的样本空间里筛选出特定特征的子集，这一节我们反过来：从范围更小的样本空间里派生出有某一特征的超集。 首先要考虑的是如何把受限的样本空间尽量扩张。上一节我们用乘法来扩展了样本分布的范围，然而乘法尤其是乘以常数是没法增加样本数量的，因此这个做法只能pass。加法可以平移样本的范围，但也不能增加样本总量，而且我们需要样本空间是[0, x)平移之后起点都变了，因此也不行。 那剩下的可行的也最稳定的办法是rand5() * rand5()。它像乘法一样能扩张样本的范围，同时因为不是乘以常数因此还有机会产生新的样本。我们列个表看看： 没错，正好产生了均等分布的0到24的整数。很神奇吧，其实想明白为什么不难。我们先看5 * rand5，这样或产生0、5、10、15、20这五个数字，我们要想有机会生成连续的整数，就一定需要把缺少的1到4，11到14这些数字补上。这时候正巧一个+ rand5就可以把这些缺的空洞全部填上。当然用进位来理解会更简单。 总结: n * randn + randn可以产生连续的范围在[0, n*n)的均匀分布的整数。注意这里没有结合律，因为randn每次的结果都是不一样的 这个样本空间是远超rand7的要求的，因此现在问题回到了第一节：如何从rand25生成rand7？现在大家都知道了： rand.Uint64()返回的随机数有足足64bits，而我们通常不需要这么大的随机数，举个例子，假如我们只需要0到15的随机数，这个数字只需要4bits就够了，如果用rand.Uint64N(16)来生成，我们会浪费掉60bits的数据。 为啥这么说？因为rand.Uint64()保证能概率均等的生成[0, 2⁶⁴)范围内的整数，这其实说明了两件事，第一是这个随机数的每一个bit也都是随机的，这个很明显；第二个是每个bits是0还是1的概率也是均等的，这个可以靠列表加归纳法得出。我们来看看第二点这么证明。 不难注意到，三个bit各有八种输出，0占四种，1占剩下四种，0和1的概率均等。这个结论能推广到任意的[0, 2^n)范围上。 同样，基于这个结论，我们还能得到这样一个结论，任意连续的n个bit，都能产生均匀分布在[0, 2^n)上的随机数，这个证明太简单了，所以我们集中注意力就行了。 现在回头看看为什么我说会浪费60bits，因为根据上面的结论，我们64位的随机整数完全可以按每四位划分一次，这样可以分成16组，而每组正好能产生[0, 16)范围内的随机数，且概率同样的均等的。也就是说一次rand.Uint64()理论上应该可以产生16个我们需要的随机数，但实际上我们只生成了一个。这里就有很大的提升空间了。 到目前为止还不错，如果你不在乎生成的随机数的概率分布或者你只想生成[0, 2^n)范围的随机数且这个n可以整除64，那么可以直接跳到下一节继续看了。 接着往下看的人肯定是希望不管在什么范围内都能生成概率均匀的随机数且尽量多利用已生成的随机bits的。但事情往往不尽人意，比如，[0, 13)和[0, 7)就是两个例子。前者右边界不是2的幂，后者虽然是2的幂但3不能整除64。 我们先从简单的问题开始解决，[0, 13)。受先表示数字12至少得用4个bit，4能整除64，所以我们还可以每4个连续的bit分割成一组，但这时概率分布均匀的条件是满足不了的。无法保证的原因很简单，和我们第一节里说的“rand7生成rand5”的情况一样，每个均匀分割出来的一组连续的bits的组合里有我们不需要的样本存在。处理这个情况的方法在第一节里已经有表述了，那就是拒绝采样。确定拒绝的范围也使用第一节说到的办法，注意到每一组bits能生成[0, 16)的随机数，再考虑到13本身是素数，这里只需要简单地把≥13的样本全部剔除即可。 如果一组不满足采样要求，我们就跳过直接去下一组，因此有可能16组里无法获得足够的随机数，因此我们得重新获取一次64位的随机数，然后再次进入分割计算。这么做会对性能产生一点点负面影响，但依旧很快： 上面那种情况还是最简单的，但[0, 7)就不好办了。首先表示6需要至少3个bit，而3不能整除64，其次6也不是2的幂。这个怎么处理呢？ 第一个想法是，既然3不能整除64，那我们选个能被3整除的，这里是63，也就是说超过2⁶³-1的样本全部丢弃，然后把符合要求的样本按每连续的3bits进行分割。这样我们先保证了3bits分割出来的每一组都能均等的生成[0, 8)范围内的随机整数。现在问题转化成了“rand8怎么生成rand7”，这题我们会做而且做了好多回了，最终代码会是这样： 代码变得很长也很复杂，而且需要两步拒绝采样，相对的我们一次也能分割出21组，比4bits的时候多了5组，所以难说性能下降还是不变，因此我们看看测试： 第二种想法只需要一步拒绝采样，既然3不能整除64，那么就找到一个离3最近的可以整除64且大于3的整数。在这里我们可以直接注意到4符合条件，实际开发中如果要找到任意符合条件的数，可以依赖一下线性探测。现在我们按连续的4位把64位随机整数分割，这样分割出来的每一组可以生成均匀分布在[0, 16)上的整数。然后问题变成了“从rand16生成rand7”。代码这样写： 代码简单了，也只需要一步拒绝采样就行，但问题在于每一组的生成范围变大导致我们不得不使用取模操作。看看性能怎么样： 想法2比想法1快了近20%，看来两步拒绝采样成了硬伤，不仅仅是因为多获取几次64位随机数更慢，多出来的一个if还可能会影响分支预测，即便最后我们多了5组可以采样也无济于事了。 讨论了半天概率均匀分布的情况，但业务中还有一种常见场景：一组样本进行采样，既要有随机性，又要样本之间在统计上尽量满足某些比例关系。 这个场景我想大家最先想到的应该是抽奖。是的没错，这是带权重随机数的常见应用。但还有一个场景，一个负载均衡器连接着一组权重不同的服务器硬件，现在想要尽量按权重来分配链接，这时候带权重随机数就有用武之地了。 假设我们有样本(1, 2, 3, 4)四个整数，然后要按1:2:3:4的比例来随机生成样本，该怎么做呢？ 按比例，我们可以得到1，2，3，4生成的概率是0.1，0.2，0.3，0.4，这些概率加起来是一定等于1的，所以我们不妨来想象有一个数轴上的0到1的区间，然后我们把这些比例“塞进”数轴里： 我故意打乱了顺序，实际上顺序并不影响结果。每个样本可以有一个数轴上的范围，范围的长度之间也是符合比重的，因此当存在一个可以均匀生成[0, 1)之间所有实数的随机数生成器时，这个生成器生成的数落在哪个范围里，我们就选择生成这个范围对应的样本，举个例子，如果生成的实数落在[0.0, 0.4)这个区间里，那么就生成样本“4”，如果落在[0.8, 1.0)这个区间，就生成“2”。这样带权重的随机数就生成了。这个看上面那个图还是挺好理解的，我就不费笔墨去证明了。 但我不是很想用这个方法。为啥呢，因为你看到了，区间的左边是闭合的，这意味着要做浮点数的等值比较，虽然很简单，但我很懒不想多写，而且浮点数没法精确表示所有情况的比例导致我们区间的两端都有精度损失，这就需要考虑误差率，虽然通常这点精度损失带来的误差可以忽略不记（尤其是在统计意义上）但只是考虑到这点我就浑身难受了。 所以我要找一种不需要浮点数的解决方案。想出来其实不难，假设我们有0到9的整数，正好10个，现在样本“1”按比例需要占用其中1个样本，样本“4”按比例要占用其中四个样本，现在我们获得了一个能均匀生成0到9的整数的随机数生成器，那只要生成的随机数正好是样本“4”占用的那几个随机数我们就生成“4”，生成的随机数是样本“1”占用的那就生成“1”。可以看到只要占够一定数量的不同的样本，那么我们一样能生成带权重的随机数。 下面有几个问题，一是样本总数怎么确定，这个简单，每个比例当成整数相加即可，比如1:2:3:4就是1+2+3+4=10，2:2:3:5就是2+2+3+5=12，依此类推。如果比例是实数呢？2.3 : 3.4 : 4.7怎么办？这就要用到比例的性质了，等比扩大后比例不变，所以我们每个实数都乘以10，然后去掉小数点后的0全部当成整数，所以23+34+47=104，理论上任意比例都能这么整，不过整数最终有大小限制的，你总不能生成个随机数还用big.Int吧，所以注意总和别超过整数范围限制。 二是样本的范围怎么算，虽然我们只需要不相同的满足1里总数的离散的点就行，但为了方便计算我们还是选择连续的整数比较好，所以范围限定为[0, sum-1]。这样我们能直接利用rand.Uint64N()来生成需要的随机数生成器。 最后我们只要让样本按比例随机占领一些连续的整数就行了。而且我们只需要记录右边界就够了，我们从范围是[0, n]的第一个样本开始比较，如果生成器给出的随机数小于等于某个右边界，那它一定落在边界代表的样本上（因为是从最小的边界开始比较的，所以随机数必然不可能落在前一个范围的样本上）。 首先是数据结构，WeightRandom是我们的带权重随机数生成器，upperBoundary是样本数量的总和，entries则是各个样本和样本占领的连续整数的右边界。 lowerBoundary用来统计有多少样本，我们最终选择了左闭右开的区间，这样方便算。rndMap的key是样本，value则是比例。当样本的范围计算并保存结束之后，我们需要按照右边界从小到大排序这些样本，因为后面的查找范围到样本的对应关系需要右边界满足从小到大的顺序。 查找时先生成一个范围在[0, upperBoundary)之间的随机数，然后我们从最小的边界开始逐一比较，一旦发现比自己大的边界，那么就说明需要生成边界对应的样本。底部那句panic如字面意思，理论上是执行不到的，但go不知道，我们要么返回个空值要么panic，考虑到能走到这里那说明我们的程序或者标准库的代码有重大bug，panic了去调试才是比较好的办法。 根据upperBoundary的大小，实际上我们还能复用上一节充分利用每一个bit的办法，不需要每次都生成新的随机数，等分割出来的分组都消耗完了再生成，这样可以大大加速这个函数。不过为了通用性和尽量简化代码，我就不这样写了。 我们按权重生成“abcd”四个字母，比例是15:30:45:60，简化一下就是1:2:3:4，所以理论上概率应该接近10%，20%，30%和40%。不过统计上的概率总是有点误差的，只要大致趋势接近于这个比例就行了。我们运行100亿次来看看结果： 非常符合预期。作为一项优化措施，我们可以利用类似二分查找的办法来定位样本，因为右边界本身是有序的，这可以显著改善在有大量样本时的性能表现。不过如果你的样本不超过10个的话我觉得现在这样的线性查找就足够了。 因为限制更少，所以在通用的工具库里用浮点数方案的人更多，但业务场景和通用工具库是不一样的，很多时候选整数方案也没啥问题，最终你应该选择一个符合业务需求并且自己和同事都看得懂的方案。 至于性能怎么样，浮点数方案的查找过程和整数方案是一样的，性能需要做一次完整的测试才能看出孰高孰低，我不好在这凭空幻想。当然测试我就不做了，我偷个懒。 “种子”其实是指一些伪随机数生成算法需要的一些初始状态，这些算法会根据初始状态来生成一系列有随机性的数值序列。 所以相同的种子通常会带来相同的序列，这时候虽然每个序列都有随机性，但两个序列之间是没有随机性的——有了其中一个序列就可以精准预测另一个序列的排列。具体表现很可能会是你编写了一个游戏，其中敌人会随机采取一些行动，然而因为种子没设置好，导致每次见到这个敌人的时候它都会采取一模一样的行动步骤，这样的游戏是极其无聊的。 种子通常只用设置一次，并且在程序第一次需要随机数的地方设置——理想情况是这样的，然而总是有倒霉蛋忘记了这一点，于是随机算法经常只能使用默认的低质量且相同的种子。所以比较现代的语言，比如go1.22和python3都选择了在程序刚开始运行的时候帮你自动设置种子。 那作为开发者我用啥呢？一般来说系统时间是够用了，自己写着完或者做些简单工具可以用这个，不过要记住系统时间是可修改的不可靠的。如果是库或者对随机性依赖比较重的比如游戏，/dev/urandom是个比较理想的选择。追求极致性能并且对种子质量要求没那么高时，像谷歌那样利用ASLR带来的随机值也是可以的。 golang实际上是先利用auxv，如果系统不支持，就回退到从urandom之类的随机设备读取随机值，这个也出问题了就使用系统时间： 这个是全局函数的设置，go还能自己创建rand.Source，这个的种子只能显式传进去，这时候传什么go就没法管了，灵活的同时牺牲了一定的安全性。 Python3则是先读取urandom，失败后会结合系统时间加当前进程pid来生成种子，这样比单使用系统时间要强： 然后这个函数会被Random对象的__init__方法调用，如果初始化一个Random对象但不传seed参数，那么就会进行默认设置。而random模块里所有的方法其实都是由一个全局的Random()对象提供的，因为没传seed进去，所以代码里会自动设置seed： 概率和统计对程序设计的影响是很大的，所以我觉得与其花时间看最近比较火的微分方程，稍微抽出点时间看看概率统计对自己的帮助可能更大。 最后，其实标准库还有各种第三方库已经贴心准备了几乎全套功能了，看懂文档就能放心用，而且我也更推荐用这些库，开源的且久经检验的代码始终是比自己闭门造车来的强。 https://stackoverflow.com/questions/18394733/generating-a-random-number-between-1-7-by-rand5 https://www.linuxquestions.org/questions/linux-kernel-70/what-does-proc-pid-auxv-mean-exactly-4175421876/",https://www.cnblogs.com/apocelipes/p/18281363,2025-03-13 10:54:19
朝野布告的后院,未知作者,我们有个海外的项目，一共70个服务，前前后后花了超过一年时间完成了云服务迁移和架构调整。主要是架构调整了，原来的docker swarm托管服务，几台云服务器将n个服务堆在一起，只会对服务器资源做整体监控，服务器没事没人管单个服务的内存情况。新架构对单个服务改动不大，但外部相关架构则改头换面了，最大的改动是容器改为Kubernetes托管，放在AWS的EKS上。新架构在新的云服务上线后，暴露了很多内存问题。在此拿某个服务根据解决过程，给个排查思路，希望能给到大家一点启示。 服务为一个普通的ASP.NET Core gRPC工程，平常没什么流量。HPA设置的最大副本数为5，生产环境服务启动后，Pod内存达到或超过K8s内存请求值(512Mi)，自动触发扩展到5个实例，即副本数达到最大。这与QA环境表现并不一样，也就没有在测试阶段发现。需要想办法复现、排查解决高副本和高内存占用问题。 部署里面对容器的资源，容器资源只做了对CPU和内存的预留设置(request)，没有做上限设置(llimit)。从内存曲线上看，很多副本的内存甚至超过了请求的512Mi。不过有一点很奇怪，服务的接口并没有出现性能下降的迹象。 尝试在QA环境对相关接口进行压测，问题能复现，表现为HPA副本扩展后各个POD的内存居高不下，维持在500~600Mi长时间不释放，有时候压测甚至能冲到800Mi。即使没有什么接口请求，也需要超过20个小时才缓慢下降到350Mi左右。但尝试本地VS诊断工具则并没有发现什么内存不释放问题，除了一些个大对象驻留问题。 因为其他类似的服务并不会这样，所以第一时间怀疑是代码问题，但这么想是错的，下面交代。怀疑代码问题后，想着是不是有什么内存泄漏，找了个服务的接口在QA压测后问题能复现（即内存长时间不释放）。看了好几遍代码，除了一些个ToList用的太过频繁并没什么问题(也与内存问题不相关)，用VS诊断工具检查内存有运行又没内存泄露问题。于是在QA环境用dotnet-dump把内存快照下载回来分析，找到了个大对象堆LOH驻留类型的类，而且VS诊断工具找到的类是同一个，接着定位到了接口调用这个类的地方。业务调用很单纯，这个类就从数据库用Dapper查出来得到列表，然后分组计算下数据，不会有什么内存泄露的机会；但注释掉此部分查询则内存不再上升到500~600Mi，只在300Mi左右，而且内存使用率下降也变快了。继续二分法+注释大法调试，最后只保留数据库查询语句而不做后续业务处理，连引用都不做，内存还是会达到500~600Mi。这就让人摸不着头脑了，代码肯定是相关的，数据库查询几下一列表数据都能让内存达到预留临界值(request)，列表也才约11000条数据，虽然确实是LOH对象，但不至于造成这么严重的内存不释放现象。 添加环境变量DOTNET_GCHeapHardLimit=0x20000000 # 512Mi的十六进制值，能限制内存的使用，但并不能让GC能敏感地进行回收，方案失败 原默认为Server GC，指定为Workstation GC后，内存占用不到180Mi，扩容缩容正常，这方案看起来也可以 调试结束，方案一和方案三似乎可行，查了相关资料后，两个方案其实都有问题。方案一是代码主动强制执行了垃圾回收，但大多数情况下并不被建议在代码里面去执行，因为执行GC.Collect多多少少会影响服务性能，GC自己能感知内存使用率然后自动进行执行回收。至于方案三，不同的模式本来就对应着不同的服务场景，服务本身就是后端接口，切换为工作站模式也许可行，但ASP.NET Core默认就是Server GC，Server GC模式本身为了支持高性能并不会频繁执行垃圾回收(从.NET 9开始不一样，.NET 9的ASP.NET Core默认是第三种模式，.NET 8也支持这种模式，只不过不是默认的)。 查资料过程中才了解K8s的资源设置是有预留设置(request，又称请求设置)和上限设置(limit)，服务只设置了请求request部分，没有limit部分，那有没有可能是服务容器因为没有被设置内存limit，导致GC如脱缰野马般豪气地使用内存呢？那为啥内存不释放？就是Server GC感觉内存还是够用的，具体文章参考：工作站和服务器垃圾回收和动态适应应用程序大小 (DATAS)。先查询下可用内存吧，于是加个下面接口查询： 可用内存居然高达4Gi，真相很接近了。接着为服务设置内存limit为512Mi，再次查询得到可用内存为512Mi。没错！就是少设置了内存上限，没有这个，此时可用内存为节点内存（4GB）；加了limit重新压测，曲线： 服务内存曲线高居不下是因为容器没有被限制内存，K8s没有指定内存limit，可用内存就是节点/宿主机的物理内存，高达4GB。没有设置内存limit，但是设置了HPA，于是服务一启动经过一些时间内存超过HPA阈值造成副本数增加；GC默认是Server GC，其感知的内存足够所以不释放（包括小对象和大对象）。虽然主动调用GC.Collect则可以释放，但一般不会这样做，因为GC有自己的一套逻辑。限定内存为0.5Gi后，内存释放曲线正常，HPA扩缩正常，响应时间正常，问题得到解决，也能解释服务的接口并没有出现性能下降的的现象。 如果遇到类似内存居高不下问题，先确定.NET版本极其GC是Server GC还是Workstation GC。然后再确定其分配的可用内存是多少，K8s下要检查其资源limit有没有被设置。如果被设置之后依然有内存不释放/泄露问题，再怀疑代码问题。,https://www.cnblogs.com/huangsheng/p/18731382,2025-03-13 10:54:37
朝野布告的后院,未知作者,我们有个海外的项目，一共70个服务，前前后后花了超过一年时间完成了云服务迁移和架构调整。主要是架构调整了，原来的docker swarm托管服务，几台云服务器将n个服务堆在一起，只会对服务器资源做整体监控，服务器没事没人管单个服务的内存情况。新架构对单个服务改动不大，但外部相关架构则改头换面了，最大的改动是容器改为Kubernetes托管，放在AWS的EKS上。新架构在新的云服务上线后，暴露了很多内存问题。在此拿某个服务根据解决过程，给个排查思路，希望能给到大家一点启示。 服务为一个普通的ASP.NET Core gRPC工程，平常没什么流量。HPA设置的最大副本数为5，生产环境服务启动后，Pod内存达到或超过K8s内存请求值(512Mi)，自动触发扩展到5个实例，即副本数达到最大。这与QA环境表现并不一样，也就没有在测试阶段发现。需要想办法复现、排查解决高副本和高内存占用问题。 部署里面对容器的资源，容器资源只做了对CPU和内存的预留设置(request)，没有做上限设置(llimit)。从内存曲线上看，很多副本的内存甚至超过了请求的512Mi。不过有一点很奇怪，服务的接口并没有出现性能下降的迹象。 尝试在QA环境对相关接口进行压测，问题能复现，表现为HPA副本扩展后各个POD的内存居高不下，维持在500~600Mi长时间不释放，有时候压测甚至能冲到800Mi。即使没有什么接口请求，也需要超过20个小时才缓慢下降到350Mi左右。但尝试本地VS诊断工具则并没有发现什么内存不释放问题，除了一些个大对象驻留问题。 因为其他类似的服务并不会这样，所以第一时间怀疑是代码问题，但这么想是错的，下面交代。怀疑代码问题后，想着是不是有什么内存泄漏，找了个服务的接口在QA压测后问题能复现（即内存长时间不释放）。看了好几遍代码，除了一些个ToList用的太过频繁并没什么问题(也与内存问题不相关)，用VS诊断工具检查内存有运行又没内存泄露问题。于是在QA环境用dotnet-dump把内存快照下载回来分析，找到了个大对象堆LOH驻留类型的类，而且VS诊断工具找到的类是同一个，接着定位到了接口调用这个类的地方。业务调用很单纯，这个类就从数据库用Dapper查出来得到列表，然后分组计算下数据，不会有什么内存泄露的机会；但注释掉此部分查询则内存不再上升到500~600Mi，只在300Mi左右，而且内存使用率下降也变快了。继续二分法+注释大法调试，最后只保留数据库查询语句而不做后续业务处理，连引用都不做，内存还是会达到500~600Mi。这就让人摸不着头脑了，代码肯定是相关的，数据库查询几下一列表数据都能让内存达到预留临界值(request)，列表也才约11000条数据，虽然确实是LOH对象，但不至于造成这么严重的内存不释放现象。 添加环境变量DOTNET_GCHeapHardLimit=0x20000000 # 512Mi的十六进制值，能限制内存的使用，但并不能让GC能敏感地进行回收，方案失败 原默认为Server GC，指定为Workstation GC后，内存占用不到180Mi，扩容缩容正常，这方案看起来也可以 调试结束，方案一和方案三似乎可行，查了相关资料后，两个方案其实都有问题。方案一是代码主动强制执行了垃圾回收，但大多数情况下并不被建议在代码里面去执行，因为执行GC.Collect多多少少会影响服务性能，GC自己能感知内存使用率然后自动进行执行回收。至于方案三，不同的模式本来就对应着不同的服务场景，服务本身就是后端接口，切换为工作站模式也许可行，但ASP.NET Core默认就是Server GC，Server GC模式本身为了支持高性能并不会频繁执行垃圾回收(从.NET 9开始不一样，.NET 9的ASP.NET Core默认是第三种模式，.NET 8也支持这种模式，只不过不是默认的)。 查资料过程中才了解K8s的资源设置是有预留设置(request，又称请求设置)和上限设置(limit)，服务只设置了请求request部分，没有limit部分，那有没有可能是服务容器因为没有被设置内存limit，导致GC如脱缰野马般豪气地使用内存呢？那为啥内存不释放？就是Server GC感觉内存还是够用的，具体文章参考：工作站和服务器垃圾回收和动态适应应用程序大小 (DATAS)。先查询下可用内存吧，于是加个下面接口查询： 可用内存居然高达4Gi，真相很接近了。接着为服务设置内存limit为512Mi，再次查询得到可用内存为512Mi。没错！就是少设置了内存上限，没有这个，此时可用内存为节点内存（4GB）；加了limit重新压测，曲线： 服务内存曲线高居不下是因为容器没有被限制内存，K8s没有指定内存limit，可用内存就是节点/宿主机的物理内存，高达4GB。没有设置内存limit，但是设置了HPA，于是服务一启动经过一些时间内存超过HPA阈值造成副本数增加；GC默认是Server GC，其感知的内存足够所以不释放（包括小对象和大对象）。虽然主动调用GC.Collect则可以释放，但一般不会这样做，因为GC有自己的一套逻辑。限定内存为0.5Gi后，内存释放曲线正常，HPA扩缩正常，响应时间正常，问题得到解决，也能解释服务的接口并没有出现性能下降的的现象。 如果遇到类似内存居高不下问题，先确定.NET版本极其GC是Server GC还是Workstation GC。然后再确定其分配的可用内存是多少，K8s下要检查其资源limit有没有被设置。如果被设置之后依然有内存不释放/泄露问题，再怀疑代码问题。,https://www.cnblogs.com/huangsheng/p/18731382,2025-03-13 10:54:36
朝野布告的后院,未知作者,我们有个海外的项目，一共70个服务，前前后后花了超过一年时间完成了云服务迁移和架构调整。主要是架构调整了，原来的docker swarm托管服务，几台云服务器将n个服务堆在一起，只会对服务器资源做整体监控，服务器没事没人管单个服务的内存情况。新架构对单个服务改动不大，但外部相关架构则改头换面了，最大的改动是容器改为Kubernetes托管，放在AWS的EKS上。新架构在新的云服务上线后，暴露了很多内存问题。在此拿某个服务根据解决过程，给个排查思路，希望能给到大家一点启示。 服务为一个普通的ASP.NET Core gRPC工程，平常没什么流量。HPA设置的最大副本数为5，生产环境服务启动后，Pod内存达到或超过K8s内存请求值(512Mi)，自动触发扩展到5个实例，即副本数达到最大。这与QA环境表现并不一样，也就没有在测试阶段发现。需要想办法复现、排查解决高副本和高内存占用问题。 部署里面对容器的资源，容器资源只做了对CPU和内存的预留设置(request)，没有做上限设置(llimit)。从内存曲线上看，很多副本的内存甚至超过了请求的512Mi。不过有一点很奇怪，服务的接口并没有出现性能下降的迹象。 尝试在QA环境对相关接口进行压测，问题能复现，表现为HPA副本扩展后各个POD的内存居高不下，维持在500~600Mi长时间不释放，有时候压测甚至能冲到800Mi。即使没有什么接口请求，也需要超过20个小时才缓慢下降到350Mi左右。但尝试本地VS诊断工具则并没有发现什么内存不释放问题，除了一些个大对象驻留问题。 因为其他类似的服务并不会这样，所以第一时间怀疑是代码问题，但这么想是错的，下面交代。怀疑代码问题后，想着是不是有什么内存泄漏，找了个服务的接口在QA压测后问题能复现（即内存长时间不释放）。看了好几遍代码，除了一些个ToList用的太过频繁并没什么问题(也与内存问题不相关)，用VS诊断工具检查内存有运行又没内存泄露问题。于是在QA环境用dotnet-dump把内存快照下载回来分析，找到了个大对象堆LOH驻留类型的类，而且VS诊断工具找到的类是同一个，接着定位到了接口调用这个类的地方。业务调用很单纯，这个类就从数据库用Dapper查出来得到列表，然后分组计算下数据，不会有什么内存泄露的机会；但注释掉此部分查询则内存不再上升到500~600Mi，只在300Mi左右，而且内存使用率下降也变快了。继续二分法+注释大法调试，最后只保留数据库查询语句而不做后续业务处理，连引用都不做，内存还是会达到500~600Mi。这就让人摸不着头脑了，代码肯定是相关的，数据库查询几下一列表数据都能让内存达到预留临界值(request)，列表也才约11000条数据，虽然确实是LOH对象，但不至于造成这么严重的内存不释放现象。 添加环境变量DOTNET_GCHeapHardLimit=0x20000000 # 512Mi的十六进制值，能限制内存的使用，但并不能让GC能敏感地进行回收，方案失败 原默认为Server GC，指定为Workstation GC后，内存占用不到180Mi，扩容缩容正常，这方案看起来也可以 调试结束，方案一和方案三似乎可行，查了相关资料后，两个方案其实都有问题。方案一是代码主动强制执行了垃圾回收，但大多数情况下并不被建议在代码里面去执行，因为执行GC.Collect多多少少会影响服务性能，GC自己能感知内存使用率然后自动进行执行回收。至于方案三，不同的模式本来就对应着不同的服务场景，服务本身就是后端接口，切换为工作站模式也许可行，但ASP.NET Core默认就是Server GC，Server GC模式本身为了支持高性能并不会频繁执行垃圾回收(从.NET 9开始不一样，.NET 9的ASP.NET Core默认是第三种模式，.NET 8也支持这种模式，只不过不是默认的)。 查资料过程中才了解K8s的资源设置是有预留设置(request，又称请求设置)和上限设置(limit)，服务只设置了请求request部分，没有limit部分，那有没有可能是服务容器因为没有被设置内存limit，导致GC如脱缰野马般豪气地使用内存呢？那为啥内存不释放？就是Server GC感觉内存还是够用的，具体文章参考：工作站和服务器垃圾回收和动态适应应用程序大小 (DATAS)。先查询下可用内存吧，于是加个下面接口查询： 可用内存居然高达4Gi，真相很接近了。接着为服务设置内存limit为512Mi，再次查询得到可用内存为512Mi。没错！就是少设置了内存上限，没有这个，此时可用内存为节点内存（4GB）；加了limit重新压测，曲线： 服务内存曲线高居不下是因为容器没有被限制内存，K8s没有指定内存limit，可用内存就是节点/宿主机的物理内存，高达4GB。没有设置内存limit，但是设置了HPA，于是服务一启动经过一些时间内存超过HPA阈值造成副本数增加；GC默认是Server GC，其感知的内存足够所以不释放（包括小对象和大对象）。虽然主动调用GC.Collect则可以释放，但一般不会这样做，因为GC有自己的一套逻辑。限定内存为0.5Gi后，内存释放曲线正常，HPA扩缩正常，响应时间正常，问题得到解决，也能解释服务的接口并没有出现性能下降的的现象。 如果遇到类似内存居高不下问题，先确定.NET版本极其GC是Server GC还是Workstation GC。然后再确定其分配的可用内存是多少，K8s下要检查其资源limit有没有被设置。如果被设置之后依然有内存不释放/泄露问题，再怀疑代码问题。,https://www.cnblogs.com/huangsheng/p/18731382,2025-03-13 10:53:54
从API到Agent：万字长文洞悉LangChain工程化设计,未知作者,"我想做一个尝试，看看能不能用尽量清晰的逻辑，给“AI外行人士”（当然，我也是……）引入一下LangChain，试着从工程角度去理解LangChain的设计和使用。同时大家也可以将此文档作为LangChain的“10分钟快速上手”手册，本意是希望帮助需要的同学实现AI工程的Bootstrap。 文中所有的示例代码都已托管到GitHub：https://github.com/TuGraph-contrib/langchain-demo 正式开始前，还是有必要从定义（What）开始。LangChain是2022年10月底，由哈佛大学的Harrison Chase发起的基于开源大语言模型的AI工程开发框架。当然也可以问一下AI： 我：LangChain是什么？ GPT-4：LangChain 是一个开源的语言模型工具链框架，旨在使研究人员和开发人员能够更轻松地构建、实验和部署以自然语言处理（NLP）为中心的应用程序。它提供了多种组件和工具，可帮助用户利用最近的语言模型进展，如大型 Transformer 模型等，并且可以与 Hugging Face 等平台集成。 LangChain 的核心理念是将语言模型用作协作工具，通过它，开发者可以构建出处理复杂任务的系统，并且可以高效地对接不同的数据源和应用程序接口（APIs）。这个框架试图简化连接不同的输入和输出流，以及在流中实现语言模型的过程。 顾名思义，LangChain中的“Lang”自然是指大语言模型，“Chain”即“链”，也就是将大模型与其他组件连接成链，借此构建AI工程应用。那么LangChain该如何（How）做到这一点的呢？解答这个问题之前，需要先回答什么是工程？什么是AI工程？ 工程是指以某组设想的目标为依据，应用有关的科学知识和技术手段，通过有组织的一群人将某个（或某些）现有实体（自然的或人造的）转化为具有预期使用价值的人造产品过程。 其中，“目标”定义了要解决的问题域，决定了工程的顶层设计和能力边界，同时也定义了“产品”的最终形态。提升“人”的ROI是工程设计的价值归属。“实体”是工程的生产材料输入，“科学 | 技术”是工程有序运行的基础，对它们的合理利用可以提升工程的整体产出效率。 很明显，LLM作为LangChain能力的基础，是了解LangChain工程化设计的前提。接下来我们就从最基础的LLM API使用谈起，一步步了解LangChain的工程化构建过程及其背后的设计理念。 架构设计领域有个比较流行的术语——乐高架构，当然也可以叫可插拔架构。说白就是通过对系统基本组件的合理抽象，找到构造复杂系统的统一规律和可达路径，从而实现在降低系统实现复杂度的同时，提升系统整体的扩展性。（非官方表达，大家能Get到我的意思就好……） LangChain实际上也遵循了乐高架构的思想。当然，作为最关键的乐高组件之一，LLM的能力自然是我们优先了解的对象，那我们就从OpenAI的API开始吧！ 图计算是一种计算模型，用于处理大规模图形结构的数据，并执行各种复杂的算法和计算。这种计算模型主要用于社交网络分析、Web搜索、生物信息学、网络路由优化、数据挖掘等领域。图计算模型的核心是将数据表示为图形结构（节点和边），这样可以更好地揭示数据之间的关系和互动。在图计算中，算法通常以迭代的方式运行，每次迭代都会更新图中节点的状态，直到达到某种停止条件。 早先的Completion API已经在2023年7月后不再维护，和最新的Chat Completion API参数和结果格式有所不同，最明显的是Prompt是以纯文本方式传递，而非Message格式。 除了文本生成服务，OpenAI也提供了大量的LLM的周边服务，以协助AI工程构建更复杂的应用能力。如：函数调用、嵌入、微调、多模态等，具体可参考OpenAI开发文档的内容。 自2022年11月底ChatGPT发布以来，AI的大门才真正地向人类打开，其中给用户留下最深印象的功能，自然是智能对话。OpenAI的Chat Completion API参数支持传入消息历史，可以轻松地实现简单的对话服务。 me: 什么是图计算？ ai: 图计算是一种计算模型，用于处理大规模图形结构数据的计算和分析。在这种计算模型中，数据被表示为图形，其中节点代表实体，边代表实体之间的关系。图计算可以用于解决许多实际问题，如社交网络分析、网络路由、生物信息学等。图计算的主要挑战是如何有效地处理大规模的图形数据，并提供快速的计算和分析结果。 me: 刚才我问了什么问题？ ai: 你问的问题是：“什么是图计算？” 到目前为止，我们还只是用OpenAI最原始的RESTful API构建LLM工程能力，甚至连OpenAI提供的SDK都未使用。显然这不是一个高效的方式，使用前边安装的LangChain-OpenAI集成包langchain-openai可以大大降低代码的开发成本。 content='图计算是一种计算模型，主要用于处理图形结构数据的计算和分析。图计算的对象是图，图由节点和边组成，节点代表实体对象，边代表实体对象之间的关系。图计算主要用于解决实体关系复杂、关系密集的问题，如社交网络分析、网络拓扑分析、推荐系统等。图计算的主要任务是通过对图中节点和边的计算，发现和提取出图中隐含的知识和信息。' 对于文本生成模型服务来说，实际的输入和输出本质上都是字符串，因此直接裸调用LLM服务带来的问题是要在输入格式化和输出结果解析上做大量的重复的文本处理工作。LangChain当然考虑到这一点，提供了Prompt和OutputParser抽象，用户可以根据自己的需要选择具体的实现类型使用。 模型的IO组件确实可以减少重复的文本处理工作，但形式上依然不够清晰，这里就引入了LangChain中的关键概念：链（Chain）。 LangChain的表达式语言（LCEL）通过重载__or__运算符的思路，构建了类似Unix管道运算符的设计，实现更简洁的LLM调用形式。 当然，为了简化Chain的参数调用格式，也可以借助RunnablePassthrough透传上游参数输入。 苹果是一种营养丰富的水果，具有帮助消化、保护心脏、降低糖尿病风险、强化免疫系统、帮助减肥、保护视力、预防哮喘、抗癌和提升记忆力等多种好处。然而，过度食用或者不适当的食用方式也可能带来一些不利影响，如引发过敏、导致腹泻、对牙齿造成伤害、可能携带农药残留、影响正常饮食和钙质吸收、增加蛀牙风险和引发胃痛等。因此，我们在享受苹果带来的好处的同时，也需要注意适量和正确的食用方式。 通过调用chain.get_graph().print_ascii()可以查看Chain的计算图结构。当然，使用LangSmith能更清晰的跟踪每一步的计算结果。 基于LCEL确实能描述比较复杂的LangChain计算图结构，但依然有DAG天然的设计限制，即不能支持“循环”。于是LangChain社区推出了一个新的项目——LangGraph，期望基于LangChain构建支持循环和跨多链的计算图结构，以描述更复杂的，甚至具备自动化属性的AI工程应用逻辑，比如智能体应用。其具体使用方式可以参考LangGraph文档。 LangGraph声称其设计理念受Pregel/Beam的启发，构建支持多步迭代的计算能力，这部分设计理念和我们设计的支持“流/批/图”计算一体化的图计算引擎TuGraph也十分相似，感兴趣的朋友可以访问TuGraph Analytics项目进行学习。 通过Chain，LangChain相当于以“工作流”的形式，将LLM与IO组件进行了有秩序的连接，从而具备构建复杂AI工程流程的能力。而我们都知道LLM提供的文本生成服务本身不提供记忆功能，需要用户自己管理对话历史。因此引入Memory组件，可以很好地扩展AI工程的能力边界。 LangChain的BaseMemory接口提供了Memory的统一抽象（截至v0.1.12还是Beta版本），提供了多种类型的Memory组件的实现，我们选用最简单的ConversationBufferMemory实现类型。 需要注意的是，要将Memory组件应用到Chain上，需要使用子类LLMChain进行创建Chain。 图计算是一种计算类型，主要处理的数据结构是图。图是由节点（或顶点）和边组成的，节点代表实体，边代表实体之间的关系。在图计算中，主要解决的问题是如何在图的结构上进行有效的计算和分析。 你问的问题是：“什么是图计算？” 这里可以看到，创建带Memory功能的Chain，并不能使用统一的LCEL语法。调用LLMChain使用的是predict而非invoke方法，直接调用invoke会返回一个LLMResult类型的结果。因此，LLMChain也不能使用管道运算符接StrOutputParser。这些设计上的问题，个人推测也是目前Memory模块还是Beta版本的原因之一吧。 但是，LangChain提供了工具类RunnableWithMessageHistory，支持了为Chain追加History的能力，从某种程度上缓解了上述问题。不过需要指定Lambda函数get_session_history以区分不同的会话，并需要在调用时通过config参数指定具体的会话ID。 SessionHistory必须是History接口类型BaseChatMessageHistory，用户可以根据需要选择不同的存储实现。这里为了简化，全局只用了一份内存类型的ChatMessageHistory。 拥有记忆后，确实扩展了AI工程的应用场景。但是在专有领域，LLM无法学习到所有的专业知识细节，因此在面向专业领域知识的提问时，无法给出可靠准确的回答，甚至会“胡言乱语”，这种现象称之为LLM的“幻觉”。 检索增强生成（RAG）把信息检索技术和大模型结合起来，将检索出来的文档和提示词一起提供给大模型服务，从而生成更可靠的答案，有效的缓解大模型推理的“幻觉”问题。 如果说LangChain相当于给LLM这个“大脑”安装了“四肢和躯干”，RAG则是为LLM提供了接入“人类知识图书馆”的能力。 相比提示词工程，RAG有更丰富的上下文和数据样本，可以不需要用户提供过多的背景描述，即能生成比较符合用户预期的答案。相比于模型微调，RAG可以提升问答内容的时效性和可靠性，同时在一定程度上保护了业务数据的隐私性。 但由于每次问答都涉及外部系统数据检索，因此RAG的响应时延相对较高。另外，引用的外部知识数据会消耗大量的模型Token资源。因此，用户需要结合自身的实际应用场景做合适的技术选型。 借助LCEL提供的RunnableParallel可以清晰描述RAG的计算图结构，其中最关键的部分是通过context键注入向量存储（Vector Store）的查询器（Retriever）。 蚂蚁图数据库目前没有公开信息表明已经开源。开源状态可能会随时间和公司政策变化，建议直接查阅蚂蚁集团或相关开源平台的官方信息获取最新和准确的消息。 是的，蚂蚁的图数据库产品TuGraph是开源的。 要构建更强大的AI工程应用，只有生成文本这样的“纸上谈兵”能力自然是不够的。工具不仅仅是“肢体”的延伸，更是为“大脑”插上了想象力的“翅膀”。借助工具，才能让AI应用的能力真正具备无限的可能，才能从“认识世界”走向“改变世界”。 这里不得不提到OpenAI的Chat Completion API提供的函数调用能力（注意这里不是Assistant的函数调用），通过在对话请求内附加tools参数描述工具的定义格式（原先的functions参数已过期），LLM会根据提示词推断出需要调用哪些工具，并提供具体的调用参数信息。用户需要根据返回的工具调用信息，自行触发相关工具的回调。下一章内容我们可以看到工具的调用动作可以通过Agent自主接管。 为了简化代码实现，我们用LangChain的注解@tool定义了一个测试用的“获取指定城市的当前气温”的工具函数。然后通过bind_tools方法绑定到LLM对象即可。需要注意的是这里需要用JsonOutputToolsParser解析结果输出。 [{'type': 'get_temperature', 'args': {'city': '杭州'}}] 实际上LangChain提供了大量的内置工具和工具库的支持。@tool只是提供了简洁的工具创建的支持，要定制复杂的工具行为需要自行实现BaseTool工具接口。同时工具库接口BaseToolkit下也有大量的实现，如向量存储、SQL数据库、GitHub等等。用户可以根据自身需求选用或自行扩展。 通用人工智能（AGI）将是AI的终极形态，几乎已成为业界共识。类比之，构建智能体（Agent）则是AI工程应用当下的“终极形态”。 Agent的核心思想是使用大型语言模型（LLM）来选择要采取的行动序列。在Chain中行动序列是硬编码的，而Agent则采用语言模型作为推理引擎来确定以什么样的顺序采取什么样的行动。 Agent相比Chain最典型的特点是“自治”，它可以通过借助LLM专长的推理能力，自动化地决策获取什么样的知识，采取什么样的行动，直到完成用户设定的最终目标。 我们使用Agent继续完成前边Tool部分没有完成的例子。这里使用create_openai_tools_agent方法创建一个简单的OpenAI工具Agent，AgentExecutor会自动接管工具调用的动作。如果希望给Agent添加记忆能力，依然可以采用前边Memory章节提过的RunnableWithMessageHistory的方案。 > Entering new AgentExecutor chain... Invoking: get_temperature with {'city': 'Hangzhou'} 16 今天杭州的气温是16度。 需要补充说明的是，LangChain提供了Hub功能，帮助大家管理共享Agent的提示词模板。上述示例代码的Agent提示词模板和hwchase17/openai-tools-agent的定义等价。 通过代码prompt = hub.pull(""hwchase17/openai-tools-agent"")可以直接引用创建prompt。 从API到Agent，我们“脑暴”了一个完整AI工程应用的发展轨迹，借此我相信大家对LangChain的设计理念应该有了进一步的理解。 最后，我们再看一眼LangChain的产品架构。除了本文未介绍的LangServe——将Chain部署为RESTful服务，其他不再赘述。 可能会有小伙伴疑问，为啥我一个搞图计算的，在这捣鼓起AI工具来了。抛开“拥抱行业，跟进趋势”这样的大口号不谈，单纯从工作需要角度，使用AI工具加速内容的产出，本身就可以帮助我节省大量的时间和精力，空出的“闲暇”时间去带娃也是极好的…… 线上关于LangChain的文章，虽不说汗牛充栋，但也能随手拈来。作为“后入场”者，我过去从事了多年的数仓、中台、云产品的架构和研发，最近两三年一直在搞图计算这样的基础软件，就想试试结合自身多面的工程经验去理解LangChain背后的设计理念，给大家一个别样的视角去观察、体验和思考AI工程。这两天被“AI程序员”的新闻刷屏了，其中引用的Adam Rackis的一条Twitter令我感受颇深：“做好磨练和深入学习的准备，熟练地引导人工智能取得好的结果可能才是未来程序员能体现出来的价值。”，或许这正是AI工程的用武之地吧。",https://www.cnblogs.com/fanzhidongyzby/p/18075179/langchain,2025-03-13 10:54:22
学习新知识的小白白,未知作者,"家里有一台机顶盒（移动魔百盒）淘汰下来了，博主想要物尽其用，看看是否能将其改造为一台Linux""开发机""，量身定制系统，用作开发和测试，以及跑服务器 机顶盒型号：移动魔百盒CM201-2(CH)，芯片组: hi3798mv300（hi3798mv3dmm），其他型号类似 理论上适用于以下SOC的机顶盒：Hi3798Mv100 / Hi3798Cv200 / Hi3798Mv200 / Hi3798Mv300 （烧录也可以命令行形式甚至不需要Windows，需要参考海思官方文档，为了方便故在Windows平台下进行烧录） 源码链接：https://github.com/martixjohn/HiSTBLinuxV100R005C00SPC050 主板接线如图，从左到右依次连接到转接板的GND, TXD, RXD，不同板子可能有所差别，可以尝试调一下TXD, RXD顺序 Board->Boot Regfile Config List: 启动的配置文件，将第一个改为自己的机顶盒匹配的配置文件名称 修改分区配置文件emmc_partitions.xml，将最后一项的SelectFile改为ubuntu镜像的文件名，Length属性改为-，表示占满存储器剩余空间 取一根网线连接机顶盒主板和路由器，确保PC和机顶盒能互通，配置IP信息（使用网络传输进行烧录，原因是串口传输速率过慢） 在板端配置下，网关一般设置为路由器内网IP，一般路由器默认为192.168.1.1，子网掩码需要按照路由器进行配置，家用路由器默认是255.255.255.0，IP随意配置保证与路由器在同一子网，如192.168.1.200，物理地址随意设置 注：以上配置的目的是让机顶盒固定IP地址，以便将固件通过以太网传输到机顶盒，机顶盒会按照配置自动设置对应IP和PC建立通信 加载分区配置和固件：依次选择HiBurn->烧写EMMC->浏览（选择分区配置文件emmc_paritions.xml） 打开机顶盒开关，先是打印bootloader等启动信息后，直到一串绿色的OK字样显示出来，表示Ubuntu已经成功刷入！ 本文来自博客园，作者：学习新知识的小白白，转载请注明原文链接：https://www.cnblogs.com/onecainiao/p/18353586",https://www.cnblogs.com/onecainiao/p/18353586,2025-03-13 10:54:15
CodeBlogMan,未知作者,写在前面的是关于自己的一点介绍，21年本科毕业，学的是计算机科学与技术专业。自己本身对编程和开发是有兴趣的，同时也对项目管理、团队建设方面有兴趣。 目前有两段工作经历，都是在一线当 Java 开发大头兵，但职级倒是逐渐有所上升。做过从 0-1 的 SaaS、PaaS 的自研项目，也做过基于多租户的标准化服务项目，还有过半年的 PM 的经历...这些都让我有了一些关于工作上的感悟。现在简单总结一下，拿出来和大家做分享交流。 下面主要从工作的方式方法、构建知识体系、尽早规划职业这三个大的方面展开。文章有点长，但都是自己一步步实践总结得来的，有写得不好的地方，还希望大家多包涵。 拿出积极的态度：首先需要自己努力去解决问题，其次包括调动资源、团队协助，最好要让领导/同事知晓你的情况。 摆烂并不可取，劳动是用来换取报酬的。如果平时的工作表现/绩效过不去，那么将可能会直接影响到我们的收入。 对于工作量的估算要留有余地。学会平衡别人的预期，如果 3 天内能完成，那最好回复一个星期：总有别的事情会打扰你。 遵守基本的职业道德：如果你负责的服务出现故障或发现一个严重的 bug ，作为开发我们应该马上对其进行修复/解决。 工作里对事不对人：我们需要推进的是项目，是一件件具体的事情，聚焦于如何解决问题，而不是和人做无效拉扯。 做好上面这些，假如是对于刚毕业的我来说可能并不容易，但过了两年自己开始负责项目了以后，我就能慢慢理解并适应了。 所以我们可以有意识地提炼日常工作成果，形成自己的个人源码库和工具库、解决某类问题的通用系统体系结构、甚至进化为框架。 众所周知对软件开发人员而言，有、无经验的一个显著区别是：新手无论完成什么开发任务都从头开始，而有经验的开发往往通过重组自己的可复用模块、类库来解决问题。 这并不是说所有可复用的东西都必须自己实现，别人成熟的、通过测试的成果也可以收集、整理到自己的知识库中。 实际程序员们可能很难做到：总有一些会议拉上你去 battle，总有一些线上的 bug 困扰着你，总会有领导安排的其它事情让你分身乏力...还有很多。工作是为了更好的生活这个主旨不会变，所以我也有自己的一些想法： 知识体系是系统化的知识，具有连续性的特点，反映了一个人经过学习后所拥有的知识构成情况与结合方式。如果一个开发人员在1、2年内都没有更新过自己的知识，那么他可能已经不再属于这个行业了。 抽象思维能够超越感官直接感知信息，通过对事物的本质属性和规律的把握，使人们在认识活动中获得更深层次的知识。 多阅读优秀的代码，多思考本质和原因，多通过实践来检验 写出可阅读的、简洁易懂、可维护的代码，应该是程序员在工作中都需要追求的。在实现业务的时候，通过借助模仿优秀的开源实现，学习他人在提高代码效率、逻辑思维、处理高并发等方面是怎么做的。 学会总结复盘，从中抽取有价值的经验，将成长记录下来 复盘的内容，可以是一次线上 bug 的处理过程，可以是一次代码重构优化的过程，也可以是对一个中间件的新认识等等。复盘不是问责也不是甩锅，而是聚焦于好和不好，知其然再知其所以然，同时避免重蹈覆辙。 不是所有人都愿意直面自己的缺点和不足，我自己最开始也不明白复盘有什么用处，觉得对领导和业务能有交代就行。但随着自己做过的项目多了起来，受到身边优秀同事的影响也在日益增多，我才明白复盘是可以让人迅速成长的不二法宝。 有意识地站在更高的角度去看待问题，可能会得到不一样的答案 作为一线开发我们更多关注的，可能是接口的参数和实现逻辑，数据库字段的设计，中间件的使用，异常的处理等实际编码问题； 而你的 TeamLeader 可能关注的是项目的高可用，处理高并发的性能，硬件资源的成本，服务挂了怎么降低对业务的影响等； 部门的总监则可能在考虑这个项目要不要做二期三期？对业务方有什么价值？能带来多少新用户？让谁来负责后续的运营等。 初/中级可以先从广度出发，多见识一些主流技术或者解决方案，这样在面对业务的时候可以更从容地进行设计，也能更清楚自己 CURD 的目的是什么。 高级/资深可能就要精通某一或者某几方面的通用架构/解决方案了，同时对于负责的业务有深刻的认识，可能也要负责团队的建设，从更广阔的视角去思考问题。 我是在工作一年半左右慢慢有了对未来进行职业规划的想法，因为时间会向前走，人也会渐渐地成长。对于未来想要什么，可以慢慢给自己多找找方向去尝试。 2010-2020 这十年，互联网发展如火如荼，各种公司如雨后春笋般冒出来，那时候招聘需求量大，互联网就业市场一片欣欣向荣。 而现在没有了当年的辉煌，很多互联网公司业务发展停滞，同时经过前面多年的发展，产品架构已经成熟稳定，不再需要这么多的程序员，势必要进行结构优化，那优化的对象自然是性价比低的人群。 其次，IT互联网行业高薪的宣传又吸引了一大批年轻人跑步进场，需求变少的情况下人又变多，内卷就这么产生了。高学历的人才多了很多，所谓学历贬值的论调在这几年一直都有出现。 让我们透过现象看本质：IT互联网行业的特殊性在于产出是可迭代的。这个行业程序员的产出就是软件&代码，而这个东西就是可以不断的迭代的。 分析了上面的原因，就有针对性了。软件代码这些东西是可迭代的，但有些东西是没法复制和迭代的：像是沟通协调能力、解决问题能力、规划组织能力、管理能力和人脉资源等等。这些其实就是我们经常说的软实力，这些能力才是越久越吃香。 这些能力不仅可以在日常工作中有意识的积累，也可以通过考取相关的证书来掌握理论知识，比如：软考高项（信息系统项目管理师、系统架构师）、PMP认证、阿里云/华为云高级认证等。 打开认知的局限，不只着眼于手里掌握的技术，拓展自己职业的综合能力，明白不是只有靠技术才能活下去，才能更好地“破局”。 文章最后，我还有一些想说的。的确，技术逐渐会变得越来越“廉价”，不再是必须要工作多年的大佬才能驾驭的高级东西。 这里面有个很重要的原因，就是开源的力量。大家可以想一想自己参与的一些项目，用到了多少开源的东西？以一个典型的后端项目为例，可能涉及到了 MySQL、Redis、ElasticSearch、RocketMQ、Nginx、SprintBoot、Jenkins、CI/CD、Docker...... 程序员要做的就是根据业务需求，把上面那一堆组件按照规则合理地堆在一起。一般就是 Nginx 负载均衡，Spring Boot/Cloud 写后端业务逻辑，Redis 做缓存，MySQL、ES、MongoDB 做数据存储，OSS/COS 做云对象存储，RocketMQ/Kafka 做消息队列，最后进行 CI/CD 操作塞到 Docker 里面部署运行。现在绝大多数的功能，开源组件都能提供，再加上现在是云的时代，各种服务都上云了，那么问题就更简单了。 既然我们身处在这个行业，那么就要做好持续学习的准备，不断打怪升级，锻炼综合能力，做到让经验随着年龄的增长成真正的正比。,https://www.cnblogs.com/CodeBlogMan/p/17983370,2025-03-13 10:54:27
从API到Agent：万字长文洞悉LangChain工程化设计,未知作者,"我想做一个尝试，看看能不能用尽量清晰的逻辑，给“AI外行人士”（当然，我也是……）引入一下LangChain，试着从工程角度去理解LangChain的设计和使用。同时大家也可以将此文档作为LangChain的“10分钟快速上手”手册，本意是希望帮助需要的同学实现AI工程的Bootstrap。 文中所有的示例代码都已托管到GitHub：https://github.com/TuGraph-contrib/langchain-demo 正式开始前，还是有必要从定义（What）开始。LangChain是2022年10月底，由哈佛大学的Harrison Chase发起的基于开源大语言模型的AI工程开发框架。当然也可以问一下AI： 我：LangChain是什么？ GPT-4：LangChain 是一个开源的语言模型工具链框架，旨在使研究人员和开发人员能够更轻松地构建、实验和部署以自然语言处理（NLP）为中心的应用程序。它提供了多种组件和工具，可帮助用户利用最近的语言模型进展，如大型 Transformer 模型等，并且可以与 Hugging Face 等平台集成。 LangChain 的核心理念是将语言模型用作协作工具，通过它，开发者可以构建出处理复杂任务的系统，并且可以高效地对接不同的数据源和应用程序接口（APIs）。这个框架试图简化连接不同的输入和输出流，以及在流中实现语言模型的过程。 顾名思义，LangChain中的“Lang”自然是指大语言模型，“Chain”即“链”，也就是将大模型与其他组件连接成链，借此构建AI工程应用。那么LangChain该如何（How）做到这一点的呢？解答这个问题之前，需要先回答什么是工程？什么是AI工程？ 工程是指以某组设想的目标为依据，应用有关的科学知识和技术手段，通过有组织的一群人将某个（或某些）现有实体（自然的或人造的）转化为具有预期使用价值的人造产品过程。 其中，“目标”定义了要解决的问题域，决定了工程的顶层设计和能力边界，同时也定义了“产品”的最终形态。提升“人”的ROI是工程设计的价值归属。“实体”是工程的生产材料输入，“科学 | 技术”是工程有序运行的基础，对它们的合理利用可以提升工程的整体产出效率。 很明显，LLM作为LangChain能力的基础，是了解LangChain工程化设计的前提。接下来我们就从最基础的LLM API使用谈起，一步步了解LangChain的工程化构建过程及其背后的设计理念。 架构设计领域有个比较流行的术语——乐高架构，当然也可以叫可插拔架构。说白就是通过对系统基本组件的合理抽象，找到构造复杂系统的统一规律和可达路径，从而实现在降低系统实现复杂度的同时，提升系统整体的扩展性。（非官方表达，大家能Get到我的意思就好……） LangChain实际上也遵循了乐高架构的思想。当然，作为最关键的乐高组件之一，LLM的能力自然是我们优先了解的对象，那我们就从OpenAI的API开始吧！ 图计算是一种计算模型，用于处理大规模图形结构的数据，并执行各种复杂的算法和计算。这种计算模型主要用于社交网络分析、Web搜索、生物信息学、网络路由优化、数据挖掘等领域。图计算模型的核心是将数据表示为图形结构（节点和边），这样可以更好地揭示数据之间的关系和互动。在图计算中，算法通常以迭代的方式运行，每次迭代都会更新图中节点的状态，直到达到某种停止条件。 早先的Completion API已经在2023年7月后不再维护，和最新的Chat Completion API参数和结果格式有所不同，最明显的是Prompt是以纯文本方式传递，而非Message格式。 除了文本生成服务，OpenAI也提供了大量的LLM的周边服务，以协助AI工程构建更复杂的应用能力。如：函数调用、嵌入、微调、多模态等，具体可参考OpenAI开发文档的内容。 自2022年11月底ChatGPT发布以来，AI的大门才真正地向人类打开，其中给用户留下最深印象的功能，自然是智能对话。OpenAI的Chat Completion API参数支持传入消息历史，可以轻松地实现简单的对话服务。 me: 什么是图计算？ ai: 图计算是一种计算模型，用于处理大规模图形结构数据的计算和分析。在这种计算模型中，数据被表示为图形，其中节点代表实体，边代表实体之间的关系。图计算可以用于解决许多实际问题，如社交网络分析、网络路由、生物信息学等。图计算的主要挑战是如何有效地处理大规模的图形数据，并提供快速的计算和分析结果。 me: 刚才我问了什么问题？ ai: 你问的问题是：“什么是图计算？” 到目前为止，我们还只是用OpenAI最原始的RESTful API构建LLM工程能力，甚至连OpenAI提供的SDK都未使用。显然这不是一个高效的方式，使用前边安装的LangChain-OpenAI集成包langchain-openai可以大大降低代码的开发成本。 content='图计算是一种计算模型，主要用于处理图形结构数据的计算和分析。图计算的对象是图，图由节点和边组成，节点代表实体对象，边代表实体对象之间的关系。图计算主要用于解决实体关系复杂、关系密集的问题，如社交网络分析、网络拓扑分析、推荐系统等。图计算的主要任务是通过对图中节点和边的计算，发现和提取出图中隐含的知识和信息。' 对于文本生成模型服务来说，实际的输入和输出本质上都是字符串，因此直接裸调用LLM服务带来的问题是要在输入格式化和输出结果解析上做大量的重复的文本处理工作。LangChain当然考虑到这一点，提供了Prompt和OutputParser抽象，用户可以根据自己的需要选择具体的实现类型使用。 模型的IO组件确实可以减少重复的文本处理工作，但形式上依然不够清晰，这里就引入了LangChain中的关键概念：链（Chain）。 LangChain的表达式语言（LCEL）通过重载__or__运算符的思路，构建了类似Unix管道运算符的设计，实现更简洁的LLM调用形式。 当然，为了简化Chain的参数调用格式，也可以借助RunnablePassthrough透传上游参数输入。 苹果是一种营养丰富的水果，具有帮助消化、保护心脏、降低糖尿病风险、强化免疫系统、帮助减肥、保护视力、预防哮喘、抗癌和提升记忆力等多种好处。然而，过度食用或者不适当的食用方式也可能带来一些不利影响，如引发过敏、导致腹泻、对牙齿造成伤害、可能携带农药残留、影响正常饮食和钙质吸收、增加蛀牙风险和引发胃痛等。因此，我们在享受苹果带来的好处的同时，也需要注意适量和正确的食用方式。 通过调用chain.get_graph().print_ascii()可以查看Chain的计算图结构。当然，使用LangSmith能更清晰的跟踪每一步的计算结果。 基于LCEL确实能描述比较复杂的LangChain计算图结构，但依然有DAG天然的设计限制，即不能支持“循环”。于是LangChain社区推出了一个新的项目——LangGraph，期望基于LangChain构建支持循环和跨多链的计算图结构，以描述更复杂的，甚至具备自动化属性的AI工程应用逻辑，比如智能体应用。其具体使用方式可以参考LangGraph文档。 LangGraph声称其设计理念受Pregel/Beam的启发，构建支持多步迭代的计算能力，这部分设计理念和我们设计的支持“流/批/图”计算一体化的图计算引擎TuGraph也十分相似，感兴趣的朋友可以访问TuGraph Analytics项目进行学习。 通过Chain，LangChain相当于以“工作流”的形式，将LLM与IO组件进行了有秩序的连接，从而具备构建复杂AI工程流程的能力。而我们都知道LLM提供的文本生成服务本身不提供记忆功能，需要用户自己管理对话历史。因此引入Memory组件，可以很好地扩展AI工程的能力边界。 LangChain的BaseMemory接口提供了Memory的统一抽象（截至v0.1.12还是Beta版本），提供了多种类型的Memory组件的实现，我们选用最简单的ConversationBufferMemory实现类型。 需要注意的是，要将Memory组件应用到Chain上，需要使用子类LLMChain进行创建Chain。 图计算是一种计算类型，主要处理的数据结构是图。图是由节点（或顶点）和边组成的，节点代表实体，边代表实体之间的关系。在图计算中，主要解决的问题是如何在图的结构上进行有效的计算和分析。 你问的问题是：“什么是图计算？” 这里可以看到，创建带Memory功能的Chain，并不能使用统一的LCEL语法。调用LLMChain使用的是predict而非invoke方法，直接调用invoke会返回一个LLMResult类型的结果。因此，LLMChain也不能使用管道运算符接StrOutputParser。这些设计上的问题，个人推测也是目前Memory模块还是Beta版本的原因之一吧。 但是，LangChain提供了工具类RunnableWithMessageHistory，支持了为Chain追加History的能力，从某种程度上缓解了上述问题。不过需要指定Lambda函数get_session_history以区分不同的会话，并需要在调用时通过config参数指定具体的会话ID。 SessionHistory必须是History接口类型BaseChatMessageHistory，用户可以根据需要选择不同的存储实现。这里为了简化，全局只用了一份内存类型的ChatMessageHistory。 拥有记忆后，确实扩展了AI工程的应用场景。但是在专有领域，LLM无法学习到所有的专业知识细节，因此在面向专业领域知识的提问时，无法给出可靠准确的回答，甚至会“胡言乱语”，这种现象称之为LLM的“幻觉”。 检索增强生成（RAG）把信息检索技术和大模型结合起来，将检索出来的文档和提示词一起提供给大模型服务，从而生成更可靠的答案，有效的缓解大模型推理的“幻觉”问题。 如果说LangChain相当于给LLM这个“大脑”安装了“四肢和躯干”，RAG则是为LLM提供了接入“人类知识图书馆”的能力。 相比提示词工程，RAG有更丰富的上下文和数据样本，可以不需要用户提供过多的背景描述，即能生成比较符合用户预期的答案。相比于模型微调，RAG可以提升问答内容的时效性和可靠性，同时在一定程度上保护了业务数据的隐私性。 但由于每次问答都涉及外部系统数据检索，因此RAG的响应时延相对较高。另外，引用的外部知识数据会消耗大量的模型Token资源。因此，用户需要结合自身的实际应用场景做合适的技术选型。 借助LCEL提供的RunnableParallel可以清晰描述RAG的计算图结构，其中最关键的部分是通过context键注入向量存储（Vector Store）的查询器（Retriever）。 蚂蚁图数据库目前没有公开信息表明已经开源。开源状态可能会随时间和公司政策变化，建议直接查阅蚂蚁集团或相关开源平台的官方信息获取最新和准确的消息。 是的，蚂蚁的图数据库产品TuGraph是开源的。 要构建更强大的AI工程应用，只有生成文本这样的“纸上谈兵”能力自然是不够的。工具不仅仅是“肢体”的延伸，更是为“大脑”插上了想象力的“翅膀”。借助工具，才能让AI应用的能力真正具备无限的可能，才能从“认识世界”走向“改变世界”。 这里不得不提到OpenAI的Chat Completion API提供的函数调用能力（注意这里不是Assistant的函数调用），通过在对话请求内附加tools参数描述工具的定义格式（原先的functions参数已过期），LLM会根据提示词推断出需要调用哪些工具，并提供具体的调用参数信息。用户需要根据返回的工具调用信息，自行触发相关工具的回调。下一章内容我们可以看到工具的调用动作可以通过Agent自主接管。 为了简化代码实现，我们用LangChain的注解@tool定义了一个测试用的“获取指定城市的当前气温”的工具函数。然后通过bind_tools方法绑定到LLM对象即可。需要注意的是这里需要用JsonOutputToolsParser解析结果输出。 [{'type': 'get_temperature', 'args': {'city': '杭州'}}] 实际上LangChain提供了大量的内置工具和工具库的支持。@tool只是提供了简洁的工具创建的支持，要定制复杂的工具行为需要自行实现BaseTool工具接口。同时工具库接口BaseToolkit下也有大量的实现，如向量存储、SQL数据库、GitHub等等。用户可以根据自身需求选用或自行扩展。 通用人工智能（AGI）将是AI的终极形态，几乎已成为业界共识。类比之，构建智能体（Agent）则是AI工程应用当下的“终极形态”。 Agent的核心思想是使用大型语言模型（LLM）来选择要采取的行动序列。在Chain中行动序列是硬编码的，而Agent则采用语言模型作为推理引擎来确定以什么样的顺序采取什么样的行动。 Agent相比Chain最典型的特点是“自治”，它可以通过借助LLM专长的推理能力，自动化地决策获取什么样的知识，采取什么样的行动，直到完成用户设定的最终目标。 我们使用Agent继续完成前边Tool部分没有完成的例子。这里使用create_openai_tools_agent方法创建一个简单的OpenAI工具Agent，AgentExecutor会自动接管工具调用的动作。如果希望给Agent添加记忆能力，依然可以采用前边Memory章节提过的RunnableWithMessageHistory的方案。 > Entering new AgentExecutor chain... Invoking: get_temperature with {'city': 'Hangzhou'} 16 今天杭州的气温是16度。 需要补充说明的是，LangChain提供了Hub功能，帮助大家管理共享Agent的提示词模板。上述示例代码的Agent提示词模板和hwchase17/openai-tools-agent的定义等价。 通过代码prompt = hub.pull(""hwchase17/openai-tools-agent"")可以直接引用创建prompt。 从API到Agent，我们“脑暴”了一个完整AI工程应用的发展轨迹，借此我相信大家对LangChain的设计理念应该有了进一步的理解。 最后，我们再看一眼LangChain的产品架构。除了本文未介绍的LangServe——将Chain部署为RESTful服务，其他不再赘述。 可能会有小伙伴疑问，为啥我一个搞图计算的，在这捣鼓起AI工具来了。抛开“拥抱行业，跟进趋势”这样的大口号不谈，单纯从工作需要角度，使用AI工具加速内容的产出，本身就可以帮助我节省大量的时间和精力，空出的“闲暇”时间去带娃也是极好的…… 线上关于LangChain的文章，虽不说汗牛充栋，但也能随手拈来。作为“后入场”者，我过去从事了多年的数仓、中台、云产品的架构和研发，最近两三年一直在搞图计算这样的基础软件，就想试试结合自身多面的工程经验去理解LangChain背后的设计理念，给大家一个别样的视角去观察、体验和思考AI工程。这两天被“AI程序员”的新闻刷屏了，其中引用的Adam Rackis的一条Twitter令我感受颇深：“做好磨练和深入学习的准备，熟练地引导人工智能取得好的结果可能才是未来程序员能体现出来的价值。”，或许这正是AI工程的用武之地吧。",https://www.cnblogs.com/fanzhidongyzby/p/18075179/langchain,2025-03-13 10:54:20
hez2010,未知作者,Brainfuck 是由 Urban Müller 在 1993 年创造的一门非常精简的图灵完备的编程语言。 正所谓大道至简，这门编程语言简单到语法只有 8 个字符，每一个字符对应一个指令，用 C 语言来描述的话就是： 然后只需要提供一个已经初始化为 0 的字节数组作为内存、一个指向数组的指针、以及用于输入输出的两个字节流就能够让程序运行了。 既然要用 C# 类型系统来构建 Brainfuck 的编译器，我们需要首先对 C# 类型系统有一些认知。 C# 的类型系统构建在 .NET 的类型系统之上，而众所周知 .NET 是一个有具现化泛型的类型系统的平台，意味着泛型参数不仅不会被擦除，还会根据泛型参数来分发甚至特化代码。 对于上面的代码，调用 new Foo ().Print() 会输出 0，调用 new Foo ().Print() 会输出 0001-01-01T00:00:00，而调用 new Foo ().Print() 则会输出 null。 你可能好奇为什么上面的 Calculator 里 left 和 right 可以直接加，这是因为 .NET 支持接口的虚静态成员。上面的 IAdditionOperators 接口其实定义长这个样子： 我们对 T 进行泛型约束 where T : IAdditionOperators 之后，就使得泛型代码中可以通过类型 T 直接调用接口中的静态抽象方法 operator+。 有了上面的知识，我想知道在这套类型系统之上，.NET 的编译器到底能生成多优化的代码，那接下来我们进行一些小的测试。 首先让我们用类型表达一下具有 int 范围的数字，毕竟之后构建 Brainfuck 编译器的时候肯定会用到。众所周知 int 有 32 位，用 16 进制表示那就是 8 位。我们可以给 16 进制的每一个数位设计一个类型，然后将 8 位十六进制数位组合起来就是数字。 这里我们想把数字和数位区分开，因此我们定义一个跟 IHex 长得差不多但是泛型的接口 INum 用来给数字 Int 实现，之所以是泛型的是因为给万一没准以后想要扩展点浮点数之类的做考虑： 这里我们给 Value 加了 [MethodImpl(MethodImplOptions.AggressiveInlining)] 确保这个方法会被编译器 inline。 如此一来，如果我们想表达一个 0x1234abcd，我们就可以用 Int 来表达。 对于 Brainfuck 源代码的解析，可以说是非常的简单，从左到右扫描一遍源代码就可以，这里就不详细说了。问题是怎么产生编译结果呢？ 我们 Brainfuck 不需要什么内存边界检查，因此这里我用了一个 UnsafeAt 扩展方法跳过边界检查： 然后，Brainfuck 是支持循环的，这要怎么处理呢？其实也很简单，模拟 while (*ptr) { 这个操作就行了，也就是反复执行当前操作更新指针，直到指针指向的数据变成 0，然后跳到下一个操作去。 这段程序很粗暴的分别把内存从左到右写成 Hello World! 的每一位，然后把指针移回到开头后逐位输出。 不过这么看 Hello World! 还是太长了，不适合用来一上来就展示，我们换个简单点的输出 123： 这里为了简洁，我把数字全都带入了数字类型，不然会变得很长。例如实际上 49 应该表达为 Int 。 如果我们想以 JIT 的形式运行 Brainfuck 代码，那如何在运行时生成类型然后运行代码呢？我们在 .NET 中有完善的反射支持，因此完全可以做到运行时创建类型。 你会发现，因为编译出的东西是类型，因此我们不仅可以在 JIT 环境下跑，还能直接把类型当作程序 AOT 编译出可执行文件！只需要编写一个入口点方法调用 Run 即可： 上面的 /p:IlcInstructionSet=native 即 C++ 世界里的 -march=native，OptimizationPreference=Speed 则是 -O2。 这里我们采用一段用 Brainfuck 编写的 Mandelbrot 程序进行性能测试，代码见 Pastebin。 最后 .NET AOT 在这个项目里取得了最好的成绩，当然，这离不开 .NET 类型系统层面的零开销抽象。,https://www.cnblogs.com/hez2010/p/18696074/brainfly-brainfuck-compiler-built-with-csharp,2025-03-13 10:54:10
老周,未知作者,"由于两个月的奋战，导致很久没更新了。就是上回老周说的那个产线和机械手搬货的项目，好不容易等到工厂放假了，我就偷偷乐了。当然也过年了，老周先给大伙伴们拜年了，P话不多讲，就祝大家身体健康、生活愉快。其实生活和健康是密不可分的，想活得好，就得健康。包括身体健康、思想健康、心理健康、精神健康。不能以为我无病无痛就很健康，你起码要全方位健康。 不管你的工作是什么，忙或者不忙，报酬高或低，但是，人，总得活，总得过日子。咱们最好多给自己点福利，多整点可以自娱自乐的东西，这就是生活。下棋、打游戏、绘画、书法、钓鱼、飙车、唢呐……不管玩点啥，只要积极正向的就好，可以大大降低得抑郁症、高血压的机率；可以减少70%无意义的烦恼；可以降低跳楼风险；在这个礼崩乐坏的社会环境中，可以抵御精神污染……总之，益处是大大的有。 然后老周再说一件事，一月份的时候常去工厂调试，也认识了机械臂厂商派的技术支持——吴大工程师。由于工厂所处地段非常繁华，因此每次出差，午饭只能在附近一家四川小吃店解决。毕竟这方圆百十里也仅此一家。不去那里吃饭除非自带面包蹲马路边啃，工厂不供食也不供午休场所。刚开始几次出差还真的像个傻子似的蹲马路边午休。后来去多了，直接钻进工厂的会议室睡午觉。 4、做过许多项目的一线开发者。我强调的项目数量多，而不是长年只维护一个项目的。只有数量多你学到的才多； 5、社区贡献较多者，这个和3差不多。不过，老周认为的社区贡献就是不仅提供代码，还提供文档、思路、技巧等； ========================================================================================================== 下面进入主题，咱们今天聊聊 IChangeToken。它的主要功能是提供更改通知。比如你的配置源发生改变了，要通知配置的使用者重新加载。你可能会疑惑，这货跟使用事件有啥区别？这个老周也不好下结论，应该是为异步代码准备的吧。 这个 Change Token 思路很清奇，实际功能类似事件，就是更改通知。咱们可以了解一下其原理，但如果你觉得太绕，不想了解也没关系的。在自定义配置源时，咱们是不需要自己写 Change Token 的，框架已有现成的。我们只要知道要触发更改通知时调用相关成员就行。 1、runtime-main\src\libraries\Common\src\Extensions\ChangeCallbackRegistrar.cs：这个主要是 UnsafeRegisterChangeCallback 方法，用于注册回调委托； 2、runtime-main\src\libraries\Microsoft.Extensions.Primitives\src\ChangeToken.cs：这个类主要是提供静态的辅助方法，用于注册回调委托。它的好处是可以循环——注册回调后，触发后委托被调用；调用完又自动重新注册，使得 Change Token 可以多次触发； 3、runtime-main\src\libraries\Microsoft.Extensions.Primitives\src\CancellationChangeToken.cs：这个类是真正实现 IChangeToken 接口的； 4、runtime-main\src\libraries\Microsoft.Extensions.Configuration\src\ConfigurationReloadToken.cs：这个也是实现 IChangeToken 接口，而且它才是咱们今天的主角，该类就是为重新加载配置数据而提供的。调用它的 OnReload 方法可以触发更改通知。 CancellationTokenSource 类表示一个取消任务的标记，访问它的 Token 属性可以获得一个 CancellationToken 结构体实例，可以检索它的 IsCancellationRequested 属性以明确是否有取消请求（有则true，无则false）。 还有更重要的，CancellationToken 结构体的 Register 方法可以注册一个委托作为回调，当收到取消请求后会触发这个委托。对的，这个就是 Change Token 灵魂所在了。一旦回调被触发后，CancellationTokenSource 就处于取消状态了，你无法再次触发，除非重置或重新实例化。这就是回调只能触发一次的原因。 下面，咱们完成一个简单的演示——用数据库做配置源。在 SQL Server 里面随便建个数据库，然后添加一个表，名为 tb_configdata。它有四个字段： ID和config_key设为主键，config_value 是配置的值，remark 是备注。备注字段其实可以不用，但实际应用的时候，可以用来给配置项写点注释。 然后，在程序里面咱们用到 EF Core，故要先生成与表对应的实体类。这里老周就不用工具了，直接手写更有效率。 上述代码的情况特殊，实体类的名称和成员名称与数据表并不一致，所以在重写 OnModelCreating 方法时，需要进行映射。 1、ToTable(""tb_configdata"") 告诉 EF 实体类对应的数据表是 tb_configdata； 2、HasKey(c => new { c.ID, c.ConfigKey })：表明该实体有两个主键——ID和ConfigKey。这里指定的是实体类的属性，而不是数据表的字段名，因为后面咱们会进行列映射； 3、HasColumnName(""config_key"")：告诉 EF，实体的 ConfigKey 属性对应的是数据表中 config_key。后面的几个属性的道理一样，都是列映射。 做映射就类似于填坑，如果你不想挖坑，那就直接让实体类名与表名一样，属性名与表字段（列）一样，这样就省事多了。不过，在实际使用中真没有那么美好。很多时候数据库是小李负责的，人家早就建好了，存储过程都写了几万个了。后面前台程序是老张来开发，对老张来说，要么把实体的命名与数据库的一致，要么就做一下映射。多数情况下是要映射的，毕竟很多时候数据库对象的命名都比较奇葩。尤其有上千个表的时候，为了看得顺眼，很多人喜欢这样给数据表命名：ta_XXX、ta_YYY、tb_ZZZ、tc_FFF、tx_PPP、ty_EEE、tz_WWW。还有这样命名的：m1_Report、m2_ReportDetails…… m105_TMD、m106_WNM、m107_DOUBI。 此处咱们不用直接实现 IConfigurationProvider 接口，而是从 ConfigurationProvider 类派生就行了。自定义配置源的东东老周以前写过，只是当时没有实现更改通知。 由于老周不知道怎么监控数据库更新，最简单的办法就是用定时器循环检查。重点是重写 Load 方法，完成加载配置的逻辑。Load 方法覆写后不需要调用 base 的 Load 方法，因为基类的方法是空的，调用了也没毛用。 在 Timer 对象调用的方法（OnTimer）中，先调用 Load 方法，再调用 OnReload 方法。这样就可以在加载数据后触发更改通知。 然后实现 IConfigurationSource 接口，提供 MyConfigurationProvider 实例。 默认的配置源有JSON文件、命令行、环境变量等，为了排除干扰，便于查看效果，在 Main 方法中咱们先把配置源列表清空，再添加咱们自定义的配置源。 这里你可能会有个疑问：连接字符串硬编码了不太好，要不写在配置文件中，可是，写在JSON文件中咱们怎么获取呢？毕竟 ConfigurationProvider 不使用依赖注入。 IConfigurationSource 不是有个 Build 方法吗？Build 方法不是有个参数是 IConfigurationBuilder 吗？用它，用它，狠狠地用它。 ----------------------------------------------------------------------------------------------------- 接下来，咱们弄个一对多的例子。逻辑是这样的：启动程序显示主窗口，接着创建五个子窗口。主窗口上有个大大的按钮，点击后，五个子窗口会收到通知。大概就这个样子： 窗口上只放了一个文本框。上面代码中，使用了 ChangeToken.OnChange 静态方法，为 Change Token 注册回调委托，本例中回调委托绑定的是 OnCallback 方法，也就是说：当 Change Token 触发后会在文本框中追加文本。OnChange 静态方法有两个重载： 上述例子用的是第一个，其实里面调用的也是第二个重载，只是把咱们传递的 OnCallback 方法当作 TState 传进去了。 请大伙伴暂时记住 changeTokenProducer 和 changeTokenConsumer 这两参数。changeTokenProducer 也是一个委托，返回 IChangeToken。用的时候一定要注意，每次触发之前，Change Token 要先创建新实例。注意是先创建新实例再触发，否则会导致无限。尽管内部会判断 HasChanged 属性，可问题是这个判断是在注册回调之后的。这个是跟 Change Token 的清奇逻辑有关，咱们看看 OnChage 的源代码就明白了。 简单来说，就是返回一个 ChangeTokenRegistration 实例，这是个私有类，咱们是访问不到的，以 IDisposable 接口公开。其中，它有两个方法是递归调用的： 在 ChangeTokenRegistration 类的构造函数中，先调用 RegisterChangeTokenCallback 方法，开始了整个递归套娃的过程。在 RegisterChangeTokenCallback 方法中，为 token 注册的回调就是调用 OnChangeTokenFired 方法。 而 OnChangeTokenFired 方法中，是先获取新的 Change Token，再触发旧 token。最后，又调用 RegisterChangeTokenCallback 方法，实现了无限套娃的逻辑。 因此，咱们在用的时候，必须先创建新的 Change Token 实例，然后再调用 RegisterChangeTokenCallback 实例的 Cancel 方法。不然这无限套娃会一直进行到栈溢出，除非你提前把 ChangeTokenRegistration 实例 Dispose 掉（由 OnChange 静态方法返回）。可是那样的话，你就不能多次接收更改了。 下面就是主窗口部分，也是最危险的部分——必须按照咱们上面分析的顺序进行，不然会 Stack Overflow。 按钮1的单击事件处理方法就是触发点，所以，CancellationTokenSource、CancellationChangeToken 要先换成新的实例，然后再用旧的实例去 Cancel。这里用 Interlocked 类会好一些，毕竟要考虑异步的情况，虽然咱这里都是在UI线程上传递的，但还是遵守这个习惯好一些。 这样处理就能避免栈溢出了。运行后，先打开五个子窗口（多点击一次就能创建十个子窗口）。接着点击大大按钮，五个子窗口就能收到通知了。",https://www.cnblogs.com/tcjiaan/p/18012397,2025-03-13 10:54:25
"Alfred Zhao


                从Oracle起航，领略精彩的IT技术。",未知作者,本文目标：部署 DeepSeek 本地模型，并通过 Ollama 提供 API 支持，Chatbox 提供 UI 界面。 原则：不搞那些高深的玩法，让小白也能理解并真正的上手实践。 目前deepseek-r1模型大小提供了7个选择：1.5b、7b、8b、14b、32b、70b、671b。 因为我笔记本的显卡配置较低，所以这里只能选择最小的1.5b模型来做演示： 你可以根据你的硬件情况选择，通常模型大小（参数量）越大，模型的理解和生成能力越强，但也会消耗更多的计算资源。 我这里选择macOS，点击下载。 下载文件大小不到200M，文件名为：Ollama-darwin.zip。 这里Ollama默认给出的例子是下载/运行llama3.2大模型， 我们这里不使用这个llama3.2模型，直接下载/运行deepseek，参数选择最小的1.5b，在终端窗口运行下面命令： 下载的Chatbox-1.9.7.dmg，大小100M多点，点击安装，按下面提示拖到Applications内： 注意，这里我故意选错成DeepSeek API，这也是初学者经常会选错的，实际上，若选择这个你就找不到你本地的DeepSeek模型。 实际正确应该选择OLLAMA API，然后就可以看到我们上一步安装好的deepseek-r1:1.5b。 配置好DeepSeek本地模型之后，就可以实现在断网情况下自由问答了，比如，此刻我正在写这篇文章，于是就问他帮我想几个备选的标题： 他真的迅速给我起了10个吸引眼球的标题，还提供了它思考的过程，而且在我这4年前的电脑上跑起来都很迅速。 嗯，真的很赞！不过我还是决定用自己最初想的朴实标题，不做标题党了。 Copyright © 2025 AlfredZhao Powered by .NET 9.0 on Kubernetes,https://www.cnblogs.com/jyzhao/p/18700202/shou-ba-shou-jiao-ni-bu-shu-deepseek-ben-de-mo-xin,2025-03-13 10:54:03
why技术,未知作者,在实际业务场景下，涉及到业务代码和不同的微服务，导致问题有点难以定位，但是最终分析出原因之后，发现可以用一个很简单的例子来演示。 这个代码的逻辑非常简单，首先我们搞了一个线程池，然后起一个 for 循环往线程池里面仍了 5 个任务，这是核心逻辑。 其他的 StopWatch 是为了统计运行时间用的。 至于 CountDownLatch，你可以理解为在业务流程中，需要这五个任务都执行完成之后才能往下走，所以我搞了一个 CountDownLatch。 这个代码运行起来是没有任何问题的，我们在日志中搜索“执行完成”，也能搜到 5 个，这个结果也能证明程序是正常结束的： 问题就是由于转异步了，所以 for 循环里面的任务中的 countDownLatch 很快就减到 0 了。 很简单嘛，需要子任务真正执行完成后，父任务的 countDownLatch 才能进行 countDown 的动作。 比如在这个场景下，我们直观上的感受是“卡住了”，那就 dump 一把线程，管它有枣没枣，打一杆子再说： 通过 Dump 文件，可以发现线程池的线程都在 MainTest 的第 30 行上 parking ，处于等待状态： 是父任务在等待子任务执行结束，运行 finally 代码，把 countDownLatchSub 的计数 countDown 到 0，才会继续执行： 换句话说就是父任务被拦住是因为子任务的 finally 代码中的 countDownLatchSub.countDown() 方法没有被执行。 如果线程池的任务之间存在父子关系，那么请不要使用同一个线程池。如果使用了同一个线程池，可能会因为子任务进了队列，导致父任务一直等待，出现假死现象。 这篇文章是 2020 年写的，其中就是使用了父子任务+CountDownLatch 的模式，来实现所谓的“多线程事务”。 它已经由一篇技术文章变成了一个生活的锚点，其中的蛛丝马迹，能让我从脑海深处想起之前生活中一些不痛不痒的印迹。,https://www.cnblogs.com/thisiswhy/p/18302939,2025-03-13 10:54:17
LemonDu,未知作者,外面，阳光明媚，一切正好。 就在我欢天喜地准备迎来愉快的双休时，忽然之间，天塌了。 刚上线一小时的服务内存异常OOM了！ 老实说，在我印象里OOM问题只存在于网上案例中，练习编码时常两年半，还是第一次遇到。不过既然遇到了，那就要尽快排查问题并解决掉，不然真要和群里大哥说的一样：要领盒饭了。 下午两点新版本上线，其中一个消费者服务的内存增长速度异常迅速，在短短五分钟内就用完了2G内存并自动重启了pod，之后又在五分钟内OOM了，在四十分钟内服务的pod已经重启了八十几次，要知道我们之前这个消费者服务正常运行时候只用了不到500M。 首先进行初步分析，这是一个消费者服务并且新版本的需求中并没有新增消费topic，并且业务量也没有大的波动，不存在是业务访问量骤增导致OOM，所以极大概率会是代码问题。当然，每一个版本的新代码都非常多，需求也比较庞杂，直接去看代码肯定是不行的，这时候就要麻烦部门的运维大佬了，让他给我们dump一下，给出一个内存溢出时的性能记录文件，通过这个文件可以分析内存分配、线程创建、CPU使用、阻塞、程序详细跟踪信息等。 内存OOM，那最重要的当然是mem文件，也就是内存分配剖析数据，不过很不幸，服务重启速度太快了，运维大佬dump的时候正好处于服务刚重启的时候，所以mem文件中显示的内存才占用不到20M，并且占比上也没看出有什么问题。想让运维再帮忙dump一下内存快要OOM的时候，但是为了线上服务的稳定性版本已经回退了，无法重新dump，只能从其他几个文件中查找问题了。 除了内存占用分析，在性能问题分析中CPU占用分析也是极为重要的一环，这一查看就有意思了，CPU总的使用率虽然不高，但是这个占比就比较奇怪了。第一占比的runtime.step是Go的运行时系统负责管理内存分配、垃圾回收、调度goroutine等底层操作，这个暂且不管，占比第二的居然是runtime.selectgo，这个就非常诡异了，select一般用于channel的非阻塞调用，但是问题是新增代码中没有地方显示地调用了select，那可以初步判断是底层源码中某处一直在调用select函数，不过目前还不知道是谁触发的这个调用，还需要继续查看其他文件。 之后继续查看互斥锁的情况，其实这个文件在目前这种情况下排查的价值已经不大了，因为出现问题的是内存溢出而不是CPU占用率，并且CPU占用率确实不是很高，而且Go中是有检索死锁的机制，大部分死锁是能够被Go发现并报一个deadlock错误，打开文件之后发现果然没有死锁发生。 接下来查看阻塞操作的分析情况，从解析结果中可以看出，select的阻塞时间遥遥领先，select出现这种情况只会是存在case但是没有default的时候，当所有case不符合的时候，负责这个select的goroutine会阻塞住直到存在符合的case出现才会唤醒继续走下去，当时我看到这我满脑子问号，谁家好人select不加default啊？ 看到这里还是没能定位到问题所在，但是别急，我们还有最重要的文件还没看，那就是trace文件，它可以记录程序执行的详细跟踪信息，包括函数调用、Goroutine 的创建和调度，使用go自带的pprof分析工具打开trace文件 在Goroutine分析中，可以锁定真正的问题所在了，在go-zero的core包下的collection文件在不到一秒内创建了两万多的Goroutine，虽然两万多数量不多，但是这个速度十分异常，最重要的是这个定时轮就很奇怪，这个项目中根本没有定时任务，接下来就很容易查询了，只要查找这次提交的代码中哪里使用到了collection包。 在新上线的版本中只有这一处用到了collection包，原本这里的意思是将建立一个缓存放到上下文中去传递，但是乍一看我没有看出有什么问题，过期时间也设置了，按照我原有理解过期时间到了就会自动释放掉，为什么还是会内存溢出了？但是我忽然意识到应该不是缓存引发的内存溢出，可能是协程过多引发的内存溢出，因为一个初始协程是2KB左右，如果数量过多也会造成内存不够。 为了探究根本原因，我点进了collection包的源码进行查看，在其中NewCache()方法中找到了造成协程数异常增加的定时轮创建方法NewTimingWheel()。 之后点进去这个方法进一步查看，可以看到这个定时轮的结构体，里面包含了四个channel以及一些其他数据结构，粗略估计这一个TimingWheel结构体所占内存要达到一百字节以上，这是一个比较庞大的对象，如果无限制的创建下去很容易造成内存OOM发生。 但是这个定时轮为什么会创建那么多呢？为什么不会关闭，按理说go-zero的源码不应该会有这么大的漏洞，继续查看这个定时轮的run()方法，终于发现了问题所在，这个定时轮中开启了select方法，并且没有default方法，所以之前阻塞文件那里才会显示select阻塞时间占比达到了99%，并且唯一关闭这个定时轮的方法是接收到tw.ticker.Stop()才会停止，那么这个stop方法会在什么时候调用呢？ 答案是会在程序停止运行的时候进行调用。所以如果程序仍在运行，就会有无限制的协程创建定时轮，这时候定时轮因为无法关闭所以协程也不会进行销毁，有点类似于守护线程，所以在协程无限制的创建下最终导致了线上内存OOM了。 那是不是说明go-zero的这块源码存在问题？其实不是的，是我们使用方法错误，正确的使用方法不应该将缓存创建在上下文中，而应该创建一个全局缓存，让所有的上下文都公用这一个缓存，这样就不会发生定时轮无限创建的问题。 后续将这块缓存放到了全局中，之后再重新发布观察了一小时左右，服务内存稳定在了500M以下，与以往服务消耗内存几乎一致。,https://www.cnblogs.com/lemondu/p/18652339,2025-03-13 10:54:13
开发中对象命名的一点思考,未知作者,在我所见过的项目中，大多数团队都倾向于“功能堆砌式”开发：需求来了就加逻辑或函数，却很少有人愿意花时间在设计上，尤其是在对象命名花费时间。这看似“快速实现需求”的方式，通常会对代码的可读性产生坏的影响，进而影响可维护性。 一个好的对象命名并非只是让代码表面看起来整洁；它背后关系到人类和 AI 对系统的认知方式，也会在后续维护和迭代中形塑程序员以及 AI 工具的行为。换句话说，合适的命名不但决定了当前代码的优雅程度，也会潜移默化地影响未来对代码进行修改、扩展和重构时的思维路径与决策过程。 下面，我们将通过几个示例，探讨如何通过合理的命名让对象真正体现业务含义与自主决策能力，而不是简单地扮演一个“被动执行者”。 想象你走进一家餐厅。你会如何点餐？是要一份宫保鸡丁，还是告诉厨师：“你先放油，然后大火爆炒，然后再加调料”？如果你选择后者，那么对应到程序设计中，大概就是下面的写法： FoodMaker ，它只是一个“做饭的人”或“执行器”，缺乏更多的“主观能动性”。这不仅增加了使用者在思维层面的负担，也显得对专业厨师的经验和技能不够尊重。 相比之下，更贴近现实的方式是直接告诉对方“我需要一份宫保鸡丁”，厨师会根据自身经验和对食材的理解来完成这道菜。例如： 这样，Chef 完整体现了专业性与自主性： 值得注意的是，很多以 “-er” 或 “-or” 结尾的对象（如 Manager、Processor、Controller、Validator 等）常常会呈现出类似的“过程化”倾向：它们更多体现的是过程集合而非业务主体。当我们把命名改为更能传达专业身份或业务角色的名词时，往往会看到对象的“自我意识”和“主观能动性”随之提升，从而让整个系统的抽象层次更高、可维护性更好。 在 AI 辅助编程工具的普及下，命名的清晰度在大型项目中显得尤为重要。模糊或过于抽象的名字不仅会增加团队成员、甚至让3个月后的你自己的认知负担，也会让 AI 在大量上下文中难以理解该对象的真实意图，甚至产生误导性补全。我们先来看一个不恰当的示例： 要让代码更易读、更具可维护性，也让 AI 分析更准确，我们可以将“餐厅运营”按照现实场景拆分成多个专门对象，让每个类名更能“自证其职”： 现实生活中，若宫保鸡丁的必需食材（例如花生）突然缺货，真正的专业厨师会主动寻找替代食材，而不会要求顾客重新“下指令”或“换个点餐方式”。同理，在软件中，一个设计得足够“智能”的对象，也应该能在外部条件或业务需求变化时，自行调整内部逻辑，而不影响调用者的使用方式。 局限： 下面的 Chef 内部自行决定花生是否可用，如果缺货就用其他食材替代。这样，即使后续有更多类似变化（换新调料、临时供应商等），也能集中在 Chef 内部调整，无需改动客户端调用代码。 好处： 无论花生库存如何，客户端调用方式始终一致：一个智能的 Chef 能在内部完成相应的“自适应”处理。 对象的命名，许多人往往只把它当作“好看”或“顺口”的问题，却忽视了它所暗含的业务理解深度与系统思维。当我们刻意避开 “-er” 结尾或“Service”、“utility”后缀等含糊标签，并让对象名真实反映其专业角色与业务职责时，能够有下述好处： 在软件工程中，命名的重要性通常被忽视，但命名本身潜移默化影响我们的编码时的思维。只有当我们真正将对象视为拥有“尊严”与“自主决策能力”的实体时，才能更容易构建出一个高内聚、易扩展、符合业务本质的系统。,https://www.cnblogs.com/CareySon/p/18711135,2025-03-13 10:54:39
师从名剑山,未知作者,"笔者尝试通过梳理字符编码的核心原理，同时简单的介绍一下常见标准，希望能够帮助各位读者构建对字符编码技术的基础认知框架。 在日常开发中，当我们尝试将中文输出到控制台时，点击编译。这时，细心的读者可能会关注到 VS 的控制台会输出一段这样的警告（也有可能是团队规定不允许有警告出现🌝）： 这一系列的问题，归根结底，就是一个字符在计算机中，应该怎么样来表示。也就是字符的编码问题。所以，让我们先来了解了解，现代计算机体系中的编码模型是什么样的。 这一系列问题，追根溯源，其实就是一个字符在计算机中该如何表示的问题，即字符的编码问题。那么，我们先来了解一下现代计算机体系中的编码模型是怎样的。 待编码字符集，定义字符的逻辑集合，不涉及具体的编码逻辑。这一层仅确定“汉”字属于某个字符集。（像 GB2312 就只收录了 6763 个常用的汉字和字符，一些生僻字就没有被收录进来。又比如 ASCII 中就没有中文字符。） 从抽象字符集（ACR）映射到一组非负整数，也就是为每一个字符分配一个唯一的二数字（码位/码点）。例如：Unicode、ASCII、USC、GBK等编码。 在 Unicode 中，“汉”，表示成：\u6C49，而在 GBK 中，“汉”，表示成：0xBABA。 一个从一组非负整数（来自 CCS）到一组特定宽度代码单元序列的映射。我们常说的 UTF-8、UTF-16、UTF-32 就是一个字符编码表。他规定了在抽象字符集中的“非负整数”怎么用字节表示。 通过上面的介绍，相信你对现代编码模型的五层有了基本的了解。感兴趣的读者可以去看 Unicode technical report #17 。 相信大家在日常的开发中，经常听到 Unicode、GB2312、GBK、UTF-8、UTF-16、UTF-32、ANSI，却又对这些概念比较模糊。首先要明确一点的是，Unicode、GB2312、GBK 都是编码字符集，而UTF-8、UTF-16、UTF-32 则是 Unicode 的编码字符表。ANSI 比较特殊，我们待会再具体介绍。 ASCII，全称American Standard Code for Information Interchange（美国信息交换标准代码），于 1963 年发布。标准 ASCII 采用 7 位二进制数来表示字符，因此它最多只能表示 128 个字符。 ASCII 编码虽然解决了英语的编码问题，但中文怎么办呢？汉字有那么多字。此时，就有了 GK2312 编码。 GB2312，又称 GB/T 2312-1980，全称《信息交换用汉字编码字符集·基本集》，与 1980 年由中国国家标准总局发布。GB2312 收录共收录 6763 个汉字，其中一级汉字3755个，二级汉字3008个；同时收录了包括拉丁字母、希腊字母、日文平假名及片假名字母、注音符号、俄语西里尔字母在内的682个字符。 GB2312 使用两个字节来表示，第一个字节称为“高位字节”，对应分区的编号（把区位码的“区码”加上特定值）；第二个字节称为“低位字节”，对应区段内的个别码位（把区位码的“位码”加上特定值）。 随着计算机技术在全世界的广泛应用，越来越多来自不同地区，拥有不同文字的人们也加入了计算机世界，同时也带来了越来越多的种类。在 1991 年，由一个非盈利机构 Unicode 联盟首次发布了 The Unicode Standard，旨在统一整个计算机世界的编码。 Unicode 的编码空间从 U+0000 到 U+10FFFF，划分为 17 个平面（plane），每个平面包含216 个码位（0x0000~0xFFFF），其中第一个平面称为基本多语言平面（Basic Multilingual Plane,BMP），其他平面称为辅助平面（Supplementary Planes）。 由于 GB2312 只收录了 6763 个汉字，有一些 GB2312 推出之后才简化的汉字，部分人用名字、繁体字等未被收录进标准，由中华人民共和国全国信息技术标准化技术委员会1995年12月1日制订了 GBK 编码。GBK 共收录 21886 个汉字和图形符号。 Unicode 转换格式（Unicode Transformation Format，简称 UTF），一个字符的 Unicode 编码虽然是确定的，但是由于不同系统平台的设计不一定一致，以及出于节省空间的目的，对 Unicode 编码的实现方式有所不同。所以就有着不同的 Unicode 转换格式：UTF-8、UTF-16、UTF-32。 UTF-8（8-bit Unicode Transformation Format）是一种用于实现Unicode的编码方式，它使用一到四个字节来表示一个字符。UTF-8具有良好的兼容性和效率，能够与ASCII字符集完全兼容，对于其他语言字符也能够以较高效的方式进行编码。 BOM，全称字节序标志（byte-order mark）。目的是为了表示 Unicode 编码的字节顺序。使用 BOM 模式会在文件头处添加 U+FEFF，对应到 UTF-8 格式的文件，则会在文件起始处添加三个字节：0xEF、0xBB、0xBF。 还记得我们之前在说字符编码方案时，说过 UTF-8 无需处理大端小端。那为什么不需要呢？ 字节序（Endianness）是指多字节数据（如一个整数或一个字符的多字节表示）在内存中的存储顺序。而对于 UTF-8 中，每个使用UTF-8存储的字符，除了第一个字节外，其余字节的头两个比特都是以""10""开始，除了第一个字符以外，其他都是唯一的。 但是 Unicode 标准并不要求也不推荐使用 BOM 来表示 UTF-8，但是某些软件如果第一个字符不是 BOM （或者文件里只包含 ASCII），则拒绝正确解释 UTF-8。 UTF-16 把 Unicode 字符集的抽象码位映射为 16 位长的整数（即码元）的序列，也就是说在 UTF-16 编码方式下，一个 Unicode 字符，需要一个或者两个 16 位长的码元来表示。因此 UTF-16 也是一种具体编码。 Unicode 的基本多语言平面（BMP）内，从U+D800到U+DFFF之间的码位区段是永久保留不映射到Unicode字符。UTF-16就利用保留下来的0xD800-0xDFFF区块的码位来对辅助平面的字符的码位进行编码。 同样我们也以“汉”字为例，它在 Unicode 中为：U+6C49，处于 BMP 中，所以直接用 0x6C49 表示。而另外一个以U+10437编码（𐐷）为例: 那么 ANSI 是什么编码呢？简而言之，ANSI 不是某一种特定的字符编码，而是在不同系统中，表示不同的编码。 在编译器中显式设置输入字符集和执行字符集。对于GCC编译器，可以使用 -finput-charset=UTF-8 -fexec-charset=UTF-8 选项；对于MSVC编译器，可以使用 /source-charset:utf-8 /execution-charset:utf-8 选项，你也可以使用 /utf-8来指定输入字符集和执行字符集都为 UTF-8。 如果输入字符集和执行字符集不一致，编译器需要在编译过程中进行字符编码的转换。当两者不一致时，编译器需进行编码转换，可能引发： 在计算机发展的早期阶段，ASCII编码（美国信息交换标准代码）是主流的字符编码方式，它使用7位二进制数表示128个字符，包括英文字母、数字和一些标点符号。然而，ASCII编码无法满足多语言环境的需求，因为世界上有成千上万种语言和符号。 为了解决这个问题，操作系统和软件开发商引入了代码页的概念。代码页允许系统支持多种字符集，尤其是那些超出ASCII范围的语言字符。在Windows操作系统中，代码页是系统用来处理文本数据的机制。例如，当用户在系统中输入或显示文本时，系统会根据当前的代码页设置来解释这些字符。 假设你有一个文本文件，内容是中文字符“你好”。如果这个文件是用GBK编码保存的，那么它的字节序列可能是 C4 E3 BA C3。操作系统会根据代码页936（GBK）来解释这些字节，并正确显示为“你好”。但如果系统错误地使用了代码页1252（西欧字符集），这些字节会被解释为乱码，因为代码页1252中没有对应的字符。 首先，在 Windows 下，控制台的默认编码是当前系统的代码页（通常是 GB2312），所以如果你输出到控制台的字符不是当前代码页编码对应的字符，那么就会发生乱码。当前系统的代码页通过 cmd 执行命令 chcp来查看。 假如文件的源格式是 UTF-8，那么""中文""这两个字的字节序列为： 涓(E4B8)(ADE6)枃(9687)，其中 ADE6 在 GB2312 中为错误编码，所以会显示一个问号。 这一个问题与输入字符集有关，当文件编码与编译器预期不一致，例如你的文件是GB2312编码，但编译器（如MSVC）默认使用UTF-8（代码页65001）来解析源文件。GB2312和UTF-8是不兼容的编码格式，导致编译器无法正确解析文件中的字符。 笔者的 Visual Studio 工程命令行有一个 /utf-8，也就代表输入、执行编码集都为 utf-8。所以，当你文件的编码为 GB2312 时， 当编译器以UTF-8编码解析文件时，会将 GB2312编码的字节序列 0xD4 0xB4 视为一个潜在的UTF-8字符。然而，根据UTF-8的编码规则： 0xD4 是一个以 1101 开头的字节，表示这是一个两字节字符，第一个字节的格式应为 110xxxxx ，第二个字节的格式应为 10xxxxxx 。但是， 0xD4 的二进制是 11010100 ，而 0xB4 的二进制是 10110100 。 虽然第二个字节符合 10xxxxxx 的格式，但第一个字节的值 0xD4 超出了UTF-8两字节字符的合法范围（ 0xC0 到 0xDF ），因此整个字节序列 0xD4 0xB4 是无效的UTF-8字符。 QString stores a string of 16-bit QChars, where each QChar corresponds to one UTF-16 code unit. 所以，调用上面这些函数就是用指定的格式读取字符，并将这些字符转换成 UTF-16 格式。参看下面的例子： 感谢各位读者阅读本博客，本博客内容在创作过程中，参考了大量百科知识以及其他优秀博客，并结合笔者自身在实际工作中遇到的相关问题。笔者希望通过这篇博客，能为各位读者在字符编码这一块提供一些有价值的见解和帮助。 在撰写过程中，笔者深刻体会到了字符编码这一主题的复杂与广泛，所以如果有哪些言论有失偏颇的，欢迎各位读者在留言区留言，也欢迎大家热烈讨论。",https://www.cnblogs.com/codegb/p/18768600,2025-03-13 10:54:35
LemonDu,未知作者,外面，阳光明媚，一切正好。 就在我欢天喜地准备迎来愉快的双休时，忽然之间，天塌了。 刚上线一小时的服务内存异常OOM了！ 老实说，在我印象里OOM问题只存在于网上案例中，练习编码时常两年半，还是第一次遇到。不过既然遇到了，那就要尽快排查问题并解决掉，不然真要和群里大哥说的一样：要领盒饭了。 下午两点新版本上线，其中一个消费者服务的内存增长速度异常迅速，在短短五分钟内就用完了2G内存并自动重启了pod，之后又在五分钟内OOM了，在四十分钟内服务的pod已经重启了八十几次，要知道我们之前这个消费者服务正常运行时候只用了不到500M。 首先进行初步分析，这是一个消费者服务并且新版本的需求中并没有新增消费topic，并且业务量也没有大的波动，不存在是业务访问量骤增导致OOM，所以极大概率会是代码问题。当然，每一个版本的新代码都非常多，需求也比较庞杂，直接去看代码肯定是不行的，这时候就要麻烦部门的运维大佬了，让他给我们dump一下，给出一个内存溢出时的性能记录文件，通过这个文件可以分析内存分配、线程创建、CPU使用、阻塞、程序详细跟踪信息等。 内存OOM，那最重要的当然是mem文件，也就是内存分配剖析数据，不过很不幸，服务重启速度太快了，运维大佬dump的时候正好处于服务刚重启的时候，所以mem文件中显示的内存才占用不到20M，并且占比上也没看出有什么问题。想让运维再帮忙dump一下内存快要OOM的时候，但是为了线上服务的稳定性版本已经回退了，无法重新dump，只能从其他几个文件中查找问题了。 除了内存占用分析，在性能问题分析中CPU占用分析也是极为重要的一环，这一查看就有意思了，CPU总的使用率虽然不高，但是这个占比就比较奇怪了。第一占比的runtime.step是Go的运行时系统负责管理内存分配、垃圾回收、调度goroutine等底层操作，这个暂且不管，占比第二的居然是runtime.selectgo，这个就非常诡异了，select一般用于channel的非阻塞调用，但是问题是新增代码中没有地方显示地调用了select，那可以初步判断是底层源码中某处一直在调用select函数，不过目前还不知道是谁触发的这个调用，还需要继续查看其他文件。 之后继续查看互斥锁的情况，其实这个文件在目前这种情况下排查的价值已经不大了，因为出现问题的是内存溢出而不是CPU占用率，并且CPU占用率确实不是很高，而且Go中是有检索死锁的机制，大部分死锁是能够被Go发现并报一个deadlock错误，打开文件之后发现果然没有死锁发生。 接下来查看阻塞操作的分析情况，从解析结果中可以看出，select的阻塞时间遥遥领先，select出现这种情况只会是存在case但是没有default的时候，当所有case不符合的时候，负责这个select的goroutine会阻塞住直到存在符合的case出现才会唤醒继续走下去，当时我看到这我满脑子问号，谁家好人select不加default啊？ 看到这里还是没能定位到问题所在，但是别急，我们还有最重要的文件还没看，那就是trace文件，它可以记录程序执行的详细跟踪信息，包括函数调用、Goroutine 的创建和调度，使用go自带的pprof分析工具打开trace文件 在Goroutine分析中，可以锁定真正的问题所在了，在go-zero的core包下的collection文件在不到一秒内创建了两万多的Goroutine，虽然两万多数量不多，但是这个速度十分异常，最重要的是这个定时轮就很奇怪，这个项目中根本没有定时任务，接下来就很容易查询了，只要查找这次提交的代码中哪里使用到了collection包。 在新上线的版本中只有这一处用到了collection包，原本这里的意思是将建立一个缓存放到上下文中去传递，但是乍一看我没有看出有什么问题，过期时间也设置了，按照我原有理解过期时间到了就会自动释放掉，为什么还是会内存溢出了？但是我忽然意识到应该不是缓存引发的内存溢出，可能是协程过多引发的内存溢出，因为一个初始协程是2KB左右，如果数量过多也会造成内存不够。 为了探究根本原因，我点进了collection包的源码进行查看，在其中NewCache()方法中找到了造成协程数异常增加的定时轮创建方法NewTimingWheel()。 之后点进去这个方法进一步查看，可以看到这个定时轮的结构体，里面包含了四个channel以及一些其他数据结构，粗略估计这一个TimingWheel结构体所占内存要达到一百字节以上，这是一个比较庞大的对象，如果无限制的创建下去很容易造成内存OOM发生。 但是这个定时轮为什么会创建那么多呢？为什么不会关闭，按理说go-zero的源码不应该会有这么大的漏洞，继续查看这个定时轮的run()方法，终于发现了问题所在，这个定时轮中开启了select方法，并且没有default方法，所以之前阻塞文件那里才会显示select阻塞时间占比达到了99%，并且唯一关闭这个定时轮的方法是接收到tw.ticker.Stop()才会停止，那么这个stop方法会在什么时候调用呢？ 答案是会在程序停止运行的时候进行调用。所以如果程序仍在运行，就会有无限制的协程创建定时轮，这时候定时轮因为无法关闭所以协程也不会进行销毁，有点类似于守护线程，所以在协程无限制的创建下最终导致了线上内存OOM了。 那是不是说明go-zero的这块源码存在问题？其实不是的，是我们使用方法错误，正确的使用方法不应该将缓存创建在上下文中，而应该创建一个全局缓存，让所有的上下文都公用这一个缓存，这样就不会发生定时轮无限创建的问题。 后续将这块缓存放到了全局中，之后再重新发布观察了一小时左右，服务内存稳定在了500M以下，与以往服务消耗内存几乎一致。,https://www.cnblogs.com/lemondu/p/18652339,2025-03-13 10:54:12
ChokCoco,未知作者,"你可以在这里看到：CodePen Demo -- CSS Various Button Shapes | CSS 各种造型按钮 接下来几篇文章中，将在上述基础上，额外补充一些在日常设计稿中，常见的，可能出现的更为复杂的几个按钮，本文，我们来尝试实现这个造型： 此造型如果内部无需镂空，整体还是比较简单的，利用 background: radial-gradient() 径向渐变或者 mask，都能比较轻松的实现。 假设我们有一张矩形背景图案，我们只需要使用 mask 实现一层遮罩，利用 mask 的特性，把 4 个角给遮住即可。 上面的方法以及不适用了，原因在于很难利用 mask 制造一个不规则的内凹圆角形状进行裁剪，因此，我们必须另辟蹊径。 起始角度以及角向渐变的圆心没有改变，但是只让前 90deg 的图形为粉色，而后 270deg 的图形，设置为了透明色。 接下来，我们再给上述图形，增加一个 background-position: -25px, -25px： 为什么会有这样一种现象？如果我们在代码中加入 background-repeat: no-repeat： 解释一下，上述渐变语句的含义是在 30px 30px 处，实现一个半径为 30px 的径向渐变，渐变颜色为透明到黑色到透明，因此可以得到一个圆环： 为了方便大家看清楚 div 整体大小，利用 border: 1px solid #ddd 展示了整个 div 的轮廓，下图开始隐藏 border 然后，利用上述的技巧，我们让渐变图形，整体偏移 -30px -30px，也就是让径向渐变图形的圆心，处于div的左上角 0 0 坐标处。 到这，大家应该能恍然大悟了吧。剩下的工作就比较简单了，我们只需要利用多重线性渐变，把剩余的线条补充出来即可，整个图形完整的代码如下： 当然，我们肯定是需要不同边框大小、颜色的各种不镂空的内凹圆角边框，我们利用 CSS 变量再进行一下封装： 完整的代码，你可以戳这里：CodePen Demo -- 内凹角边框 & inner-corner-border 更多精彩 CSS 技术文章汇总在我的 Github -- iCSS ，持续更新，欢迎点个 star 订阅收藏。",https://www.cnblogs.com/coco1s/p/18699898,2025-03-13 10:54:06
开发中对象命名的一点思考,未知作者,在我所见过的项目中，大多数团队都倾向于“功能堆砌式”开发：需求来了就加逻辑或函数，却很少有人愿意花时间在设计上，尤其是在对象命名花费时间。这看似“快速实现需求”的方式，通常会对代码的可读性产生坏的影响，进而影响可维护性。 一个好的对象命名并非只是让代码表面看起来整洁；它背后关系到人类和 AI 对系统的认知方式，也会在后续维护和迭代中形塑程序员以及 AI 工具的行为。换句话说，合适的命名不但决定了当前代码的优雅程度，也会潜移默化地影响未来对代码进行修改、扩展和重构时的思维路径与决策过程。 下面，我们将通过几个示例，探讨如何通过合理的命名让对象真正体现业务含义与自主决策能力，而不是简单地扮演一个“被动执行者”。 想象你走进一家餐厅。你会如何点餐？是要一份宫保鸡丁，还是告诉厨师：“你先放油，然后大火爆炒，然后再加调料”？如果你选择后者，那么对应到程序设计中，大概就是下面的写法： FoodMaker ，它只是一个“做饭的人”或“执行器”，缺乏更多的“主观能动性”。这不仅增加了使用者在思维层面的负担，也显得对专业厨师的经验和技能不够尊重。 相比之下，更贴近现实的方式是直接告诉对方“我需要一份宫保鸡丁”，厨师会根据自身经验和对食材的理解来完成这道菜。例如： 这样，Chef 完整体现了专业性与自主性： 值得注意的是，很多以 “-er” 或 “-or” 结尾的对象（如 Manager、Processor、Controller、Validator 等）常常会呈现出类似的“过程化”倾向：它们更多体现的是过程集合而非业务主体。当我们把命名改为更能传达专业身份或业务角色的名词时，往往会看到对象的“自我意识”和“主观能动性”随之提升，从而让整个系统的抽象层次更高、可维护性更好。 在 AI 辅助编程工具的普及下，命名的清晰度在大型项目中显得尤为重要。模糊或过于抽象的名字不仅会增加团队成员、甚至让3个月后的你自己的认知负担，也会让 AI 在大量上下文中难以理解该对象的真实意图，甚至产生误导性补全。我们先来看一个不恰当的示例： 要让代码更易读、更具可维护性，也让 AI 分析更准确，我们可以将“餐厅运营”按照现实场景拆分成多个专门对象，让每个类名更能“自证其职”： 现实生活中，若宫保鸡丁的必需食材（例如花生）突然缺货，真正的专业厨师会主动寻找替代食材，而不会要求顾客重新“下指令”或“换个点餐方式”。同理，在软件中，一个设计得足够“智能”的对象，也应该能在外部条件或业务需求变化时，自行调整内部逻辑，而不影响调用者的使用方式。 局限： 下面的 Chef 内部自行决定花生是否可用，如果缺货就用其他食材替代。这样，即使后续有更多类似变化（换新调料、临时供应商等），也能集中在 Chef 内部调整，无需改动客户端调用代码。 好处： 无论花生库存如何，客户端调用方式始终一致：一个智能的 Chef 能在内部完成相应的“自适应”处理。 对象的命名，许多人往往只把它当作“好看”或“顺口”的问题，却忽视了它所暗含的业务理解深度与系统思维。当我们刻意避开 “-er” 结尾或“Service”、“utility”后缀等含糊标签，并让对象名真实反映其专业角色与业务职责时，能够有下述好处： 在软件工程中，命名的重要性通常被忽视，但命名本身潜移默化影响我们的编码时的思维。只有当我们真正将对象视为拥有“尊严”与“自主决策能力”的实体时，才能更容易构建出一个高内聚、易扩展、符合业务本质的系统。,https://www.cnblogs.com/CareySon/p/18711135,2025-03-13 10:53:54
救园行动——赞助商计划,未知作者,在求救的同时，我们也在想办法奋力自救，会采取一系列的救园行动，这一次一定要把园子救下来，因为没有下一次了。 有人可能会笑话这个救园计划是一个幼稚的想法，没有直接的收益，哪个企业会愿意花钱支持一个困境中的技术社区。 但我们还是想试一试，万一找到呢，哪怕只找到一家，也会帮助加快救园，也会让大家见证的确有这样非同寻常的企业。,https://www.cnblogs.com/cmt/p/18333440,2025-03-13 10:54:29
感谢「河南图奕网络」赞助园子，成为第一家创始赞助商,未知作者,在8月1日发布救援行动-赞助商计划后，我们并没有抱什么奢望，更没有妄想很快能找到赞助商，只是为救园多一点可能的希望，万一找到一家赞助商，就会多一份救园力量。 没想到第2天就有幸遇到一家有意向的企业，中午加微信开始沟通赞助商计划的细节，晚上快7点的时候就收到了赞助款，万一很快成真。 相信这样非同寻常的公司做产品也会很用心，而且他们的产品目标用户群也是开发者，欢迎大家关注一下他们旗下的两款产品。 IShell现在有个小福利，加入IShell用户交流QQ群时备注「博客园」（QQ群号：936123255），可以免费领取三个月VIP。,https://www.cnblogs.com/cmt/p/18341478,2025-03-13 10:53:47
探秘Transformer系列之（1）：注意力机制,未知作者,"因为各种事情，好久没有写博客了，之前写得一些草稿也没有时间整理（都没有时间登录博客和微信，导致最近才发现好多未读消息和私信，在这里和各位朋友说下万分抱歉）。现在恢复更新，是因为最近有些从非AI领域转过来的新同学来找我询问是否有比较好的学习资料，他们希望在短期内迅速上手 Transformer。我在网上找了下，但是没有找到非常合适的系统的学习资料，于是就萌发了自己写一个系列的想法，遂有此系列。在整理过程中，我也发现了自己很多似是而非的错误理解，因此这个系列也是自己一个整理、学习和提高的过程。 本文为系列第一篇，主要目的是引入Transformer概念和其相关背景。在2017年，Google Brain的Vaswani等人在论文”Attention is All You Need“中发布了Transformer。原始论文中给出Transformer的定义如下： Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution。 其中提到了sequence，RNN，convolution，self-attention等概念，所以我们接下来就从这些概念入手进行分析。我们先开始从Seq2Seq介绍，然后逐渐切换到注意力机制，最后再导出Transformer模型架构。 seq2seq（Sequence to Sequence/序列到序列）概念最早由Bengio在2014年的论文“Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation”中提出，其代表从一个源序列生成一个目标序列的操作。因为机器翻译是大家较熟悉且容易理解的领域，因此后续我们主要使用机器翻译来进行讲解，避免引入过多概念。 机器翻译其实就是文本生成。语言模型将文本看作是时间序列。在此视角下，每个单词都和它之前的单词相关，通过学习前面单词序列的统计规律就可以预测下一个单词。因此，机器翻译会从概率角度对语言建模，让新预测的单词和之前单词连成整个句子后最合理，即原有句子加上新预测单词后，成为整个句子的概率最大。这就涉及到自回归模型。 自回归（Autoregressive）模型是一种生成模型，其语言建模目标是根据给定的上下文来预测下一个单词。遵循因果原则（当前单词只受到其前面单词的影响），自回归模型的核心思想是利用一个变量的历史值来预测其未来的值，其将""序列数据的生成""建模为一个逐步预测每个新元素的条件概率的过程。在每个时间步，模型根据之前生成的元素预测当前元素的概率分布。 下图给出了自回归模型的示例。模型每次推理只会预测输出一个 token，当前轮输出token 与历史输入 token 拼接，作为下一轮的输入 token，这样逐次生成后面的预测token，直到新输出一个结束符号或者句子长度达到预设的最大阈值。就下图来说，模型执行序列如下： 将预测出的第一个单词“wear”结合原输入一起提供给模型，即第二次模型的输入是“You should wear”。 将预测出的第二个单词“shoes”结合原输入一起提供给模型，即第三次模型的输入是“You should wear shoes”。 该过程中的每一步预测都需要依赖上一步预测的结果，且从第二轮开始，前后两轮的输入只相差一个 token。 隐变量模型是一种引入隐变量来表示过去信息的模型。自回归模型在预测时会把过去观测到的信息总结起来记作\(h_t\)，并且更新预测\(x_t\)。即总结\(h_t = g(h_{t-1}, x_{t-1})\)，然后基于\(x_t = P(x_t \mid h_{t})\)来估计\(x_t\)。由于\(h_t\)从未被观测到，因此\(h_t\)就是隐变量，这类模型也被称为隐变量自回归模型（latent autoregressive models）。 有了\(ℎ_t\)之后，其实预测变换为两个子问题。一个问题是如何根据之前的隐变量 \(ℎ_{t−1}\) 和之前的输入信息\(x_{t-1}\)得到现在的隐变量\(ℎ_t\)，另一个问题是如何根据当前的隐变量\(ℎ_t\)和之前的输入\(x_{t-1}\)得到当前的\(x_t\) 。其实，这就是编码器-解码器模型要面对的问题。 目前，处理序列转换的神经网络模型大多是编码器-解码器（Encoder-Decoder）模型。传统的RNN架构仅适用于输入和输出等长的任务。然而，大多数情况下，机器翻译的输出和输入都不是等长的，因此，对于输入输出都是变长的序列，研究人员决定使用一个定长的状态机来作为输入和输出之间的桥梁。于是人们使用了一种新的架构：前半部分的RNN只有输入，后半部分的RNN只有输出（上一轮的输出会当作下一轮的输入以补充信息），两个部分通过一个隐状态（hidden state）来传递信息。把隐状态看成对输入信息的一种编码的话，前半部分可以叫做编码器（Encoder），后半部分可以叫做解码器（Decoder）。这种架构因而被称为编码器-解码器架构，所用到的模型就是编码器-解码器模型，具体如下图所示，图中编码器和解码器通过一个中间隐状态C来完成信息交互。 如何压缩？人们很容易想到马尔可夫假设，即系统的未来状态只与当前状态有关。这也被称为近因效应：从文本生成角度看，就是当前词只与距离它比较近的个词更加相关。如果考虑前面n个单词，这就得到了N-gram模型，即当前单词的概率取决于前n个单词。但是基于马尔科夫假设的模型难以处理句子中的长距离依赖关系（某个单词依赖句子中很早的单词），也没有考虑深层语义关系。而且N-gram模型的大小几乎是n的指数倍，如果n数目太大，则n个单词组合数可能性太多，对资源消耗过大，因此需要新的模型。新模型应该不仅仅简单的关注单词的频率和顺序，还可以考虑更长距离的依赖关系，却又不必显式考虑那么多单词组合可能性，于是人们想到使用神经网络来拟合。 MLP 是最基本的神经网络模型之一，它可以将词向量序列映射到一个固定长度的向量表示，然后将这个向量输入到一个softmax层中，计算出下一个词的概率分布。MLP虽然理论上没有距离长依赖问题，但是很难训练好。CNN/RNN/Transformer等网络结构都可以看作是在MLP上添加的约束条件。通过这些先验约束，同等参数量下的优化难度被降低，模型更容易找到最优解。前面提到的“使用神经网络来拟合”就是使用CNN，RNN或者Transformer来实现编码器和解码器。 因为我们本系列介绍的主角是Transformer，就说明Transformer在实现编码器和解码器上是存在优势的，因此我们先来看看CNN和RNN方案的问题所在。 注：本节只是从普遍意义或者说是在典型问题上进行阐释，并非定论。因为CNN和RNN方案也是在不停的发展，其某一个阶段的方案可能解决（或者缓解）了下面提到的问题。 当面临冗长且信息密集的输入序列时，编码器-解码器模型在整个解码过程中保持相关性的能力可能会减弱。为了更好的说明，我们先看看序列转换面对的主要技术挑战：对齐问题和长依赖问题（或者说是遗忘问题）。 我们来看看为什么要对齐。首先，在某些领域（比如语音识别）中，虽然输入与输出的顺序相同，但是没有一一对应的关系。其次，在某些领域（比如机器翻译）中，在处理完整个输入序列后，模型的输出序列可能和输入序列的顺序不一致。以机器翻译为例，假如我们要让模型将英语 ""how are you"" 翻译为中文 ""你好吗""，或者将“Where are you”翻译成“你在哪里”，我们会发现翻译结果中的语序和原来句子的语序并不相同，同时，一些翻译结果并不能与英语中的词汇一一对应到。 不对齐问题带来的最大困境是：在时间序列的 t 时刻，我们并不能确定此时的模型是否已经获得了输出正确结果所需要的所有信息。因此，人们往往先把所有输入编码到一个隐状态，然后逐步对这个隐状态进行解码，这样才能确保在解码过程中模型一定收到了所需的全部信息。虽然此方案可以保证输入信息的完整性，但却有一个明显缺陷，即在解码过程中，无法确定贡献度。比如，当把”I love you""翻译成“我爱你”时，“我”应该与“I”对齐（因为其贡献最大），但是该方案中，""I"",""love"",""you""这三个词对“我”的贡献都是一致的。 我们以下面句子为例来进行分析。“秋蝉的衰弱的残声，更是北国的特产，因为北平处处全长着树，屋子又低，所以无论在什么地方，都听得见它们的啼唱。” 将例句从英文翻译成中文时，英文和中文明显是有对齐关系的，因此需要知道哪个英文单词对应到哪个中文。比如将”它们“翻译成”They“，但是“它们”代表什么呢？是“树”？“屋子“？还是”秋蝉“？通过”啼唱“和知识，我们知道“秋蝉”和“它们”指代是同一个对象，但是如果把”听得见它们的啼唱“ 修改为“看见它们的树荫”，则“它们”指代的就是“树”了。 人类可以很容易的同时看到“秋蝉”和“它们”这两个词然后把这两个词关联起来，即人们知道“它们”和“秋蝉”有长距离的依赖关系，从而理解整个句子。但是对于计算机或者对于模型来说，”秋蝉“和”它们“在例句中的距离太长了，很容易被两个词中间的其它词干扰。为了准确给出最终的答案，神经网络需要对前面”秋蝉的衰弱的残声“和”它们“之间的交互关系进行建模。然而，某些神经网络很难处理长距离依赖关系，因为处理这种依赖关系的关键因素之一是信号在网络中穿越路径的长度，两个位置之间路径越短，神经网络就越容易学习到这种长距离依赖关系，两个位置之间距离越远，建模难度就越大。如果模型无法处理长距离依赖，则会出现长时信息丢失，也就是产生了遗忘问题。 CNN的本质是学习空间数据中的局部依赖关系。CNN的卷积操作可以提取重要特征，但是因为单个卷积核的长度一般比较小，所以CNN的卷积感受视野是局部的，提取的是局部特征，进行局部信息计算。即CNN对相对位置敏感，对绝对位置不敏感，难以提取序列的长距离依赖关系。 为了让CNN可以处理长序列，人们一般会堆叠更多的卷积，通过叠加多层卷积区去把局部感受野进行扩大，让卷积网络依靠深度来弥补全局信息的缺失，从而捕捉长距离依赖。在这种方案中，不同卷积层会提供不同层次的特征，进而在上层完成长距离信息依赖计算，把长序列的信息压缩到一个卷积窗口内部，以此来让模型有机会捕捉到长距离依赖和复杂的结构关系。 比如下图所示，最下层CNN使用滑动窗口作用于这个文本序列，每个窗口都对其中的数据进行处理。A1窗口获取到了“秋蝉”这个信息，A3窗口获取到了“它们”这个信息。但是，因为“秋蝉”和“它们”距离太远，所以没有一个单一窗口可以把这两个词建立起依赖关系，即没有单一窗口可以同时看到这两个词。因此，模型只能持续堆叠卷积网络，不断加深整个网络，这样C1这个窗口才可以同时包含“秋蝉”和“它们”的信息。 但是深度就代表间接，间接就代表着流失。在CNN方案中，因为信息被“逐级加工，层层抽象”，而且信息传递过程不够透明，信息会在过深的网络传播过程之后只有部分保留，导致模型性能下降。所以CNN一般在长依赖关系建模场景使用较少，更适合短文本计算。 从表象上看，RNN是时序结构，后面的时刻天然就依赖于前面时刻的输出。从本质上来说，RNN是一种连接主义模型，具有选择性地在序列步骤间传递信息的能力，可以对序列上下文信息以及不同范围元素间的局部依赖关系进行捕捉。RNN的独到之处在于引入了“记忆”功能，让网络能记住之前输入的信息。随着数据在RNN中的流动，之前时间步的记忆会作为输入参与到当前数据的处理中，让模型能够动态地融合时间上下文和序列的历史信息。因为可以有效地处理变长的序列数据，理论上，RNN可以预测无限长句子，利用前面所有信息，所以非常适合翻译场景。 事实上，在Transformer出现之前，编码器和解码器通常是由RNN或其变体（如 LSTM 或 GRU）组成的。我们先看看如何用RNN实现编码器。以下图为例，编码器需要把“北国的特产”这个句子编码成隐状态。图中的每个方块都是一个简单的RNN单元。每个RNN单元接收两个输入（输入词和隐状态），输出一个隐状态。 模型在第一步得到输入h0和“北”，调用函数f()进行计算，得到输出h1=f(h0, 北)。h0是第一个隐状态（通常内容是数值0或者随机值）。模型在第二步得到输入h1和“国”，同样调用函数f()进行计算，得到h2。以此类推，模型最终输出h5。在计算过程的每一步t，前面所有节点得到的信息都保存在上一步计算出来的中间隐状态ht中，因此，后一个词的计算都会用到前面所有词的输出结果。 可以把隐藏状态ht看作是信息循环的载体，其承载的信息可以跨时间步在RNN中进行传递。随着数据在RNN中的流动，之前时间步的激活状态会作为输入参与到当前数据的处理中，让模型能够动态地融合时间上下文和序列的历史信息。因此从理论上来说，RNN可以通过隐状态得到任意两个词的依赖，无论这两个词距离多远，它们的信息一定会在某一步计算之中汇聚。 我们再给出编码器-解码器的结构图，其中si是解码器在时刻\(i\)的隐状态。编码器读入输入的token xi，并对每个token xi生成一个隐状态hi。从第一个h1到最后一个hm，这些隐状态不断将前面信息积累。最后的携带整个输入序列信息的hm会作为解码器的输入s0。 RNN的缺点同样鲜明。在RNN方案中，在每个时间步，RNN都会将序列中之前的所有信息压缩到一个固定长度的隐向量，最终编码器和解码器之间只通过这个固定长度的隐状态来传递信息。固定长度的隐状态或者说有限的记忆能力在处理长序列时会导致接下来的几个问题，比如信息遗失、信息瓶颈等。 RNN需要对序列内容进行逐步处理， 每一步的输出取决于先前的隐藏状态和当前的输入。RNN这种串行计算在本质上是一种递归，其阻碍了训练时的并行计算，导致训练效率较低，训练时间过长。 RNN的网络结构特点导致难以训练。RNN 用于信息传输通路只有一条，并且该通路上的计算包含多次非线性激活操作，当 RNN 处理长序列时，因为时间步增加带来的多层激活函数的嵌套会导致梯度反传时指数级地衰减（消失）或增长（爆炸），这就是梯度消失问题或者梯度爆炸问题。当梯度消失时，前面的梯度信息无法有效地传递 到后面，也就导致词之间距离越远，前面对后面的影响越弱，所以RNN网络难以学习远距离依赖关系。当梯度爆炸时，网络的权重会变得极大，导致网络不稳定。而且当面对长序列时，RNN需要大量内存来维持长序列的隐状态，比如需要完整理解整个句子乃至整篇文章才能做出判断，这些内存负担对训练也造成了很大挑战。 我们再仔细看看”关系距离问题“。对于CNN方案，序列的第一个词和最后一个词要通过多层卷积后才可以建立联系。下图中的粗线为CNN结构中两个词建立关系所需的最长距离。 而RNN方案需要对序列“从头看到尾”才能确定给这两个词建立关系所需的距离。下图中的粗线是RNN结构中两个词建立关系所需的最长距离。 因此我们要面对的问题是：如何把大量的token压缩到一个隐藏状态中，而且该状态还可以有效的捕捉它们的底层结构和关系。如果想从根本上解决问题，我们有如下几步可以选择： 注意力（Attention）机制由Bengio团队2015年在论文“NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE”中提出，其主要思路为通过模仿人类观察事物的行为来降低算法复杂度、提高性能。人类在感知、认知和行为决策过程中会选择性地关注和处理相关信息，从而提高认知效率和精度，比如人类可以依据兴趣和需求选择关注某些信息而忽略或抑制其它信息，并且可以在多任务上分配注意力从而达到同时处理多个信息的目的。大家最熟悉的例子就是，当一个人看图片时，他会先快速通览，然后对重点区域进行特殊关注。而在文本生成的每个阶段，并非输入上下文的所有片段都同样重要。比如在机器翻译中，句子“A boy is eating the banana”中的“boy”一词并不需要了解整个句子的上下文之后，再来进行准确翻译。 注意力机制的本质可以用一句话来总结：上下文决定一切（context is everything）。一个字 / 词在一篇文本中表达的意思通常与它的上下文有关。比如下面两个句子中都有“transformer”这个单词，但是第一个“transformer”应该翻译成“变压器”，第二个“Transformer”最好不做翻译。 如何才能对“transformer”这个多义词进行语义区分？我们必须考虑单词的上下文才能更好的把单词的语义识别出来，即不仅仅要考虑到词本身，还要考虑其他词对这个词的影响，也就是语境的影响。比如第一个句子的“pole”、”fallen”和“wires”这几个邻近单词暗示了此处的“transformer”和真实物理环境相关。第二个句子的“model”和“natural language processing and image classification”则直接告诉我们此处的“Transformer”是深度学习相关概念。最终我们通过上下文语境可以推断出“Transformer”的准确含义，从而把这两个英文句子翻译成如下： 这就是注意力机制的作用：将每个词与序列中的其他词联系起来，通过句子中的其它词对我们关注词的语义进行推断。 注意力机制也是一种资源分配方式。目前我们知道了上下文的重要性，但是这还不够，因为一个单词的上下文包括很多其它单词，不同单词对目标单词的影响力往往不同。以翻译为例，因为输入语句是一个连贯的整体，所以每个输入词\(X_i\)都会给每个输出词\(Y_i\)造成影响。因此在考虑单词上下文时候，也需要考虑出上下文中每一个元素应该考虑多少。比如第二个英文句子中，“model”对“Transformer”的影响力必然是最大的。所以我们需要一种机制来依据不同的上下文聚焦于不同的信息，这样，一来可以使得序列中的重要元素被投射以较高注意力，重要信息不会被淹没；二来可以将有限的计算资源用来处理更重要的信息，从而做到提纲挈领。 注意力机制就是这种资源分配机制。其在学习的过程中自适应的给予输入的不同词赋予不同的注意力权重，从而区分输入的不同部分对输出的影响，自适应的学习应该关注的重点位置，做出准确判断，即注意力赋予了模型分辨的能力。 其实，论文“Recurrent Models of Visual Attention”中有一段话就深刻的印证了资源分配这个角度。具体如下：人类感知的一个重要特性是，人们不会一次处理整个场景。相反，人类有选择地将注意力集中在视觉空间的某些部分上，以在需要的时间和地点获取信息，并随着时间的推移将不同注视点的信息结合起来，建立场景的内部表示，指导未来的眼球运动和决策。将计算资源集中在场景的各个部分可以节省“带宽”，因为需要处理的“像素”更少。但它也大大降低了任务的复杂性，因为感兴趣的对象可以放置在注视的中心，而注视区域外的视觉环境的无关特征（“杂乱”）自然会被忽略。其英文原文如下： 确定了资源分配的原则之后就可以进行信息交换。注意力机制计算过程就是序列中元素交换信息的过程。注意力机制的输入是一个序列或者集合，注意力机制会从输入中选择性提取信息，据此计算出一组权重。这组权重代表了各个信息的重要程度，使用这种权重与原始信息相乘就得到了注意力处理后的加权信息。 信息交换在一定程度上起到了RNN中记忆力模块的作用，使得注意力编码器也能像RNN一样对一个复杂的语句或者场景进行理解和解析，比如： 将注意力机制应用在一个序列内部（这就是人们经常提到的自注意力机制），可以让序列中每个词都和序列中其他词关联起来，这样序列中每一个元素都有机会根据自身特征，有选择性地依据词之间的相关性来吸取整个序列中每一个其他元素的信息，对本身进行动态调整。这使得模型能够捕捉长距离依赖关系，不受距离的影响。考虑以下两个句子，第一句中的“it”就指代猫，因此“it”更多吸收了“cat”的信息。第二个句子的“it”指代牛奶，因此“it”更多吸收了“milk”的信息。 自注意力机制的目的是为当前单词创建抽象的、丰富的表征。这个表征是本单词被相同序列中的其它单词所影响的结果。经过自注意力机制处理之后，现在每个新单词都融入了其它单词的部分信息，这是一种数据依赖的加权平均，这种表征更加丰富。如果非要将自注意力机制和之前的注意力机制进行对比的话： 通过上面的原理分析，我们可以知道，注意力的核心思想就是帮助模型为输入的不同部分分配不同的权重，这样可以提取关键信息，让模型判断更精准，更加节省算力和存储。但是如何实现注意力机制？这就要解决两个问题： 论文”A General Survey on Attention Mechanisms in Deep Learning“用下图总结了注意力模型的通用结构，论文作者把这种通用架构叫做任务模型。任务模型包含四个部分： 我们用一个句子“北国的特产”为例来解释下图。图上X就是“北国的特产”这个完整的句子，F是从“北国的特产”提取到的特征向量列表，Z是“特”，q是从“特”提取出来的特征向量，我们目的就是要获取特征向量列表中哪个特征向量包含对于”特“最重要的信息。 因为此处提到了Q、K、V这三个术语，我们先用文本翻译为例来做初步介绍，后续章节会对Q、K、V进行深入分析。从上文的分析可知，序列中的每个单词都需要了解序列中其它单词的信息，从而才能确定彼此间的相互关系。因此每个单词都要去询问其它单词：咱俩之间是否密切？其它单词回复关系密切与否。拿到相互关系后，每个单词会据此把其它单词的信息吸收进来做信息融合。上述操作其实是一种搜索+合并的运算，我们需要找到一个适合的机制将这种运算进行落地。而上图注意力模型中，有两个输入：q（正在处理的序列）和F（被关注的序列），F又分别转换为K和V，这三个变量综合起来使用就可以满足我们的需求。 从词典的角度来看也许可以促进理解。query是你要找的内容，key是字典的索引（字典里面有什么样的信息），value是对应的信息。普通的字典查找是精确匹配，即依据匹配的键来返回其对应的值。而注意力机制是向量化+模糊匹配+信息合并。注意力机制不仅查找最佳匹配，还要依据匹配程度做加权求和。源序列每个元素转化为 对，这就构成了源序列的字典。目标序列每个元素提出了query，这就是要查询的内容。在查找中，目标序列中每个元素会用自己的query去和目标序列每个元素的key计算得到对齐系数。这个对齐系数就是元素之间的相似度或者相关性。query和key越相似就代表value对query的影响力越大，query越需要吸收value的信息。随后query会根据两个词之间的亲密关系来决定从V中提取出多少信息出来融入到自身。 通过 query、key、value 这三个向量的相互作用，模型得以衡量每个单词对其他单词的关注度。最终，源序列的每个元素会把自己融合其它单词提供的信息之后得到的真实数据放到一个向量中。 我们假设源序列和目标序列为同一个序列，下图中给出了序列中”一个“和其它单词之间的相似度。虚线代表key与query的相关度，线条的粗细分布就叫做”注意力分布“。或者说，线条的粗细就是权重，线条越粗，表示key与query越相关，对理解query越重要，value的权重就越大。 我们首先看看总体思路。自注意力层是一种和循环层和卷积层等效的计算单元。它们的目的都是把一个向量序列映射成另一个向量序列，比如说编码器把x映射成中间表示z。我们先回想翻译场景。如果是RNN方案，则最终编码器会生成一个隐向量，然后把这个隐向量传递给解码器进行解码。前面已经分析了这种方案的弊端，比如隐向量是固定的。为了克服这种弊端，我们应该在每一个时间步t都会产生一个隐向量\(h_t\)，把这些\(h_t\) 保存起来。当产生新输出时，我们让模型回顾之前保存的所有隐状态，发现隐状态中的关键信息就用上，这样就摆脱了RNN中隐向量是固定长度的弊端。但是如何判断某个隐状态对当前生成词是否重要？这就需要模型采用某种机制进行学习，从而才能了解需要给这个隐状态多少注意力。总而言之，注意力机制的任务应该是找到解码器当前隐向量和编码器所有隐向量之间的相互关系。按照上述思路，注意力机制的计算总体可以分为两步： 得到的注意力权重为 $$w_{3,1},w_{3,2},w_{3,3},w_{3,4}=softmax(g(Y2, C1, C2, C3, C4))$$。 当模型需要决定给予序列中某个单词以多大的“注意力”时，它会计算该单词与其他单词之间的注意力分数。注意力分数是衡量序列中不同单词对当前单词的重要性程度的指标，或者说是目标单词和输入中某单词对齐的可能性大小。可能性越大就应该赋予更大的权重。大权重代表在生成输出时，当下预测单词应该更加关注源文本中其对应单词的信息。 注意力分数通过相似度计算函数得到。该函数一般接受key和value向量作为输入，输出key和value向量之间的相关性，即注意力分数。下图提供了这些函数的概述，其中\(q\)是查询向量，\(k_l\)是矩阵\(K\)的第\(l\)列。如果对应到seq2seq，则q可以认为是解码器输出的隐向量，k可以认为是编码器内部的隐向量。现实中，相似度计算函数是通过矩阵形式来计算，而非单独计算某一列。 得到注意力分数之后，模型会使用softmax操作将注意力分数进行归一化得到注意力权重，这样可以使得所有权重总和为1，确保所有源元素贡献的特征总量保持一定，也可以更加突出重要的权重。 得到注意力权重之后，每个query都能从对应的key上获取相应的信息，此时，就需要由输出函数（Output Function）来将各个部分组合成最终的高级特征向量进行输出。在本阶段，注意力机制会以加权求和的模式对数据进行加工。这意味着每个单词在新的表示中不只是自身的信息，还包含了其他单词的信息，这帮助模型捕捉输入序列中的依赖关系。 回忆下之前提到的RNN方案和CNN方案遇到的问题：对齐问题、信息遗失问题和长依赖问题。以及为了从根本上解决这些问题，我们提出的几种改进思路： 我们接下来就以自注意力机制为例，看看其如何通过自己的优点来解决之前提到的问题，顺便也和这两个方案做下对比分析。 与RNN、CNN方案相比，自注意力机制可以增大信息含量，从而有效的解决RNN中的信息遗失问题。从某种程度上来看，所有的序列建模都在做如下操作： RNN方案中，解码器把过去所有的上下文信息压缩到一个固定大小低维向量（隐状态）中。解码器的不同阶段都使用这个隐状态。该方案的优势是在长上下文中的线性（相对于二次）复杂性。但是在长上下文时，RNN受限于固定大小的隐状态（表达能力有限），很难利用额外的条件信息。 TTT（Test-Time Training）则把上下文压缩到模型的权重中，其优势是既能在时间上保持固定大小，又能大大增强表达能力。 自注意力机制使用一个列表（从后续文章中我们会知道这其实是KV Cache）作为隐状态。所有的上下文都存储在列表中，没有被压缩。列表中所有的上下文共同构成了统一的隐状态（每个阶段的隐状态是列表的一项），这样编码器可以将更多数据传递给解码器。 因此，我们可以看到自注意力机制（Transformer）的优势：不只像RNN那样只传递一个编码器最终的隐状态，而是传入所有的隐藏状态（对应处理过的所有token）到解码器。这样新的token就可以和过去所有上下文进行交互。当然，随着上下文长度的增加，使用列表的成本也会越来越高，其处理时间随上下文长度急剧增长，列表的内存也急剧增加。 自注意力机制中，当某单词获取其它单词信息时，其它单词和它的位置距离都是固定常数。这样，两个词之间的紧密程度仅取决于它们的真实相关性，而非取决于距离。或者说对绝对位置不敏感，任意两个词都可以直接建模。该特点可以解决长距离依赖问题。我们具体分析下。 首先，CNN虽然可以借助增加卷积层数来扩大视野，融合信息，但是信息在过深的网络传播过程之中容易丢失，导致模型性能下降。自注意力机制摒弃了 CNN 的局部假设，把感知域（perceptive field）调整到了整个序列。RNN方案因为无法弥补长距离依赖问题会导致梯度消失和梯度爆炸问题。在处理任何一个单词时，自注意力可以注意到整个句子的所有单词，这样可以将序列中的任意两个位置之间的距离缩小为一个常量，能在常数时间内有效地捕捉到它们之间的依赖关系，直接建立联系，从而消除了距离概念。因为此特点，自注意力机制的信息传播损失小。下图为三个方案构建词语之间关系所需距离的对比。 另外，常数距离特点也让自注意力机制没有“有序假设”。让自注意力机制在处理每个位置时，并非像RNN那样更容易注意到靠后的内容，忽略靠前的输入。而是平等看待序列中每个单词的顺序，这样能够有效得考虑到输入序列中的其他位置，可以更好的将对其他单词的“理解”融入到当前处理的单词中，信息融合效率高。 加权求和可以分为加权和求和两方面来看，前者是对数据进行有差别的对待，而后者是做数据融合，综合在一起就是用提纲挈领（对于不同信息量的元素给予不同的关注度）的方式对数据进行加工。我们接下来从不同角度对加权求和与CNN、全连接层进行比对分析。 首先，注意力机制是动态产生权重。CNN或者全连接层的权重是静态权重，这些权重在训练时候固定下来，在推理时使用相同权重进行推理，并没有建立权重和输入实体本身的关系。注意力机制是动态权重，用输入的query、key通过相似度计算来得到注意力权重。注意力机制根据输入来计算出输出应该更关注哪些输入，权重的产生随着输入变化，这是一种自适应操作。 其次，从另一个角度来看，注意力机制是站在某个输入对象的立场上看待上下文和它之间的关系，并以此关系为权重来确定如何吸取其它对象的信息。因此，注意力机制是一种相对关系建模。而这种操作大多采用全局注意力以建模更广范围的依赖关系，是一种全局操作。 将注意力应用在序列转换的源序列和目标序列之间就是对齐机制。在机器翻译中，源序列中的每个单词都可能会对输出造成影响，不仅仅是待翻译词之前的输入，其后续的输入也可能对输出产生影响，因此需要结合源序列和目标序列进行综合考虑。比如下面中译英的例子，当翻译到”一个“时，我们无法确定英文应该翻译成”a“还是”an“，必须往后看到”苹果“才能确定，因此需要在翻译时把”a or an“的操作和”苹果“进行对齐。 注意力允许依据输入序列和输出序列的不同部分进行相关度计算，这样就可以决定在输出某个单词时，模型将重点关注输入数据的哪些部分。从而在输入序列和输出序列进行对齐，更准确地学习序列之间的关系，提高模型的泛化能力和性能。下图中虚线代表key与query的相关度，线条越粗，表示越相关。我们可以看出来，“一个”和“苹果”等词对“an”的编码更重要（或更相关或更相似），应该承担更多的对“an”的预测，需要赋予更多的权重。因此，在模型生成“an“的时候，不但要从”一个“提取语义，也要从”苹果“提取语义，才能判断出来应该是”an“还是”a“。最终”一个“和”苹果“共同决定了对应的英文单词应该是""an""。 编码器可以同时从左到右和从右到左读取输入序列，并将每个时间步的隐状态拼接起来作为输出。这样做的好处是，可以让编码器同时考虑输入序列中每个单词的前后上下文信息，从而生成更丰富和完整的表示。 综上所述，注意力机制确保每个解码步骤都由最相关的上下文片段提供信息，为长期存在的长距离依赖问题提供了稳健的解决方案，从而重新定义了序列建模的格局。而且，注意力机制的提出不仅是技术上的突破，也为机器学习领域开辟了新的视野，更反映了人类认知的深刻影响。正如 Andrej Karpathy 所评价的：“注意力是一种重大解锁，是神经网络架构设计的一次飞跃。” 注意力机制只是一种思想，可以用到很多任务上。我们接下来看看几个经典的使用注意力机制对Encoder-Decoder进行改进的案例，也梳理一下历史上的重要节点。用范伟的话说：我们要知道Transformer是怎么来的。当然，我们也希望可以看到Transformer是怎么没的，毕竟如果有新方案取代了Transformer，那就说明AI领域将迎来新的历史性突破。 我们首先用一个图来展示下注意力发展历史。大家可以看到， Transformer是建立在很多巨人肩膀之上的集大成者。 论文Recurrent Continuous Translation Models 被认为是神经网络机器翻译(NMT)的开篇之作，其特点如下： 论文""Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation""这篇是Bengio 团队的成果，其特点如下： 没有把编码器-解码器做成一个端到端的模型，只是把解码器的输出概率作为特征喂给了统计机器翻译（SMT）模型。 论文“Sequence to Sequence Learning with Neural Networks”也起到了重要作用，其特点如下： 论文“Neural Machine Translation by Learning to Jointly Align and Translate”提出的Bahdanau Attention是Attention的开创者之一。其特点如下： 作者在论文中使用注意力机制解决了如何建模“与距离无关的依赖”，如何把编码器产生的信息更有效的传递给解码器的问题。这样注意力机制就和RNN一起配合，解决了遗忘和对齐问题。 !Andrej Karpathy分享了Dzmitry Bahdanau 的私人邮件，从中我们可以看到，为了克服编码器和解码器之间的瓶颈，Dzmitry Bahdanau的思路发展历程。 可以看到，突破性的想法往往来自那些在实践中寻求解决问题的创新者，而非空想理论家。Dzmitry Bahdanau在邮件中说得非常好： 我的AI宏愿是启动更多像机器翻译项目那样的惊人应用项目。优秀的研发工作对基础技术的进步贡献远胜过那些我们常认为是“真正”AI研究的复杂理论。 论文“Effective Approaches to Attention-based Neural Machine Translation”基于Bahdanau Attention对注意力机制的计算方式进行多样性探索。 ResNet是何凯明大神的经典之作，其可以有效解决梯度消失/爆炸问题，让神经网络层数的瓶颈得以突破。后面有文章详细讲解，这里不再赘述。 之前的注意力机制都关注不同序列之间的注意力，即交叉注意力。而论文Long Short-Term Memory-Networks for Machine Reading在交叉注意力的基础上提出了self-attenton（自注意力机制），或者叫 intra-attention。作者认为我们在顺序地阅读每个单词的时候，每个单词和前面单词的关联度是不一样的，因此使用了注意力机制来完成这个分析工作。 论文的动机如下。传统的LSTM在处理输入时把句子视为单词序列，并递归的将每个词与之前的记忆结合起来，直至得到整个句子的语义表示。这种方案面临两个问题： 论文QKV-Attention: FRUSTRATINGLY SHORT ATTENTION SPANS IN NEURAL LANGUAGE MODELING这可能是最早提出QKV概念的论文。 作者认为目前注意力机制的隐向量承担了太多功能，这样会分身乏术，影响模型效果。因为同一个词，它在背景向量中的意义和它作为查询词时候代表的意义不同，所以要区分开来。因此该论文对每个时间步产生的输出向量进行分割成三个向量key, value, predict，各自有不同的分工。以下是修改逻辑和历程。 原始模型结构如下图所示，神经网络语言模型只有一个输出向量，同时用来计算注意力向量，编码历史上下文信息和预测下一个词的分布。 论文作者将原始模型的输出划分成两部分：key和value，用key来计算注意力向量，用value来编码下一个词的分布和上下文信息。 但是value仍然同时被用来编码上下文信息和下一个词的分布表示（既要对查询的结果负责，又要对聚合的结果负责），一心二用容易出错。因此作者做了进一步改进。 新模型结构如下图所示，作者对原始模型的输出做进一步划分为三部分：key，value和predict。用key来计算注意力向量，用value来编码上下文信息，用predict来编码下一个词的分布。 作者又继续进行改进，将原始模型的输出划分成N－1部分，用前N－1个时间步的输出向量的一部分来计算下一个词的分布表示。模型结构如下图所示。 论文Self Attention & MultiHead Attention : A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING 把注意力机制用到了文本表示学习中，让注意力机制完成多项工作。论文的注意力计算过程如下： 论文[Convolutional Sequence to Sequence Learning](Convolutional Sequence to Sequence Learning) 使用CNN模型结合注意力机制进行操作，从而解决RNN不能并行的问题。 与RNN相比，CNN存在一定的优势。一来CNN可以并行处理，因此训练速度快，二则RNN并不能很好地处理句子中的结构化信息。因此，作者使用CNN来做编码器和解码器，利用层次结构来捕获词之间远程的依赖关系，也可以更好地捕捉更复杂的关系。在解码过程中，每一个卷积层都会进行注意力操作，称为multi-step attention。 我们可以看到，在上述的演进过程中，Transformer所包括或者说所需要的组件一步步的被实现出来。然而上述注意力方案中依然有RNN、CNN的存在，这拖了后腿。比如RNN无法并行训练，不利于大规模快速训练和部署，也不利于整个算法领域发展。 因此，Transformer作者彻底摒弃了RNN和CNN，构建了一个全新的序列转换架构。Transformer整个网络结构完全是由注意力机制组成，通过把序列元素两两直接比较，Transformer可以学习到输入序列里所有词的相关性，一步到位捕捉到全局的联系。同时，因为Transformer不是按顺序分析，这样它可以并行操作，也使得它比 RNN 具有更高的计算效率和可扩展性。最终，Transformer就是这样一个融汇贯通的集大成者。 当然我们也要看到，RNN和CNN一直没有放弃努力，各自的发展也是有声有色。尤其在强化学习领域，RNN更是在发挥巨大的作用。我们也期望未来会有更多创新的模型和方法来让Transformer在强化学习领域发挥更大的作用。 A General Survey on Attention Mechanisms in Deep Learning Gianni Brauwers and Flavius Frasincar Andrej Karpathy 首次公开私人邮件：揭秘Transformer 注意力机制真相 AI寒武纪 Attention is All your Need Bahdanau 和 Luong Attention 直观介绍 Honoria [Convolutional Sequence to Sequence Learning](Convolutional Sequence to Sequence Learning) Effective Approaches to Attention-based Neural Machine Translation Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation Long Short-Term Memory-Networks for Machine Reading NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE QKV-Attention: FRUSTRATINGLY SHORT ATTENTION SPANS IN NEURAL LANGUAGE MODELING Recurrent Continuous Translation Models Recurrent Continuous Translation Models Self Attention & MultiHead Attention : A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING Self Attention & MultiHead Attention : A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING Self Attention 1: Long Short-Term Memory-Networks for Machine Reading seq2seq中的两种attention机制（图+公式） 胡文星 Sequence to Sequence Learning with Neural Networks Thang Luong's Thesis on Neural Machine Translation Minh-Thang Luong Transformer自下而上理解(4) Attention without RNN marsggbo 《FRUSTRATINGLY SHORT ATTENTION SPANS IN NEURAL LANGUAGE MODELING》阅读笔记 Simple 一文看懂 Bahdanau 和 Luong 两种 Attention 机制的区别 Flitter 注意力机制 OnlyInfo 理解Attention:从起源到MHA,MQA和GQA Linsight 看图学大模型：Transformers 的前生今世(上） 看图学 第四篇：一文搞懂Transformer架构的三种注意力机制 AIwithGary 综述：图像处理中的注意力机制 极市平台 非Transformer架构的进展与猜想 StormBlafe",https://www.cnblogs.com/rossiXYZ/p/18705809,2025-03-13 10:54:43
CodeBlogMan,未知作者,写在前面的是关于自己的一点介绍，21年本科毕业，学的是计算机科学与技术专业。自己本身对编程和开发是有兴趣的，同时也对项目管理、团队建设方面有兴趣。 目前有两段工作经历，都是在一线当 Java 开发大头兵，但职级倒是逐渐有所上升。做过从 0-1 的 SaaS、PaaS 的自研项目，也做过基于多租户的标准化服务项目，还有过半年的 PM 的经历...这些都让我有了一些关于工作上的感悟。现在简单总结一下，拿出来和大家做分享交流。 下面主要从工作的方式方法、构建知识体系、尽早规划职业这三个大的方面展开。文章有点长，但都是自己一步步实践总结得来的，有写得不好的地方，还希望大家多包涵。 拿出积极的态度：首先需要自己努力去解决问题，其次包括调动资源、团队协助，最好要让领导/同事知晓你的情况。 摆烂并不可取，劳动是用来换取报酬的。如果平时的工作表现/绩效过不去，那么将可能会直接影响到我们的收入。 对于工作量的估算要留有余地。学会平衡别人的预期，如果 3 天内能完成，那最好回复一个星期：总有别的事情会打扰你。 遵守基本的职业道德：如果你负责的服务出现故障或发现一个严重的 bug ，作为开发我们应该马上对其进行修复/解决。 工作里对事不对人：我们需要推进的是项目，是一件件具体的事情，聚焦于如何解决问题，而不是和人做无效拉扯。 做好上面这些，假如是对于刚毕业的我来说可能并不容易，但过了两年自己开始负责项目了以后，我就能慢慢理解并适应了。 所以我们可以有意识地提炼日常工作成果，形成自己的个人源码库和工具库、解决某类问题的通用系统体系结构、甚至进化为框架。 众所周知对软件开发人员而言，有、无经验的一个显著区别是：新手无论完成什么开发任务都从头开始，而有经验的开发往往通过重组自己的可复用模块、类库来解决问题。 这并不是说所有可复用的东西都必须自己实现，别人成熟的、通过测试的成果也可以收集、整理到自己的知识库中。 实际程序员们可能很难做到：总有一些会议拉上你去 battle，总有一些线上的 bug 困扰着你，总会有领导安排的其它事情让你分身乏力...还有很多。工作是为了更好的生活这个主旨不会变，所以我也有自己的一些想法： 知识体系是系统化的知识，具有连续性的特点，反映了一个人经过学习后所拥有的知识构成情况与结合方式。如果一个开发人员在1、2年内都没有更新过自己的知识，那么他可能已经不再属于这个行业了。 抽象思维能够超越感官直接感知信息，通过对事物的本质属性和规律的把握，使人们在认识活动中获得更深层次的知识。 多阅读优秀的代码，多思考本质和原因，多通过实践来检验 写出可阅读的、简洁易懂、可维护的代码，应该是程序员在工作中都需要追求的。在实现业务的时候，通过借助模仿优秀的开源实现，学习他人在提高代码效率、逻辑思维、处理高并发等方面是怎么做的。 学会总结复盘，从中抽取有价值的经验，将成长记录下来 复盘的内容，可以是一次线上 bug 的处理过程，可以是一次代码重构优化的过程，也可以是对一个中间件的新认识等等。复盘不是问责也不是甩锅，而是聚焦于好和不好，知其然再知其所以然，同时避免重蹈覆辙。 不是所有人都愿意直面自己的缺点和不足，我自己最开始也不明白复盘有什么用处，觉得对领导和业务能有交代就行。但随着自己做过的项目多了起来，受到身边优秀同事的影响也在日益增多，我才明白复盘是可以让人迅速成长的不二法宝。 有意识地站在更高的角度去看待问题，可能会得到不一样的答案 作为一线开发我们更多关注的，可能是接口的参数和实现逻辑，数据库字段的设计，中间件的使用，异常的处理等实际编码问题； 而你的 TeamLeader 可能关注的是项目的高可用，处理高并发的性能，硬件资源的成本，服务挂了怎么降低对业务的影响等； 部门的总监则可能在考虑这个项目要不要做二期三期？对业务方有什么价值？能带来多少新用户？让谁来负责后续的运营等。 初/中级可以先从广度出发，多见识一些主流技术或者解决方案，这样在面对业务的时候可以更从容地进行设计，也能更清楚自己 CURD 的目的是什么。 高级/资深可能就要精通某一或者某几方面的通用架构/解决方案了，同时对于负责的业务有深刻的认识，可能也要负责团队的建设，从更广阔的视角去思考问题。 我是在工作一年半左右慢慢有了对未来进行职业规划的想法，因为时间会向前走，人也会渐渐地成长。对于未来想要什么，可以慢慢给自己多找找方向去尝试。 2010-2020 这十年，互联网发展如火如荼，各种公司如雨后春笋般冒出来，那时候招聘需求量大，互联网就业市场一片欣欣向荣。 而现在没有了当年的辉煌，很多互联网公司业务发展停滞，同时经过前面多年的发展，产品架构已经成熟稳定，不再需要这么多的程序员，势必要进行结构优化，那优化的对象自然是性价比低的人群。 其次，IT互联网行业高薪的宣传又吸引了一大批年轻人跑步进场，需求变少的情况下人又变多，内卷就这么产生了。高学历的人才多了很多，所谓学历贬值的论调在这几年一直都有出现。 让我们透过现象看本质：IT互联网行业的特殊性在于产出是可迭代的。这个行业程序员的产出就是软件&代码，而这个东西就是可以不断的迭代的。 分析了上面的原因，就有针对性了。软件代码这些东西是可迭代的，但有些东西是没法复制和迭代的：像是沟通协调能力、解决问题能力、规划组织能力、管理能力和人脉资源等等。这些其实就是我们经常说的软实力，这些能力才是越久越吃香。 这些能力不仅可以在日常工作中有意识的积累，也可以通过考取相关的证书来掌握理论知识，比如：软考高项（信息系统项目管理师、系统架构师）、PMP认证、阿里云/华为云高级认证等。 打开认知的局限，不只着眼于手里掌握的技术，拓展自己职业的综合能力，明白不是只有靠技术才能活下去，才能更好地“破局”。 文章最后，我还有一些想说的。的确，技术逐渐会变得越来越“廉价”，不再是必须要工作多年的大佬才能驾驭的高级东西。 这里面有个很重要的原因，就是开源的力量。大家可以想一想自己参与的一些项目，用到了多少开源的东西？以一个典型的后端项目为例，可能涉及到了 MySQL、Redis、ElasticSearch、RocketMQ、Nginx、SprintBoot、Jenkins、CI/CD、Docker...... 程序员要做的就是根据业务需求，把上面那一堆组件按照规则合理地堆在一起。一般就是 Nginx 负载均衡，Spring Boot/Cloud 写后端业务逻辑，Redis 做缓存，MySQL、ES、MongoDB 做数据存储，OSS/COS 做云对象存储，RocketMQ/Kafka 做消息队列，最后进行 CI/CD 操作塞到 Docker 里面部署运行。现在绝大多数的功能，开源组件都能提供，再加上现在是云的时代，各种服务都上云了，那么问题就更简单了。 既然我们身处在这个行业，那么就要做好持续学习的准备，不断打怪升级，锻炼综合能力，做到让经验随着年龄的增长成真正的正比。,https://www.cnblogs.com/CodeBlogMan/p/17983370,2025-03-13 10:54:27
.NET研究社,未知作者,"目前国产化系统浪潮下，适配鸿蒙是中国软件大势所趋，.NET作为最适合开发客户端语言之一，适配鸿蒙系统(HarmonyOS Next)是目前.NET开发者最关心的事情。我目前业余时间正在移植Avalonia到HarmonyOS，去年在.NET Conf CN上分享过，目前又取得一点进展，所以本文把所有问题进行整合与大家进行分享。 自HarmonyOS 5.0.0(12)起，禁止匿名内存申请可执行权限，除系统内置的JavaScript引擎外，其他虚拟机不能使用Jit功能，所以无法将CoreCLR接入到鸿蒙系统中，而最新版的Mono虽然支持解释执行，但是由于性能问题也不会接入Mono到鸿蒙系统，最终只能选择接入NativeAOT运行时。 支撑鸿蒙可以接入NativeAOT的原理是鸿蒙系统兼容libc是musl的Linux系统的动态库(.so)。而.NET的RID支持linux-musl-arm64/linux-musl-x64，所以理论上可以将.NET程序编译为原生的Linux动态库(.so)，然后在鸿蒙的原生项目中，通过dlopen以及dlsym等函数调用C#中的入口函数。 而C#调用鸿蒙api则通过P/Invoke调用鸿蒙的NDK，而ArkUI的TypeScript api则通过NDK中的napi调用。 具体做法可以参考我正在做的Avalonia移植项目: https://github.com/OpenHarmony-NET/OpenHarmony.Avalonia 鸿蒙系统使用了seccomp限制危险的syscall调用。标准posix下，如果系统不支持某个syscall则返回错误码，而seccomp非常激进，如果调用了非法的sycall则直接杀掉进程。.NET的运行时初始化时，会调用__NR_get_mempolicy系统调用对numa支持进行检查，而这个系统调用不在鸿蒙的seccomp白名单中，所以导致直接宕机。 鸿蒙系统中seccomp的系统调用白名单如下：https://gitee.com/openharmony/startup_init/blob/master/services/modules/seccomp/seccomp_policy/app.seccomp.policy 其实安卓中也有类似的限制，.NET的NativeAOT之所以能在安卓平台下运行是因为.NET中对安卓进行了特殊处理，而在鸿蒙平台我们使用的是Linux平台的代码，所以没有对这些系统调用进行处理。 解决上个问题后，.NET运行时初始化依然不能成功，导致程序崩溃，经过排查发现是GC初始化时会申请256G左右的虚拟内存，导致mmap返回Out Of Memory错误。 解决办法1：设置环境变量“DOTNET_GCHeapHardLimit”，将虚拟内存申请控制在约180G以下即可。 解决方案1：从Alpine上偷包 ，因为Alpine的libc是musl，所以理论上Alpine的库在鸿蒙上大部分都能使用。 阿里云Alpine软件包镜像地址: arm64架构：https://mirrors.aliyun.com/alpine/edge/main/aarch64/ amd64架构：https://mirrors.aliyun.com/alpine/edge/main/x86_64/ 鸿蒙的ICU配置文件路径与默认路径不同，需要调用修改环境变量API，将ICU_DATA修改为/system/usr/ohos_icu 且鸿蒙平台上libICU的大版本是72，要使用这个版本的库。 NativeAOT众所周知不支持跨平台编译，而我的方案需要发布到linux-musl平台，所以无法在Windows上发布，影响开发效率。 解决方案：在项目中引入项目https://github.com/OpenHarmony-NET/PublishAotCross Marshal.GetDelegateForFunctionPointer的实现依赖动态生成汇编，而HarmonyOS不支持动态生成汇编代码执行(Jit)，使用该函数会导致崩溃。 前文中提到部分问题的解决方案是修改源码，具体操作步骤如下： 修改完代码，执行以下命令进行编译(linux平台下，需要有编译环境)： 编译成功后，打开目录 运行时/artifacts/bin/coreclr/linux.arm64.Release/aotsdk，将这里所有的替换到自己电脑nuget的缓存目录, 例如C:\Users\用户名\.nuget\packages\runtime.linux-musl-arm64.microsoft.dotnet.ilcompiler\dotnet版本\sdk https://github.com/dotnet/runtime/issues/110074 https://github.com/dotnet/runtime/issues/111649",https://www.cnblogs.com/CeSun/p/18706813,2025-03-13 10:53:56
Chat2DB 3.0 发布：AI 能力脱胎换骨，全方位的 SQL 开发助手,未知作者,AI 助手不仅支持生成 SQL，还能直接执行展示查询到的结果，所见即所得，进一步提升开发与数据分析效率。 这次真不是简单的 SQL 工具了！而是升级为全方位的开发助手。它可以使用代码模型，基于数据库表结构直接生成 Python、Java 等多种语言的代码，还能通过 推理模型 提供各种技术难题的解决思路，助你轻松应对复杂的技术挑战。 ✅ SQL 准确性评估：通过高级算法预测并评估SQL语句的正确性，防止跨库关联或权限问题导致的执行失败 新版本中，Chat2DB 正式接入了 DeepSeek-V3 和 DeepSeek-R1，同时新增了对 CodeGeeX、通义千问 Qwen、GLM-4 等模型的支持，涵盖代码模型、推理模型和混合模型等 10+ 先进模型。 对于数据库管理者来说，清晰的数据结构是高效工作的基础。本次升级中，Chat2DB 新增了 ER 图功能，用户只需一键操作，即可直观查看数据库的结构和关系。 Chat2DB 的 AI 功能已全面覆盖 所有类型的数据库，包括之前未支持的 MongoDB、Redis 等。无论你使用的是关系型数据库还是非关系型数据库，现在都可以通过 AI 快速生成 SQL。 查询结果集优化：支持 Cmd/Ctrl+S 修改数据、Cmd/Ctrl+R 刷新数据，滚动条体验更流畅。 数据库兼容性提升：修复 Oracle、SQL Server、DM 等数据库的语法和展示问题，确保操作精准无误。 修复 SQL Server 的 TIME、DATETIME2、DATETIMEOFFSET 类型在修改表操作时生成的 SQL 错误。 在过去的不到一年里，我们支持了数百个用户需求，平均每周发布一个版本，持续优化迭代，始终将用户体验放在首位。感谢每一位用户的信任与支持！！！ 最后的最后，小编再告诉大家一个好消息：为了满足企业对数据安全和定制化的需求，我们现已正式推出 Chat2DB 企业私有化部署方案，如果有兴趣了解或咨询详情，欢迎扫描下方二维码添加客服微信获取报价！,https://www.cnblogs.com/cmt/p/18765612,2025-03-13 10:53:52
痴者工良,未知作者,"很多人想学习 AI，但是不知道怎么入门。笔者开始也是，先是学习了 Python，然后是 Tensorflow ，还准备看一堆深度学习的书。但是逐渐发现，这些知识太深奥了，无法在短时间内学会。此外还有另一个问题，学这些对自己有什么帮助？虽然学习这些技术是很 NB，但是对自己作用有多大？自己到底需要学什么？ 这这段时间，接触了一些需求，先后搭建了一些聊天工具和 Fastgpt 知识库平台，经过一段时间的使用和研究之后，开始确定了学习目标，是能够做出这些应用。而做出这些应用是不需要深入学习 AI 相关底层知识的。 所以，AI 的知识宇宙非常庞大，那些底层的细节我们可能无法探索，但是并不重要，我们只需要能够做出有用的产品即可。基于此，本文的学习重点在于 Semantic Kernel 和 Kernel Memory 两个框架，我们学会这两个框架之后，可以编写聊天工具、知识库工具。 要学习本文的教程也很简单，只需要有一个 Open AI、Azure Open AI 即可，甚至可以使用国内百度文心。 部署 one-api 不是必须的，如果有 Open AI 或 Azure Open AI 账号，可以直接跳过。如果因为账号或网络原因不能直接使用这些 AI 接口，可以使用国产的 AI 模型，然后使用 one-api 转换成 Open AI 格式接口即可。 one-api 的作用是支持各种大厂的 AI 接口，比如 Open AI、百度文心等，然后在 one-api 上创建一层新的、与 Open AI 一致的。这样一来开发应用时无需关注对接的厂商，不需要逐个对接各种 AI 模型，大大简化了开发流程。 one-api 开源仓库地址：https://github.com/songquanpeng/one-api one-api 需要依赖 redis、mysql ，在 docker-compose.yml 配置文件中有详细的配置，同时 one-api 默认管理员账号密码为 root、123456，也可以在此修改。 执行 docker-compose up -d 开始部署 one-api，然后访问 3000 端口，进入管理系统。 比如，笔者的 Azure Open AI 是不能直接选择使用模型的，而是使用模型创建一个部署，然后通过指定的部署使用模型，因此在 api 中不能直接指定使用 gpt-4-32k 这个模型，而是通过部署名称使用，在模型列表中选择可以使用的模型，而在模型重定向中设置部署的名称。 然后在令牌中，创建一个与 open ai 官方一致的 key 类型，外部可以通过使用这个 key，从 one-api 的 api 接口中，使用相关的 AI 模型。 one-api 的设计，相对于一个代理平台，我们可以通过后台接入自己账号的 AI 模型，然后创建二次代理的 key 给其他人使用，可以在里面配置每个账号、key 的额度。 使用 one-api 接口时，只需要使用 http://192.0.0.1:3000/v1 格式作为访问地址即可，后面需不需要加 /v1 视情况而定，一般需要携带。 创建一个 BaseCore 项目，在这个项目中复用重复的代码，编写各种示例时可以复用相同的代码，引入 Microsoft.SemanticKernel 包。 因为开发时需要使用到密钥等相关信息，因此不太好直接放到代码里面，这时可以使用环境变量或者 json 文件存储相关私密数据。 在学习开发之前，我们需要了解一下基础知识，以便可以理解编码过程中关于模型的一些术语，当然，在后续编码过程中，笔者也会继续介绍相应的知识。 虽然这些接口都是连接到 Azure Open AI 的，但是使用的是不同类型的模型，对应的使用场景也不一样，相关接口的说明如下： 因为 Azure Open AI 的接口名称跟 Open AI 的接口名称只在于差别一个 ”Azure“ ，因此本文读者基本只提 Azure 的接口形式。 这些接口使用的模型类型也不一样，其中 GPT-4 和 GPT3.5 都可以用于文本生成和大模型聊天，其它的模型在功能上有所区别。 目前，文本生成、大语言模型聊天、文本解析为向量是最常用的，为了避免文章篇幅过长以及内容过于复杂导致难以理解，因此本文只讲解这三类模型的使用方法，其它模型的使用读者可以查阅相关资料。 聊天模型主要有 gpt-4 和 gpt-3.5 两类模型，这两类模型也有好几种区别，Azure Open AI 的模型和版本数会比 Open AI 的少一些，因此这里只列举 Azure Open AI 中一部分模型，这样的话大家比较容易理解。 使用 Azure Open AI 官方模型说明地址：https://learn.microsoft.com/zh-cn/azure/ai-services/openai/concepts/models Open AI 官方模型说明地址：https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo 简单来说， gpt-4、gpt-4-32k 区别在于支持 tokens 的最大长度，32k 即 32000 个 tokens，tokens 越大，表示支持的上下文可以越多、支持处理的文本长度越大。 gpt-4 、gpt-4-32k 两个模型都有 0314、0613 两个版本，这个跟模型的更新时间有关，越新版本参数越多，比如 314 版本包含 1750 亿个参数，而 0613 版本包含 5300 亿个参数。 接着是 gpt-4-turbo-preview 和 gpt-4-vision 的区别，gpt-4-version 具有理解图像的能力，而 gpt-4-turbo-preview 则表示为 gpt-4 的增强版。这两个的 tokens 都贵一些。 由于配置模型构建服务的代码很容易重复编写，配置代码比较繁杂，因此在 Env.cs 文件中添加以下内容，用于简化配置和复用代码。 Azure Open AI 比 Open AI 多一个 ChatCompletionDeploymentName ，是指部署名称。 这段代码非常简单，输入问题，然后使用 kernel.InvokePromptAsync(request); 提问，拿到结果后使用 result.GetValue () 提取结果为字符串，然后打印出来。 Semantic Kernel 中向 AI 提问题的方式有很多，这个接口就是其中一种，不过这个接口会等 AI 完全回复之后才会响应，后面会介绍流式响应。另外，在 AI 对话中，用户的提问、上下文对话这些，不严谨的说法来看，都可以叫 prompt，也就是提示。为了优化 AI 对话，有一个专门的技术就叫提示工程。关于这些，这里就不赘述了，后面会有更多说明。 第二个是 result.GetValue ()，返回的 FunctionResult 类型对象中，有很多重要的信息，比如 tokens 数量等，读者可以查看源码了解更多，这里只需要知道使用 result.GetValue () 可以拿到 AI 的回复内容即可。 大家在学习工程中，可以降低日志等级，以便查看详细的日志，有助于深入了解 Semantic Kernel 的工作原理。 修改 .WithAzureOpenAIChat() 或 .WithOpenAIChat() 中的日志配置。 Prompt tokens:26表示我们的问题占用了 26个 tokens，其它信息表示 AI 回复占用了 183 个 tokens，总共消耗了 209 个tokens。 聊天对话上下文中，主要有三种角色：system、assistant、user，其中 assistant 表示机器人角色，system 一般用于设定对话场景等。 我们的问题，都是以 prompt 的形式提交给 AI 的。从日志的 Prompt tokens: 26. Completion tokens: 183 可以看到，prompt 表示提问的问题。 prompt 在大型语言模型(Large Language Models，LLMs) AI 的通信和行为指导中起着至关重要的作用。它们充当输入或查询，用户可以提供这些输入或查询，从而从模型中获得特定的响应。 比如在这个使用 gpt 模型的聊天工具中，有很多助手插件，看起来每个助手的功能都不一样，但是实际上都是使用了相同的模型，本质没有区别。 打开对话，还没有开始用呢，就扣了我 438 个 tokens，这是因为这些背景设定都会出现在提示词里面，占用一部分 tokens。 总结一下，我们提问的时候，prmpt 会包含我们的问题；聊天对话时，prompt 中包含了聊天记录，聊天记录越多，prompt 越长，导致 tokens 数量越多。prompt 除了包含我们的问题、聊天记录，还有可以包含一些相关的背景知识等。 那么是如何实现聊天对话的呢？大家使用 chat 聊天工具时，AI 会根据以前的问题进行下一步补充，我们不需要重复以前的问题。 这在于每次聊天时，需要将历史记录一起带上去！如果聊天记录太多，这就导致后面对话中，携带过多的聊天内容。 从前面使用聊天工具对话时，大家可以看到聊天上下文中有三个角色，其中背景设定是以 system 角色出现的。 这里笔者使用了 xml 格式进行角色提示，这是因为 xml 格式是最正规的提示方法。而使用非 xml 时，角色名称不同的厂商或模型中可能有所差异。 这样使用角色名称做前缀的提示词，也是可以的。为了简单，本文后面的提示词，大多会使用非 xml 的方式。 一般 AI 回复都是以 markdown 语法输出文字，当然，我们通过提示词的方式，可以让 AI 以特定的格式回复内容，代码示例如下： 注意，该示例并非让 AI 直接回复 json，而是以 markdown 代码包裹 json。该示例从 sk 官方示例移植。 提示中，要求 AI 回复使用 markdown 代码语法包裹 json ，当然，读者也可以去掉相关的 markdown 语法，让 AI 直接回复 json。 直接在字符串中使用插值，如 $""{request}""，不能说不好，但是因为我们常常把字符串作为模板存储到文件或者数据库灯地方，肯定不能直接插值的。如果使用 数值表示插值，又会导致难以理解，如： Semantic Kernel 中提供了一种模板字符串插值的的办法，这样会给我们编写提示模板带来便利。 Semantic Kernel 语法规定，使用 {{$system}} 来在提示模板中表示一个名为 system 的变量。后续可以使用 KernelArguments 等类型，替换提示模板中的相关变量标识。示例如下： 在这段代码中，演示了如何在提示模板中使用变量标识，以及再向 AI 提问时传递变量值。此外，为了避免一直等带 AI 回复，我们需要使用流式对话 .InvokeStreamingAsync ()，这样一来就可以呈现逐字回复的效果。 此外，这里不再使用直接使用字符串提问的方法，而是使用 .CreateFunctionFromPrompt() 先从字符串创建提示模板对象。 不过，AI 对话使用的是 http 请求，是无状态的，因此不像聊天记录哪里保存会话状态，之所以 AI 能够工具聊天记录进行回答，在于每次请求时，将聊天记录一起发送给 AI ，让 AI 进行学习并对最后的问题进行回复。 这是因为为了实现聊天的功能，使用了一种很笨的方法。虽然 AI 不会保存聊天记录，但是客户端可以保存，然后下次提问时，将将聊天记录都一起带上去。不过这样会导致 tokens 越来越大！ 下面为了演示对话聊天记录的场景，我们设定 AI 是一个运维专家，我们提问时，选择使用 mysql 相关的问题，除了第一次提问指定是 mysql 数据库，后续都不需要再说明是 mysql。 这段代码有两个地方要说明，第一个是如何存储聊天记录。Semantic Kernel 提供了 ChatHistory 存储聊天记录，当然我们手动存储到字符串、数据库中也是一样的。 但是 ChatHistory 对象不能直接给 AI 使用。所以需要自己从 ChatHistory 中读取聊天记录后，生成字符串，替换提示模板中的 {{$history}}。 历史记录还能通过手动创建 ChatMessageContent 对象的方式添加到 ChatHistory 中： 手动拼接聊天记录太麻烦了，我们可以使用 IChatCompletionService 服务更好的处理聊天对话。 在高层次上，插件是一组可以公开给 AI 应用程序和服务的函数。然后，AI 应用程序可以对插件中的功能进行编排，以完成用户请求。在语义内核中，您可以通过函数调用或规划器手动或自动地调用这些函数。 Semantic Kernel 可以直接加载本地类型中的函数，这一过程不需要经过 AI，完全在本地完成。 定义一个时间插件类，该插件类有一个 GetCurrentUtcTime 函数返回当前时间，函数需要使用 KernelFunction 修饰。 当然，这个示例在实际开发中可能没什么用，不过大家要理解在 Semantic Kernel 是怎样调用一个函数的。 Semantic Kernel 很多地方都跟 Function 相关，你会发现代码中很多代码是以 Function 作为命名的。 然后回到本节的主题，Semantic Kernel 还可以将提示模板存储到文件中，然后以插件的形式加载模板文件。 config.json 文件是固定名称，存储描述信息，比如需要的变量名称、描述等。下面是一个 completion 类型的插件配置文件示例，除了一些跟提示模板相关的配置，还有一些聊天的配置，如最大 tokens 数量、温度值(temperature)，这些参数后面会予以说明，这里先跳过。 插件文件的编写可参考官方文档：https://learn.microsoft.com/en-us/semantic-kernel/prompts/saving-prompts-as-files?tabs=Csharp 使用 Semantic Kernel 加载插件类后，Semantic Kernel 可以自动根据 AI 对话调用这些插件类中的函数。 每个函数都使用了 [Description] 特性设置了注释信息，这些注释信息非常重要，AI 靠这些注释理解函数的功能作用。 读者可以在官方文档了解更多：https://learn.microsoft.com/en-us/semantic-kernel/agents/plugins/using-the-kernelfunction-decorator?tabs=Csharp 定义一个插件类型 ConversationSummaryPlugin，其功能十分简单，将历史记录直接返回，input 参数表示历史记录。 为了在聊天记录中使用该插件函数，我们需要在提示模板中使用 {{ConversationSummaryPlugin.SummarizeConversation $history}}，其中 $history 是自定义的变量名称，名称可以随意，只要是个字符串即可。 由于模板的开头是 {{ConversationSummaryPlugin.SummarizeConversation $history}}，因此，每次聊天之前，都会先调用该函数。 比如输入 吃饭睡觉打豆豆 的时候，首先执行 ConversationSummaryPlugin.SummarizeConversation 函数，然后将返回结果存储到模板中。 Semantic Kernel 中有很多文本处理工具，比如 TextChunker 类型，可以帮助我们提取文本中的行、段。设定场景如下，用户提问一大段文本，然后我们使用 AI 总结这段文本。 设定一个场景，用户可以每行输入一句话，当用户使用 000 结束输入后，每句话都推送给 AI 总结（不是全部放在一起总结）。 不过经过调试发现，TextChunker 对这段文本的处理似乎不佳，因为文本这么多行只识别为一行、一段。 本小节的演示效果不佳，不过主要目的是，让用户了解 KernelFunctionFactory.CreateFromPrompt 可以更加方便创建提示模板、使用 PromptExecutionSettings 配置温度、使用 TextChunker 切割文本。 配置 PromptExecutionSettings 时，出现了三个参数，其中 MaxTokens 表示机器人回复最大的 tokens 数量，这样可以避免机器人废话太多。 Temperature：值范围在 0-2 之间，简单来说，temperature 的参数值越小，模型就会返回越确定的一个结果。值越大，AI 的想象力越强，越可能偏离现实。一般诗歌、科幻这些可以设置大一些，让 AI 实现天马行空的回复。 TopP：与 Temperature 不同的另一种方法，称为核抽样，其中模型考虑了具有 TopP 概率质量的令牌的结果。因此，0.1 意味着只考虑构成前10% 概率质量的令牌的结果。 更多相关的参数配置，请查看 https://learn.microsoft.com/en-us/azure/ai-services/openai/reference ExecutionSettings 部分的配置，可以针对使用的模型起效，这里的配置不会全部同时起效，会根据实际使用的模型起效。 模板文件由 config.json 和 skprompt.txt 组成，skprompt.txt 中配置提示词，跟 PromptTemplateConfig 的 Template 字段配置一致。 config.json 中涉及的内容比较多，你可以对照下面的 json 跟 实现总结 一节的代码，两者几乎是一模一样的。 目前，我们已经有两个地方使用到提示模板的语法，即变量和函数调用，因为前面已经介绍过相关的用法，因此这里再简单提及一下。 函数一定需要使用 [KernelFunction] 标识，[Description] 描述函数的作用。函数可以一个或多个参数，每个参数最好都使用 [Description] 描述作用。 函数参数中，可以带一个 Kernel kernel，可以放到开头或末尾 ，也可以不带，主要作用是注入 Kernel 对象。 其它一些特殊字符的转义方法等，详见官方文档：https://learn.microsoft.com/en-us/semantic-kernel/prompts/prompt-template-syntax 按照这个示例，我们先在 Env.cs 中编写扩展函数，配置使用 .AddAzureOpenAITextGeneration() 文本生成，而不是聊天对话。 然后编写提问代码，用户可以多行输入文本，最后使用 000 结束输入，将文本提交给 AI 进行总结。进行总结时，为了避免 AI 废话太多，因此这里使用 ExecutionSettings 配置相关参数。 Semantic Kernel 在 Microsoft.SemanticKernel.Plugins 开头的包中提供了一些插件，不同的包有不同功能的插件。大部分目前还是属于半成品，因此这部分不详细讲解，本节只做简单说明。 Semantic Kernel 还有通过远程 swagger.json 使用插件的做法，详细请参考文档：https://learn.microsoft.com/en-us/semantic-kernel/agents/plugins/openai-plugins 安装 Microsoft.SemanticKernel.Plugins.Document（需要勾选预览版），里面包含了文档插件，该文档插件使用了 DocumentFormat.OpenXml 项目，DocumentFormat.OpenXml 支持以下文档格式： DocumentFormat.OpenXml 项目地址 https://github.com/dotnet/Open-XML-SDK Kernel Memory 是一个歪果仁的个人项目，支持 PDF 和 Word 文档、 PowerPoint 演示文稿、图像、电子表格等，通过利用大型语言模型(llm)、嵌入和矢量存储来提取信息和生成记录，主要目的是提供文档处理相关的接口，最常使用的场景是知识库系统。读者可能对知识库系统不了解，如果有条件，建议部署一个 Fastgpt 系统研究一下。 但是目前 Kernel Memory 依然是半产品，文档也不完善，所以接下来笔者也只讲解最核心的部分，感兴趣的读者建议直接看源码。 Kernel Memory 项目文档：https://microsoft.github.io/kernel-memory/ Kernel Memory 项目仓库：https://github.com/microsoft/kernel-memory 要讲解知识库系统，可以这样理解。大家都知道，训练一个医学模型是十分麻烦的，别说机器的 GPU 够不够猛，光是训练 AI ，就需要掌握各种专业的知识。如果出现一个新的需求，可能又要重新训练一个模型，这样太麻烦了。 首先，将 docx、pdf 等问题提取出文本，然后切割成多个段落，每一段都使用 AI 模型生成相关向量，这个向量的原理笔者也不懂，大家可以简单理解为分词，生成向量后，将段落文本和向量都存储到数据库中（数据库需要支持向量）。 然后在用户提问 “什么是报表” 时，首先在数据库中搜索，根据向量来确定相似程度，把几个跟问题相关的段落拿出来，然后把这几段文本和用户的问题一起发给 AI。相对于在提示模板中，塞进一部分背景知识，然后加上用户的问题，再由 AI 进行总结回答。 笔者建议大家有条件的话，部署一个开源版本的 Fastgpt 系统，把这个系统研究一下，学会这个系统后，再去研究 Kernel Memory ，你就会觉得非常简单了。同理，如果有条件，可以先部署一个 LobeHub ，开源的 AI 对话系统，研究怎么用，再去研究 Semantic Kernel 文档，接着再深入源码。 Kernel Memory 支持从网页爬取、导入文档、直接给定字符串三种方式导入信息，由于 Kernel Memory 提供了一个 Service 示例，里面有一些值得研究的代码写法，因此下面的示例是启动 Service 这个 Web 服务，然后在客户端将文档推送该 Service 处理，客户端本身不对接 AI。 这个控制台的作用是帮助我们生成相关配置的。启动这个控制台之后，根据提示选择对应的选项（按上下键选择选项，按下回车键确认），以及填写配置内容，配置会被存储到 appsettings.Development.json 中。 如果读者搞不懂这个控制台怎么使用，那么可以直接将替换下面的 json 到 appsettings.Development.json 。 详细可参考文档： https://microsoft.github.io/kernel-memory/quickstart/configuration 然后编写代码连接到知识库系统，推送要处理的网页地址给 Service。创建一个项目，引入 Microsoft.KernelMemory.WebClient 包。 此外还有 ImportTextAsync、ImportDocumentAsync 来个导入知识的方法。 本节内容稍多，主要讲解如何使用 Kernel Memory 从将文档导入、生成向量、存储向量、搜索问题等。 首先使用 KernelMemoryBuilder 构建配置，配置的内容比较多，这里会使用到两个模型，一个是向量模型，一个是文本生成模型（可以使用对话模型，如 gpt-4-32k）。 首先是讲解将文件存储到哪里，也就是导入文件之后，将文件存储到哪里，存储文件的接口是 IContentStorage，目前有两个实现： Kernel Memory 还不支持 Mongodb，不过可以自己使用 IContentStorage 接口写一个。 接着是，配置文档生成向量模型，导入文件文档后，在本地提取出文本，需要使用 AI 模型从文本中生成向量。 当用户提问时，首先会在这里的 IMemoryDb 调用相关方法查询文档中的向量、索引等，查找出相关的文本。 查出相关的文本后，需要发送给 AI 处理，需要使用 ITextGenerator 接口，目前有两个实现： 将每一段文本使用向量模型解析出向量，存储到 IMemoryDb 接口提供的服务中，如 Postgres数据库。 提问问题或搜索内容时，从 IMemoryDb 所在的位置搜索向量，查询到相关的文本，然后将文本收集起来，发送给 AI（使用文本生成模型），这些文本相对于提示词，然后 AI 从这些提示词中学习并回答用户的问题。 详细源码可以参考 Microsoft.KernelMemory.Search.SearchClient ，由于源码比较多，这里就不赘述了。",https://www.cnblogs.com/whuanle/p/18045341,2025-03-13 10:54:23
软件开发众包平台，园子商业化的未来,未知作者,众包平台是园子正在探索的与开发者共赢的商业模式，目前一边在召集早期合作开发者，一边在基于企业微信简单运营。 如果您有软件开发外包需求，如果您想找高手帮您解决棘手的软件开发问题，欢迎加企业微信联系我们发单，加好友时请备注【众包需求方】。 如果您想成为众包平台的合作开发者，欢迎加企业微信申请，加好友时请备注【擅长的技术栈与业务方向-工作年限-所在城市】，至少需要3年工作经验。,https://www.cnblogs.com/cmt/p/18500368,2025-03-13 10:54:30
开发中对象命名的一点思考,未知作者,在我所见过的项目中，大多数团队都倾向于“功能堆砌式”开发：需求来了就加逻辑或函数，却很少有人愿意花时间在设计上，尤其是在对象命名花费时间。这看似“快速实现需求”的方式，通常会对代码的可读性产生坏的影响，进而影响可维护性。 一个好的对象命名并非只是让代码表面看起来整洁；它背后关系到人类和 AI 对系统的认知方式，也会在后续维护和迭代中形塑程序员以及 AI 工具的行为。换句话说，合适的命名不但决定了当前代码的优雅程度，也会潜移默化地影响未来对代码进行修改、扩展和重构时的思维路径与决策过程。 下面，我们将通过几个示例，探讨如何通过合理的命名让对象真正体现业务含义与自主决策能力，而不是简单地扮演一个“被动执行者”。 想象你走进一家餐厅。你会如何点餐？是要一份宫保鸡丁，还是告诉厨师：“你先放油，然后大火爆炒，然后再加调料”？如果你选择后者，那么对应到程序设计中，大概就是下面的写法： FoodMaker ，它只是一个“做饭的人”或“执行器”，缺乏更多的“主观能动性”。这不仅增加了使用者在思维层面的负担，也显得对专业厨师的经验和技能不够尊重。 相比之下，更贴近现实的方式是直接告诉对方“我需要一份宫保鸡丁”，厨师会根据自身经验和对食材的理解来完成这道菜。例如： 这样，Chef 完整体现了专业性与自主性： 值得注意的是，很多以 “-er” 或 “-or” 结尾的对象（如 Manager、Processor、Controller、Validator 等）常常会呈现出类似的“过程化”倾向：它们更多体现的是过程集合而非业务主体。当我们把命名改为更能传达专业身份或业务角色的名词时，往往会看到对象的“自我意识”和“主观能动性”随之提升，从而让整个系统的抽象层次更高、可维护性更好。 在 AI 辅助编程工具的普及下，命名的清晰度在大型项目中显得尤为重要。模糊或过于抽象的名字不仅会增加团队成员、甚至让3个月后的你自己的认知负担，也会让 AI 在大量上下文中难以理解该对象的真实意图，甚至产生误导性补全。我们先来看一个不恰当的示例： 要让代码更易读、更具可维护性，也让 AI 分析更准确，我们可以将“餐厅运营”按照现实场景拆分成多个专门对象，让每个类名更能“自证其职”： 现实生活中，若宫保鸡丁的必需食材（例如花生）突然缺货，真正的专业厨师会主动寻找替代食材，而不会要求顾客重新“下指令”或“换个点餐方式”。同理，在软件中，一个设计得足够“智能”的对象，也应该能在外部条件或业务需求变化时，自行调整内部逻辑，而不影响调用者的使用方式。 局限： 下面的 Chef 内部自行决定花生是否可用，如果缺货就用其他食材替代。这样，即使后续有更多类似变化（换新调料、临时供应商等），也能集中在 Chef 内部调整，无需改动客户端调用代码。 好处： 无论花生库存如何，客户端调用方式始终一致：一个智能的 Chef 能在内部完成相应的“自适应”处理。 对象的命名，许多人往往只把它当作“好看”或“顺口”的问题，却忽视了它所暗含的业务理解深度与系统思维。当我们刻意避开 “-er” 结尾或“Service”、“utility”后缀等含糊标签，并让对象名真实反映其专业角色与业务职责时，能够有下述好处： 在软件工程中，命名的重要性通常被忽视，但命名本身潜移默化影响我们的编码时的思维。只有当我们真正将对象视为拥有“尊严”与“自主决策能力”的实体时，才能更容易构建出一个高内聚、易扩展、符合业务本质的系统。,https://www.cnblogs.com/CareySon/p/18711135,2025-03-13 10:53:55
痴者工良,未知作者,"Maomi.Torch 项目仓库：https://github.com/whuanle/Maomi.Torch 本章内容主要基于 Pytorch 官方入门教程编写，使用 C# 代码代替 Python，主要内容包括处理数据、创建模型、优化模型参数、保存模型、加载模型，读者通过本章内容开始了解 TorchSharp 框架的使用方法。 https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html 首先添加以下代码，查找最适合当前设备的工作方式，主要是选择 GPU 开发框架，例如 CUDA、MPS，CPU，有 GPU 就用 GPU，没有 GPU 降级为 CPU。 训练模型最重要的一步是准备数据，但是准备数据集是一个非常繁杂和耗时间的事情，对于初学者来说也不现实，所以 Pytorch 官方在框架集成了一些常见的数据集，开发者可以直接通过 API 使用这些提前处理好的数据集和标签。 Pytorch 使用 torch.utils.data.Dataset 表示数据集抽象接口，存储了数据集的样本和对应标签；torch.utils.data.DataLoader 表示加载数据集的抽象接口，主要是提供了迭代器。这两套接口是非常重要的，对于开发者自定义的数据集，需要实现这两套接口，自定义加载数据集方式。 Pytorch 有三大领域的类库，分别是 TorchText、TorchVision、TorchAudio，这三个库都自带了一些常用开源数据集，但是 .NET 里社区仓库只提供了 TorchVision，生态严重落后于 Pytorch。TorchVision 是一个工具集，可以从 Fashion-MNIST 等下载数据集以及进行一些数据类型转换等功能。 在本章中，使用的数据集叫 FashionMNIST，Pytorch 还提供了很多数据集，感兴趣的读者参考：https://pytorch.org/vision/stable/datasets.html 现在开始讲解如何通过 TorchSharp 框架加载 FashionMNIST 数据集，首先添加引用： 注意，与 Python 版本有所差异， Pytorch 官方给出了 ToTensor() 函数用于将图像转换为 torch.Tensor 张量类型，但是由于 C# 版本并没有这个函数，因此只能手动指定一个转换器。 启动项目，会自动下载数据集，接着在程序运行目录下会自动创建一个 data 目录，里面是数据集文件，包括用于训练的数据和测试的数据集。 数据集是 Dataset 类型，继承了 Dataset > 类型，Dataset 本质是列表，我们把 Dataset 列表的 item 称为数据，每个 item 都是一个字典类型，每个字典由 data、label 两个 key 组成。 在上一节，已经编写好如何加载数据集，将训练数据和测试数据分开加载，为了了解 Dataset ，读者可以通过以下代码将数据集的结构打印到控制台。 通过观察控制台，可以知道，每个数据元素都是一个字典，每个字典由 data、label 两个 key 组成,dic[""data""] 是一个图片，而 label 就是表示该图片的文本值是什么。 Maomi.Torch 框架提供了将张量转换为图片并显示的方法，例如下面在窗口显示数据集前面的三张图片： 使用 Maomi.ScottPlot.Winforms 库，还可以通过 img.ShowImageToForm() 接口通过窗口的形式显示图片。 由于 FashionMNIST 数据集有 6 万张图片，一次性加载所有图片比较消耗内存，并且一次性训练对 GPU 的要求也很高，因此我们需要分批处理数据集。 torch.utils.data 中有数据加载器，可以帮助我们分批加载图片集到内存中，开发时使用迭代器直接读取，不需要关注分批情况。 注意，分批是在 DataLoader 内部发生的，我们可以理解为缓冲区大小，对于开发者来说，并不需要关注分批情况。 接下来定义一个神经网络，神经网络有多个层，通过神经网络来训练数据，通过数据的训练可以的出参数、权重等信息，这些信息会被保存到模型中，加载模型时，必须要有对应的网络结构，比如神经网络的层数要相同、每层的结构一致。 该网络通过接受 28*28 大小的图片，经过处理后输出 10 个分类值，每个分类结果都带有其可能的概率，概率最高的就是识别结果。 为了训练模型，需要定义一个损失函数和一个优化器，损失函数的主要作用是衡量模型的预测结果与真实标签之间的差异，即误差或损失，有了损失函数后，通过优化器可以指导模型参数的调整，使预测结果能够逐步靠近真实值，从而提高模型的性能。Pytorch 自带很多损失函数，这里使用计算交叉熵损失的损失函数。 torch.Tensor 类型的 .shape 属性比较特殊，是一个数组类型，主要用于存储当前类型的结构，要结合上下文才能判断，例如在当前训练中，x.shape 值是 [64,1,28,28]，shape[1] 是图像的通道，1 是灰色，3 是彩色(RGB三通道)；shape[2]、shape[3] 分别是图像的长度和高度。 通过上面步骤可以看出，“训练” 是一个字面意思，跟人类的学习不一样，这里是先使用模型识别一个图片，然后计算误差，更新模型参数和权重，然后进入下一次调整。 要使用模型识别图片，只需要使用 var pred = model.call(x); 即可，但是因为模型并不能直接输出识别结果，而是根据网络结构输出到每个神经元中，每个神经元都表示当前概率。在前面定义的网络中，nn.Linear(512, 10)) 会输出 10 个分类结果，每个分类结果都带有概率，那么我们将概率最高的一个结果拿出来，就相当于图片的识别结果了。",https://www.cnblogs.com/whuanle/p/18700127,2025-03-13 10:54:02
痴者工良,未知作者,"很多人想学习 AI，但是不知道怎么入门。笔者开始也是，先是学习了 Python，然后是 Tensorflow ，还准备看一堆深度学习的书。但是逐渐发现，这些知识太深奥了，无法在短时间内学会。此外还有另一个问题，学这些对自己有什么帮助？虽然学习这些技术是很 NB，但是对自己作用有多大？自己到底需要学什么？ 这这段时间，接触了一些需求，先后搭建了一些聊天工具和 Fastgpt 知识库平台，经过一段时间的使用和研究之后，开始确定了学习目标，是能够做出这些应用。而做出这些应用是不需要深入学习 AI 相关底层知识的。 所以，AI 的知识宇宙非常庞大，那些底层的细节我们可能无法探索，但是并不重要，我们只需要能够做出有用的产品即可。基于此，本文的学习重点在于 Semantic Kernel 和 Kernel Memory 两个框架，我们学会这两个框架之后，可以编写聊天工具、知识库工具。 要学习本文的教程也很简单，只需要有一个 Open AI、Azure Open AI 即可，甚至可以使用国内百度文心。 部署 one-api 不是必须的，如果有 Open AI 或 Azure Open AI 账号，可以直接跳过。如果因为账号或网络原因不能直接使用这些 AI 接口，可以使用国产的 AI 模型，然后使用 one-api 转换成 Open AI 格式接口即可。 one-api 的作用是支持各种大厂的 AI 接口，比如 Open AI、百度文心等，然后在 one-api 上创建一层新的、与 Open AI 一致的。这样一来开发应用时无需关注对接的厂商，不需要逐个对接各种 AI 模型，大大简化了开发流程。 one-api 开源仓库地址：https://github.com/songquanpeng/one-api one-api 需要依赖 redis、mysql ，在 docker-compose.yml 配置文件中有详细的配置，同时 one-api 默认管理员账号密码为 root、123456，也可以在此修改。 执行 docker-compose up -d 开始部署 one-api，然后访问 3000 端口，进入管理系统。 比如，笔者的 Azure Open AI 是不能直接选择使用模型的，而是使用模型创建一个部署，然后通过指定的部署使用模型，因此在 api 中不能直接指定使用 gpt-4-32k 这个模型，而是通过部署名称使用，在模型列表中选择可以使用的模型，而在模型重定向中设置部署的名称。 然后在令牌中，创建一个与 open ai 官方一致的 key 类型，外部可以通过使用这个 key，从 one-api 的 api 接口中，使用相关的 AI 模型。 one-api 的设计，相对于一个代理平台，我们可以通过后台接入自己账号的 AI 模型，然后创建二次代理的 key 给其他人使用，可以在里面配置每个账号、key 的额度。 使用 one-api 接口时，只需要使用 http://192.0.0.1:3000/v1 格式作为访问地址即可，后面需不需要加 /v1 视情况而定，一般需要携带。 创建一个 BaseCore 项目，在这个项目中复用重复的代码，编写各种示例时可以复用相同的代码，引入 Microsoft.SemanticKernel 包。 因为开发时需要使用到密钥等相关信息，因此不太好直接放到代码里面，这时可以使用环境变量或者 json 文件存储相关私密数据。 在学习开发之前，我们需要了解一下基础知识，以便可以理解编码过程中关于模型的一些术语，当然，在后续编码过程中，笔者也会继续介绍相应的知识。 虽然这些接口都是连接到 Azure Open AI 的，但是使用的是不同类型的模型，对应的使用场景也不一样，相关接口的说明如下： 因为 Azure Open AI 的接口名称跟 Open AI 的接口名称只在于差别一个 ”Azure“ ，因此本文读者基本只提 Azure 的接口形式。 这些接口使用的模型类型也不一样，其中 GPT-4 和 GPT3.5 都可以用于文本生成和大模型聊天，其它的模型在功能上有所区别。 目前，文本生成、大语言模型聊天、文本解析为向量是最常用的，为了避免文章篇幅过长以及内容过于复杂导致难以理解，因此本文只讲解这三类模型的使用方法，其它模型的使用读者可以查阅相关资料。 聊天模型主要有 gpt-4 和 gpt-3.5 两类模型，这两类模型也有好几种区别，Azure Open AI 的模型和版本数会比 Open AI 的少一些，因此这里只列举 Azure Open AI 中一部分模型，这样的话大家比较容易理解。 使用 Azure Open AI 官方模型说明地址：https://learn.microsoft.com/zh-cn/azure/ai-services/openai/concepts/models Open AI 官方模型说明地址：https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo 简单来说， gpt-4、gpt-4-32k 区别在于支持 tokens 的最大长度，32k 即 32000 个 tokens，tokens 越大，表示支持的上下文可以越多、支持处理的文本长度越大。 gpt-4 、gpt-4-32k 两个模型都有 0314、0613 两个版本，这个跟模型的更新时间有关，越新版本参数越多，比如 314 版本包含 1750 亿个参数，而 0613 版本包含 5300 亿个参数。 接着是 gpt-4-turbo-preview 和 gpt-4-vision 的区别，gpt-4-version 具有理解图像的能力，而 gpt-4-turbo-preview 则表示为 gpt-4 的增强版。这两个的 tokens 都贵一些。 由于配置模型构建服务的代码很容易重复编写，配置代码比较繁杂，因此在 Env.cs 文件中添加以下内容，用于简化配置和复用代码。 Azure Open AI 比 Open AI 多一个 ChatCompletionDeploymentName ，是指部署名称。 这段代码非常简单，输入问题，然后使用 kernel.InvokePromptAsync(request); 提问，拿到结果后使用 result.GetValue () 提取结果为字符串，然后打印出来。 Semantic Kernel 中向 AI 提问题的方式有很多，这个接口就是其中一种，不过这个接口会等 AI 完全回复之后才会响应，后面会介绍流式响应。另外，在 AI 对话中，用户的提问、上下文对话这些，不严谨的说法来看，都可以叫 prompt，也就是提示。为了优化 AI 对话，有一个专门的技术就叫提示工程。关于这些，这里就不赘述了，后面会有更多说明。 第二个是 result.GetValue ()，返回的 FunctionResult 类型对象中，有很多重要的信息，比如 tokens 数量等，读者可以查看源码了解更多，这里只需要知道使用 result.GetValue () 可以拿到 AI 的回复内容即可。 大家在学习工程中，可以降低日志等级，以便查看详细的日志，有助于深入了解 Semantic Kernel 的工作原理。 修改 .WithAzureOpenAIChat() 或 .WithOpenAIChat() 中的日志配置。 Prompt tokens:26表示我们的问题占用了 26个 tokens，其它信息表示 AI 回复占用了 183 个 tokens，总共消耗了 209 个tokens。 聊天对话上下文中，主要有三种角色：system、assistant、user，其中 assistant 表示机器人角色，system 一般用于设定对话场景等。 我们的问题，都是以 prompt 的形式提交给 AI 的。从日志的 Prompt tokens: 26. Completion tokens: 183 可以看到，prompt 表示提问的问题。 prompt 在大型语言模型(Large Language Models，LLMs) AI 的通信和行为指导中起着至关重要的作用。它们充当输入或查询，用户可以提供这些输入或查询，从而从模型中获得特定的响应。 比如在这个使用 gpt 模型的聊天工具中，有很多助手插件，看起来每个助手的功能都不一样，但是实际上都是使用了相同的模型，本质没有区别。 打开对话，还没有开始用呢，就扣了我 438 个 tokens，这是因为这些背景设定都会出现在提示词里面，占用一部分 tokens。 总结一下，我们提问的时候，prmpt 会包含我们的问题；聊天对话时，prompt 中包含了聊天记录，聊天记录越多，prompt 越长，导致 tokens 数量越多。prompt 除了包含我们的问题、聊天记录，还有可以包含一些相关的背景知识等。 那么是如何实现聊天对话的呢？大家使用 chat 聊天工具时，AI 会根据以前的问题进行下一步补充，我们不需要重复以前的问题。 这在于每次聊天时，需要将历史记录一起带上去！如果聊天记录太多，这就导致后面对话中，携带过多的聊天内容。 从前面使用聊天工具对话时，大家可以看到聊天上下文中有三个角色，其中背景设定是以 system 角色出现的。 这里笔者使用了 xml 格式进行角色提示，这是因为 xml 格式是最正规的提示方法。而使用非 xml 时，角色名称不同的厂商或模型中可能有所差异。 这样使用角色名称做前缀的提示词，也是可以的。为了简单，本文后面的提示词，大多会使用非 xml 的方式。 一般 AI 回复都是以 markdown 语法输出文字，当然，我们通过提示词的方式，可以让 AI 以特定的格式回复内容，代码示例如下： 注意，该示例并非让 AI 直接回复 json，而是以 markdown 代码包裹 json。该示例从 sk 官方示例移植。 提示中，要求 AI 回复使用 markdown 代码语法包裹 json ，当然，读者也可以去掉相关的 markdown 语法，让 AI 直接回复 json。 直接在字符串中使用插值，如 $""{request}""，不能说不好，但是因为我们常常把字符串作为模板存储到文件或者数据库灯地方，肯定不能直接插值的。如果使用 数值表示插值，又会导致难以理解，如： Semantic Kernel 中提供了一种模板字符串插值的的办法，这样会给我们编写提示模板带来便利。 Semantic Kernel 语法规定，使用 {{$system}} 来在提示模板中表示一个名为 system 的变量。后续可以使用 KernelArguments 等类型，替换提示模板中的相关变量标识。示例如下： 在这段代码中，演示了如何在提示模板中使用变量标识，以及再向 AI 提问时传递变量值。此外，为了避免一直等带 AI 回复，我们需要使用流式对话 .InvokeStreamingAsync ()，这样一来就可以呈现逐字回复的效果。 此外，这里不再使用直接使用字符串提问的方法，而是使用 .CreateFunctionFromPrompt() 先从字符串创建提示模板对象。 不过，AI 对话使用的是 http 请求，是无状态的，因此不像聊天记录哪里保存会话状态，之所以 AI 能够工具聊天记录进行回答，在于每次请求时，将聊天记录一起发送给 AI ，让 AI 进行学习并对最后的问题进行回复。 这是因为为了实现聊天的功能，使用了一种很笨的方法。虽然 AI 不会保存聊天记录，但是客户端可以保存，然后下次提问时，将将聊天记录都一起带上去。不过这样会导致 tokens 越来越大！ 下面为了演示对话聊天记录的场景，我们设定 AI 是一个运维专家，我们提问时，选择使用 mysql 相关的问题，除了第一次提问指定是 mysql 数据库，后续都不需要再说明是 mysql。 这段代码有两个地方要说明，第一个是如何存储聊天记录。Semantic Kernel 提供了 ChatHistory 存储聊天记录，当然我们手动存储到字符串、数据库中也是一样的。 但是 ChatHistory 对象不能直接给 AI 使用。所以需要自己从 ChatHistory 中读取聊天记录后，生成字符串，替换提示模板中的 {{$history}}。 历史记录还能通过手动创建 ChatMessageContent 对象的方式添加到 ChatHistory 中： 手动拼接聊天记录太麻烦了，我们可以使用 IChatCompletionService 服务更好的处理聊天对话。 在高层次上，插件是一组可以公开给 AI 应用程序和服务的函数。然后，AI 应用程序可以对插件中的功能进行编排，以完成用户请求。在语义内核中，您可以通过函数调用或规划器手动或自动地调用这些函数。 Semantic Kernel 可以直接加载本地类型中的函数，这一过程不需要经过 AI，完全在本地完成。 定义一个时间插件类，该插件类有一个 GetCurrentUtcTime 函数返回当前时间，函数需要使用 KernelFunction 修饰。 当然，这个示例在实际开发中可能没什么用，不过大家要理解在 Semantic Kernel 是怎样调用一个函数的。 Semantic Kernel 很多地方都跟 Function 相关，你会发现代码中很多代码是以 Function 作为命名的。 然后回到本节的主题，Semantic Kernel 还可以将提示模板存储到文件中，然后以插件的形式加载模板文件。 config.json 文件是固定名称，存储描述信息，比如需要的变量名称、描述等。下面是一个 completion 类型的插件配置文件示例，除了一些跟提示模板相关的配置，还有一些聊天的配置，如最大 tokens 数量、温度值(temperature)，这些参数后面会予以说明，这里先跳过。 插件文件的编写可参考官方文档：https://learn.microsoft.com/en-us/semantic-kernel/prompts/saving-prompts-as-files?tabs=Csharp 使用 Semantic Kernel 加载插件类后，Semantic Kernel 可以自动根据 AI 对话调用这些插件类中的函数。 每个函数都使用了 [Description] 特性设置了注释信息，这些注释信息非常重要，AI 靠这些注释理解函数的功能作用。 读者可以在官方文档了解更多：https://learn.microsoft.com/en-us/semantic-kernel/agents/plugins/using-the-kernelfunction-decorator?tabs=Csharp 定义一个插件类型 ConversationSummaryPlugin，其功能十分简单，将历史记录直接返回，input 参数表示历史记录。 为了在聊天记录中使用该插件函数，我们需要在提示模板中使用 {{ConversationSummaryPlugin.SummarizeConversation $history}}，其中 $history 是自定义的变量名称，名称可以随意，只要是个字符串即可。 由于模板的开头是 {{ConversationSummaryPlugin.SummarizeConversation $history}}，因此，每次聊天之前，都会先调用该函数。 比如输入 吃饭睡觉打豆豆 的时候，首先执行 ConversationSummaryPlugin.SummarizeConversation 函数，然后将返回结果存储到模板中。 Semantic Kernel 中有很多文本处理工具，比如 TextChunker 类型，可以帮助我们提取文本中的行、段。设定场景如下，用户提问一大段文本，然后我们使用 AI 总结这段文本。 设定一个场景，用户可以每行输入一句话，当用户使用 000 结束输入后，每句话都推送给 AI 总结（不是全部放在一起总结）。 不过经过调试发现，TextChunker 对这段文本的处理似乎不佳，因为文本这么多行只识别为一行、一段。 本小节的演示效果不佳，不过主要目的是，让用户了解 KernelFunctionFactory.CreateFromPrompt 可以更加方便创建提示模板、使用 PromptExecutionSettings 配置温度、使用 TextChunker 切割文本。 配置 PromptExecutionSettings 时，出现了三个参数，其中 MaxTokens 表示机器人回复最大的 tokens 数量，这样可以避免机器人废话太多。 Temperature：值范围在 0-2 之间，简单来说，temperature 的参数值越小，模型就会返回越确定的一个结果。值越大，AI 的想象力越强，越可能偏离现实。一般诗歌、科幻这些可以设置大一些，让 AI 实现天马行空的回复。 TopP：与 Temperature 不同的另一种方法，称为核抽样，其中模型考虑了具有 TopP 概率质量的令牌的结果。因此，0.1 意味着只考虑构成前10% 概率质量的令牌的结果。 更多相关的参数配置，请查看 https://learn.microsoft.com/en-us/azure/ai-services/openai/reference ExecutionSettings 部分的配置，可以针对使用的模型起效，这里的配置不会全部同时起效，会根据实际使用的模型起效。 模板文件由 config.json 和 skprompt.txt 组成，skprompt.txt 中配置提示词，跟 PromptTemplateConfig 的 Template 字段配置一致。 config.json 中涉及的内容比较多，你可以对照下面的 json 跟 实现总结 一节的代码，两者几乎是一模一样的。 目前，我们已经有两个地方使用到提示模板的语法，即变量和函数调用，因为前面已经介绍过相关的用法，因此这里再简单提及一下。 函数一定需要使用 [KernelFunction] 标识，[Description] 描述函数的作用。函数可以一个或多个参数，每个参数最好都使用 [Description] 描述作用。 函数参数中，可以带一个 Kernel kernel，可以放到开头或末尾 ，也可以不带，主要作用是注入 Kernel 对象。 其它一些特殊字符的转义方法等，详见官方文档：https://learn.microsoft.com/en-us/semantic-kernel/prompts/prompt-template-syntax 按照这个示例，我们先在 Env.cs 中编写扩展函数，配置使用 .AddAzureOpenAITextGeneration() 文本生成，而不是聊天对话。 然后编写提问代码，用户可以多行输入文本，最后使用 000 结束输入，将文本提交给 AI 进行总结。进行总结时，为了避免 AI 废话太多，因此这里使用 ExecutionSettings 配置相关参数。 Semantic Kernel 在 Microsoft.SemanticKernel.Plugins 开头的包中提供了一些插件，不同的包有不同功能的插件。大部分目前还是属于半成品，因此这部分不详细讲解，本节只做简单说明。 Semantic Kernel 还有通过远程 swagger.json 使用插件的做法，详细请参考文档：https://learn.microsoft.com/en-us/semantic-kernel/agents/plugins/openai-plugins 安装 Microsoft.SemanticKernel.Plugins.Document（需要勾选预览版），里面包含了文档插件，该文档插件使用了 DocumentFormat.OpenXml 项目，DocumentFormat.OpenXml 支持以下文档格式： DocumentFormat.OpenXml 项目地址 https://github.com/dotnet/Open-XML-SDK Kernel Memory 是一个歪果仁的个人项目，支持 PDF 和 Word 文档、 PowerPoint 演示文稿、图像、电子表格等，通过利用大型语言模型(llm)、嵌入和矢量存储来提取信息和生成记录，主要目的是提供文档处理相关的接口，最常使用的场景是知识库系统。读者可能对知识库系统不了解，如果有条件，建议部署一个 Fastgpt 系统研究一下。 但是目前 Kernel Memory 依然是半产品，文档也不完善，所以接下来笔者也只讲解最核心的部分，感兴趣的读者建议直接看源码。 Kernel Memory 项目文档：https://microsoft.github.io/kernel-memory/ Kernel Memory 项目仓库：https://github.com/microsoft/kernel-memory 要讲解知识库系统，可以这样理解。大家都知道，训练一个医学模型是十分麻烦的，别说机器的 GPU 够不够猛，光是训练 AI ，就需要掌握各种专业的知识。如果出现一个新的需求，可能又要重新训练一个模型，这样太麻烦了。 首先，将 docx、pdf 等问题提取出文本，然后切割成多个段落，每一段都使用 AI 模型生成相关向量，这个向量的原理笔者也不懂，大家可以简单理解为分词，生成向量后，将段落文本和向量都存储到数据库中（数据库需要支持向量）。 然后在用户提问 “什么是报表” 时，首先在数据库中搜索，根据向量来确定相似程度，把几个跟问题相关的段落拿出来，然后把这几段文本和用户的问题一起发给 AI。相对于在提示模板中，塞进一部分背景知识，然后加上用户的问题，再由 AI 进行总结回答。 笔者建议大家有条件的话，部署一个开源版本的 Fastgpt 系统，把这个系统研究一下，学会这个系统后，再去研究 Kernel Memory ，你就会觉得非常简单了。同理，如果有条件，可以先部署一个 LobeHub ，开源的 AI 对话系统，研究怎么用，再去研究 Semantic Kernel 文档，接着再深入源码。 Kernel Memory 支持从网页爬取、导入文档、直接给定字符串三种方式导入信息，由于 Kernel Memory 提供了一个 Service 示例，里面有一些值得研究的代码写法，因此下面的示例是启动 Service 这个 Web 服务，然后在客户端将文档推送该 Service 处理，客户端本身不对接 AI。 这个控制台的作用是帮助我们生成相关配置的。启动这个控制台之后，根据提示选择对应的选项（按上下键选择选项，按下回车键确认），以及填写配置内容，配置会被存储到 appsettings.Development.json 中。 如果读者搞不懂这个控制台怎么使用，那么可以直接将替换下面的 json 到 appsettings.Development.json 。 详细可参考文档： https://microsoft.github.io/kernel-memory/quickstart/configuration 然后编写代码连接到知识库系统，推送要处理的网页地址给 Service。创建一个项目，引入 Microsoft.KernelMemory.WebClient 包。 此外还有 ImportTextAsync、ImportDocumentAsync 来个导入知识的方法。 本节内容稍多，主要讲解如何使用 Kernel Memory 从将文档导入、生成向量、存储向量、搜索问题等。 首先使用 KernelMemoryBuilder 构建配置，配置的内容比较多，这里会使用到两个模型，一个是向量模型，一个是文本生成模型（可以使用对话模型，如 gpt-4-32k）。 首先是讲解将文件存储到哪里，也就是导入文件之后，将文件存储到哪里，存储文件的接口是 IContentStorage，目前有两个实现： Kernel Memory 还不支持 Mongodb，不过可以自己使用 IContentStorage 接口写一个。 接着是，配置文档生成向量模型，导入文件文档后，在本地提取出文本，需要使用 AI 模型从文本中生成向量。 当用户提问时，首先会在这里的 IMemoryDb 调用相关方法查询文档中的向量、索引等，查找出相关的文本。 查出相关的文本后，需要发送给 AI 处理，需要使用 ITextGenerator 接口，目前有两个实现： 将每一段文本使用向量模型解析出向量，存储到 IMemoryDb 接口提供的服务中，如 Postgres数据库。 提问问题或搜索内容时，从 IMemoryDb 所在的位置搜索向量，查询到相关的文本，然后将文本收集起来，发送给 AI（使用文本生成模型），这些文本相对于提示词，然后 AI 从这些提示词中学习并回答用户的问题。 详细源码可以参考 Microsoft.KernelMemory.Search.SearchClient ，由于源码比较多，这里就不赘述了。",https://www.cnblogs.com/whuanle/p/18045341,2025-03-13 10:54:22
师从名剑山,未知作者,"笔者尝试通过梳理字符编码的核心原理，同时简单的介绍一下常见标准，希望能够帮助各位读者构建对字符编码技术的基础认知框架。 在日常开发中，当我们尝试将中文输出到控制台时，点击编译。这时，细心的读者可能会关注到 VS 的控制台会输出一段这样的警告（也有可能是团队规定不允许有警告出现🌝）： 这一系列的问题，归根结底，就是一个字符在计算机中，应该怎么样来表示。也就是字符的编码问题。所以，让我们先来了解了解，现代计算机体系中的编码模型是什么样的。 这一系列问题，追根溯源，其实就是一个字符在计算机中该如何表示的问题，即字符的编码问题。那么，我们先来了解一下现代计算机体系中的编码模型是怎样的。 待编码字符集，定义字符的逻辑集合，不涉及具体的编码逻辑。这一层仅确定“汉”字属于某个字符集。（像 GB2312 就只收录了 6763 个常用的汉字和字符，一些生僻字就没有被收录进来。又比如 ASCII 中就没有中文字符。） 从抽象字符集（ACR）映射到一组非负整数，也就是为每一个字符分配一个唯一的二数字（码位/码点）。例如：Unicode、ASCII、USC、GBK等编码。 在 Unicode 中，“汉”，表示成：\u6C49，而在 GBK 中，“汉”，表示成：0xBABA。 一个从一组非负整数（来自 CCS）到一组特定宽度代码单元序列的映射。我们常说的 UTF-8、UTF-16、UTF-32 就是一个字符编码表。他规定了在抽象字符集中的“非负整数”怎么用字节表示。 通过上面的介绍，相信你对现代编码模型的五层有了基本的了解。感兴趣的读者可以去看 Unicode technical report #17 。 相信大家在日常的开发中，经常听到 Unicode、GB2312、GBK、UTF-8、UTF-16、UTF-32、ANSI，却又对这些概念比较模糊。首先要明确一点的是，Unicode、GB2312、GBK 都是编码字符集，而UTF-8、UTF-16、UTF-32 则是 Unicode 的编码字符表。ANSI 比较特殊，我们待会再具体介绍。 ASCII，全称American Standard Code for Information Interchange（美国信息交换标准代码），于 1963 年发布。标准 ASCII 采用 7 位二进制数来表示字符，因此它最多只能表示 128 个字符。 ASCII 编码虽然解决了英语的编码问题，但中文怎么办呢？汉字有那么多字。此时，就有了 GK2312 编码。 GB2312，又称 GB/T 2312-1980，全称《信息交换用汉字编码字符集·基本集》，与 1980 年由中国国家标准总局发布。GB2312 收录共收录 6763 个汉字，其中一级汉字3755个，二级汉字3008个；同时收录了包括拉丁字母、希腊字母、日文平假名及片假名字母、注音符号、俄语西里尔字母在内的682个字符。 GB2312 使用两个字节来表示，第一个字节称为“高位字节”，对应分区的编号（把区位码的“区码”加上特定值）；第二个字节称为“低位字节”，对应区段内的个别码位（把区位码的“位码”加上特定值）。 随着计算机技术在全世界的广泛应用，越来越多来自不同地区，拥有不同文字的人们也加入了计算机世界，同时也带来了越来越多的种类。在 1991 年，由一个非盈利机构 Unicode 联盟首次发布了 The Unicode Standard，旨在统一整个计算机世界的编码。 Unicode 的编码空间从 U+0000 到 U+10FFFF，划分为 17 个平面（plane），每个平面包含216 个码位（0x0000~0xFFFF），其中第一个平面称为基本多语言平面（Basic Multilingual Plane,BMP），其他平面称为辅助平面（Supplementary Planes）。 由于 GB2312 只收录了 6763 个汉字，有一些 GB2312 推出之后才简化的汉字，部分人用名字、繁体字等未被收录进标准，由中华人民共和国全国信息技术标准化技术委员会1995年12月1日制订了 GBK 编码。GBK 共收录 21886 个汉字和图形符号。 Unicode 转换格式（Unicode Transformation Format，简称 UTF），一个字符的 Unicode 编码虽然是确定的，但是由于不同系统平台的设计不一定一致，以及出于节省空间的目的，对 Unicode 编码的实现方式有所不同。所以就有着不同的 Unicode 转换格式：UTF-8、UTF-16、UTF-32。 UTF-8（8-bit Unicode Transformation Format）是一种用于实现Unicode的编码方式，它使用一到四个字节来表示一个字符。UTF-8具有良好的兼容性和效率，能够与ASCII字符集完全兼容，对于其他语言字符也能够以较高效的方式进行编码。 BOM，全称字节序标志（byte-order mark）。目的是为了表示 Unicode 编码的字节顺序。使用 BOM 模式会在文件头处添加 U+FEFF，对应到 UTF-8 格式的文件，则会在文件起始处添加三个字节：0xEF、0xBB、0xBF。 还记得我们之前在说字符编码方案时，说过 UTF-8 无需处理大端小端。那为什么不需要呢？ 字节序（Endianness）是指多字节数据（如一个整数或一个字符的多字节表示）在内存中的存储顺序。而对于 UTF-8 中，每个使用UTF-8存储的字符，除了第一个字节外，其余字节的头两个比特都是以""10""开始，除了第一个字符以外，其他都是唯一的。 但是 Unicode 标准并不要求也不推荐使用 BOM 来表示 UTF-8，但是某些软件如果第一个字符不是 BOM （或者文件里只包含 ASCII），则拒绝正确解释 UTF-8。 UTF-16 把 Unicode 字符集的抽象码位映射为 16 位长的整数（即码元）的序列，也就是说在 UTF-16 编码方式下，一个 Unicode 字符，需要一个或者两个 16 位长的码元来表示。因此 UTF-16 也是一种具体编码。 Unicode 的基本多语言平面（BMP）内，从U+D800到U+DFFF之间的码位区段是永久保留不映射到Unicode字符。UTF-16就利用保留下来的0xD800-0xDFFF区块的码位来对辅助平面的字符的码位进行编码。 同样我们也以“汉”字为例，它在 Unicode 中为：U+6C49，处于 BMP 中，所以直接用 0x6C49 表示。而另外一个以U+10437编码（𐐷）为例: 那么 ANSI 是什么编码呢？简而言之，ANSI 不是某一种特定的字符编码，而是在不同系统中，表示不同的编码。 在编译器中显式设置输入字符集和执行字符集。对于GCC编译器，可以使用 -finput-charset=UTF-8 -fexec-charset=UTF-8 选项；对于MSVC编译器，可以使用 /source-charset:utf-8 /execution-charset:utf-8 选项，你也可以使用 /utf-8来指定输入字符集和执行字符集都为 UTF-8。 如果输入字符集和执行字符集不一致，编译器需要在编译过程中进行字符编码的转换。当两者不一致时，编译器需进行编码转换，可能引发： 在计算机发展的早期阶段，ASCII编码（美国信息交换标准代码）是主流的字符编码方式，它使用7位二进制数表示128个字符，包括英文字母、数字和一些标点符号。然而，ASCII编码无法满足多语言环境的需求，因为世界上有成千上万种语言和符号。 为了解决这个问题，操作系统和软件开发商引入了代码页的概念。代码页允许系统支持多种字符集，尤其是那些超出ASCII范围的语言字符。在Windows操作系统中，代码页是系统用来处理文本数据的机制。例如，当用户在系统中输入或显示文本时，系统会根据当前的代码页设置来解释这些字符。 假设你有一个文本文件，内容是中文字符“你好”。如果这个文件是用GBK编码保存的，那么它的字节序列可能是 C4 E3 BA C3。操作系统会根据代码页936（GBK）来解释这些字节，并正确显示为“你好”。但如果系统错误地使用了代码页1252（西欧字符集），这些字节会被解释为乱码，因为代码页1252中没有对应的字符。 首先，在 Windows 下，控制台的默认编码是当前系统的代码页（通常是 GB2312），所以如果你输出到控制台的字符不是当前代码页编码对应的字符，那么就会发生乱码。当前系统的代码页通过 cmd 执行命令 chcp来查看。 假如文件的源格式是 UTF-8，那么""中文""这两个字的字节序列为： 涓(E4B8)(ADE6)枃(9687)，其中 ADE6 在 GB2312 中为错误编码，所以会显示一个问号。 这一个问题与输入字符集有关，当文件编码与编译器预期不一致，例如你的文件是GB2312编码，但编译器（如MSVC）默认使用UTF-8（代码页65001）来解析源文件。GB2312和UTF-8是不兼容的编码格式，导致编译器无法正确解析文件中的字符。 笔者的 Visual Studio 工程命令行有一个 /utf-8，也就代表输入、执行编码集都为 utf-8。所以，当你文件的编码为 GB2312 时， 当编译器以UTF-8编码解析文件时，会将 GB2312编码的字节序列 0xD4 0xB4 视为一个潜在的UTF-8字符。然而，根据UTF-8的编码规则： 0xD4 是一个以 1101 开头的字节，表示这是一个两字节字符，第一个字节的格式应为 110xxxxx ，第二个字节的格式应为 10xxxxxx 。但是， 0xD4 的二进制是 11010100 ，而 0xB4 的二进制是 10110100 。 虽然第二个字节符合 10xxxxxx 的格式，但第一个字节的值 0xD4 超出了UTF-8两字节字符的合法范围（ 0xC0 到 0xDF ），因此整个字节序列 0xD4 0xB4 是无效的UTF-8字符。 QString stores a string of 16-bit QChars, where each QChar corresponds to one UTF-16 code unit. 所以，调用上面这些函数就是用指定的格式读取字符，并将这些字符转换成 UTF-16 格式。参看下面的例子： 感谢各位读者阅读本博客，本博客内容在创作过程中，参考了大量百科知识以及其他优秀博客，并结合笔者自身在实际工作中遇到的相关问题。笔者希望通过这篇博客，能为各位读者在字符编码这一块提供一些有价值的见解和帮助。 在撰写过程中，笔者深刻体会到了字符编码这一主题的复杂与广泛，所以如果有哪些言论有失偏颇的，欢迎各位读者在留言区留言，也欢迎大家热烈讨论。",https://www.cnblogs.com/codegb/p/18768600,2025-03-13 10:53:50
ChokCoco,未知作者,"你可以在这里看到：CodePen Demo -- CSS Various Button Shapes | CSS 各种造型按钮 接下来几篇文章中，将在上述基础上，额外补充一些在日常设计稿中，常见的，可能出现的更为复杂的几个按钮，本文，我们来尝试实现这个造型： 此造型如果内部无需镂空，整体还是比较简单的，利用 background: radial-gradient() 径向渐变或者 mask，都能比较轻松的实现。 假设我们有一张矩形背景图案，我们只需要使用 mask 实现一层遮罩，利用 mask 的特性，把 4 个角给遮住即可。 上面的方法以及不适用了，原因在于很难利用 mask 制造一个不规则的内凹圆角形状进行裁剪，因此，我们必须另辟蹊径。 起始角度以及角向渐变的圆心没有改变，但是只让前 90deg 的图形为粉色，而后 270deg 的图形，设置为了透明色。 接下来，我们再给上述图形，增加一个 background-position: -25px, -25px： 为什么会有这样一种现象？如果我们在代码中加入 background-repeat: no-repeat： 解释一下，上述渐变语句的含义是在 30px 30px 处，实现一个半径为 30px 的径向渐变，渐变颜色为透明到黑色到透明，因此可以得到一个圆环： 为了方便大家看清楚 div 整体大小，利用 border: 1px solid #ddd 展示了整个 div 的轮廓，下图开始隐藏 border 然后，利用上述的技巧，我们让渐变图形，整体偏移 -30px -30px，也就是让径向渐变图形的圆心，处于div的左上角 0 0 坐标处。 到这，大家应该能恍然大悟了吧。剩下的工作就比较简单了，我们只需要利用多重线性渐变，把剩余的线条补充出来即可，整个图形完整的代码如下： 当然，我们肯定是需要不同边框大小、颜色的各种不镂空的内凹圆角边框，我们利用 CSS 变量再进行一下封装： 完整的代码，你可以戳这里：CodePen Demo -- 内凹角边框 & inner-corner-border 更多精彩 CSS 技术文章汇总在我的 Github -- iCSS ，持续更新，欢迎点个 star 订阅收藏。",https://www.cnblogs.com/coco1s/p/18699898,2025-03-13 10:54:08
why技术,未知作者,在实际业务场景下，涉及到业务代码和不同的微服务，导致问题有点难以定位，但是最终分析出原因之后，发现可以用一个很简单的例子来演示。 这个代码的逻辑非常简单，首先我们搞了一个线程池，然后起一个 for 循环往线程池里面仍了 5 个任务，这是核心逻辑。 其他的 StopWatch 是为了统计运行时间用的。 至于 CountDownLatch，你可以理解为在业务流程中，需要这五个任务都执行完成之后才能往下走，所以我搞了一个 CountDownLatch。 这个代码运行起来是没有任何问题的，我们在日志中搜索“执行完成”，也能搜到 5 个，这个结果也能证明程序是正常结束的： 问题就是由于转异步了，所以 for 循环里面的任务中的 countDownLatch 很快就减到 0 了。 很简单嘛，需要子任务真正执行完成后，父任务的 countDownLatch 才能进行 countDown 的动作。 比如在这个场景下，我们直观上的感受是“卡住了”，那就 dump 一把线程，管它有枣没枣，打一杆子再说： 通过 Dump 文件，可以发现线程池的线程都在 MainTest 的第 30 行上 parking ，处于等待状态： 是父任务在等待子任务执行结束，运行 finally 代码，把 countDownLatchSub 的计数 countDown 到 0，才会继续执行： 换句话说就是父任务被拦住是因为子任务的 finally 代码中的 countDownLatchSub.countDown() 方法没有被执行。 如果线程池的任务之间存在父子关系，那么请不要使用同一个线程池。如果使用了同一个线程池，可能会因为子任务进了队列，导致父任务一直等待，出现假死现象。 这篇文章是 2020 年写的，其中就是使用了父子任务+CountDownLatch 的模式，来实现所谓的“多线程事务”。 它已经由一篇技术文章变成了一个生活的锚点，其中的蛛丝马迹，能让我从脑海深处想起之前生活中一些不痛不痒的印迹。,https://www.cnblogs.com/thisiswhy/p/18302939,2025-03-13 10:54:16
痴者工良,未知作者,"Maomi.Torch 项目仓库：https://github.com/whuanle/Maomi.Torch 本章内容主要基于 Pytorch 官方入门教程编写，使用 C# 代码代替 Python，主要内容包括处理数据、创建模型、优化模型参数、保存模型、加载模型，读者通过本章内容开始了解 TorchSharp 框架的使用方法。 https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html 首先添加以下代码，查找最适合当前设备的工作方式，主要是选择 GPU 开发框架，例如 CUDA、MPS，CPU，有 GPU 就用 GPU，没有 GPU 降级为 CPU。 训练模型最重要的一步是准备数据，但是准备数据集是一个非常繁杂和耗时间的事情，对于初学者来说也不现实，所以 Pytorch 官方在框架集成了一些常见的数据集，开发者可以直接通过 API 使用这些提前处理好的数据集和标签。 Pytorch 使用 torch.utils.data.Dataset 表示数据集抽象接口，存储了数据集的样本和对应标签；torch.utils.data.DataLoader 表示加载数据集的抽象接口，主要是提供了迭代器。这两套接口是非常重要的，对于开发者自定义的数据集，需要实现这两套接口，自定义加载数据集方式。 Pytorch 有三大领域的类库，分别是 TorchText、TorchVision、TorchAudio，这三个库都自带了一些常用开源数据集，但是 .NET 里社区仓库只提供了 TorchVision，生态严重落后于 Pytorch。TorchVision 是一个工具集，可以从 Fashion-MNIST 等下载数据集以及进行一些数据类型转换等功能。 在本章中，使用的数据集叫 FashionMNIST，Pytorch 还提供了很多数据集，感兴趣的读者参考：https://pytorch.org/vision/stable/datasets.html 现在开始讲解如何通过 TorchSharp 框架加载 FashionMNIST 数据集，首先添加引用： 注意，与 Python 版本有所差异， Pytorch 官方给出了 ToTensor() 函数用于将图像转换为 torch.Tensor 张量类型，但是由于 C# 版本并没有这个函数，因此只能手动指定一个转换器。 启动项目，会自动下载数据集，接着在程序运行目录下会自动创建一个 data 目录，里面是数据集文件，包括用于训练的数据和测试的数据集。 数据集是 Dataset 类型，继承了 Dataset > 类型，Dataset 本质是列表，我们把 Dataset 列表的 item 称为数据，每个 item 都是一个字典类型，每个字典由 data、label 两个 key 组成。 在上一节，已经编写好如何加载数据集，将训练数据和测试数据分开加载，为了了解 Dataset ，读者可以通过以下代码将数据集的结构打印到控制台。 通过观察控制台，可以知道，每个数据元素都是一个字典，每个字典由 data、label 两个 key 组成,dic[""data""] 是一个图片，而 label 就是表示该图片的文本值是什么。 Maomi.Torch 框架提供了将张量转换为图片并显示的方法，例如下面在窗口显示数据集前面的三张图片： 使用 Maomi.ScottPlot.Winforms 库，还可以通过 img.ShowImageToForm() 接口通过窗口的形式显示图片。 由于 FashionMNIST 数据集有 6 万张图片，一次性加载所有图片比较消耗内存，并且一次性训练对 GPU 的要求也很高，因此我们需要分批处理数据集。 torch.utils.data 中有数据加载器，可以帮助我们分批加载图片集到内存中，开发时使用迭代器直接读取，不需要关注分批情况。 注意，分批是在 DataLoader 内部发生的，我们可以理解为缓冲区大小，对于开发者来说，并不需要关注分批情况。 接下来定义一个神经网络，神经网络有多个层，通过神经网络来训练数据，通过数据的训练可以的出参数、权重等信息，这些信息会被保存到模型中，加载模型时，必须要有对应的网络结构，比如神经网络的层数要相同、每层的结构一致。 该网络通过接受 28*28 大小的图片，经过处理后输出 10 个分类值，每个分类结果都带有其可能的概率，概率最高的就是识别结果。 为了训练模型，需要定义一个损失函数和一个优化器，损失函数的主要作用是衡量模型的预测结果与真实标签之间的差异，即误差或损失，有了损失函数后，通过优化器可以指导模型参数的调整，使预测结果能够逐步靠近真实值，从而提高模型的性能。Pytorch 自带很多损失函数，这里使用计算交叉熵损失的损失函数。 torch.Tensor 类型的 .shape 属性比较特殊，是一个数组类型，主要用于存储当前类型的结构，要结合上下文才能判断，例如在当前训练中，x.shape 值是 [64,1,28,28]，shape[1] 是图像的通道，1 是灰色，3 是彩色(RGB三通道)；shape[2]、shape[3] 分别是图像的长度和高度。 通过上面步骤可以看出，“训练” 是一个字面意思，跟人类的学习不一样，这里是先使用模型识别一个图片，然后计算误差，更新模型参数和权重，然后进入下一次调整。 要使用模型识别图片，只需要使用 var pred = model.call(x); 即可，但是因为模型并不能直接输出识别结果，而是根据网络结构输出到每个神经元中，每个神经元都表示当前概率。在前面定义的网络中，nn.Linear(512, 10)) 会输出 10 个分类结果，每个分类结果都带有概率，那么我们将概率最高的一个结果拿出来，就相当于图片的识别结果了。",https://www.cnblogs.com/whuanle/p/18700127,2025-03-13 10:54:01
学习新知识的小白白,未知作者,"家里有一台机顶盒（移动魔百盒）淘汰下来了，博主想要物尽其用，看看是否能将其改造为一台Linux""开发机""，量身定制系统，用作开发和测试，以及跑服务器 机顶盒型号：移动魔百盒CM201-2(CH)，芯片组: hi3798mv300（hi3798mv3dmm），其他型号类似 理论上适用于以下SOC的机顶盒：Hi3798Mv100 / Hi3798Cv200 / Hi3798Mv200 / Hi3798Mv300 （烧录也可以命令行形式甚至不需要Windows，需要参考海思官方文档，为了方便故在Windows平台下进行烧录） 源码链接：https://github.com/martixjohn/HiSTBLinuxV100R005C00SPC050 主板接线如图，从左到右依次连接到转接板的GND, TXD, RXD，不同板子可能有所差别，可以尝试调一下TXD, RXD顺序 Board->Boot Regfile Config List: 启动的配置文件，将第一个改为自己的机顶盒匹配的配置文件名称 修改分区配置文件emmc_partitions.xml，将最后一项的SelectFile改为ubuntu镜像的文件名，Length属性改为-，表示占满存储器剩余空间 取一根网线连接机顶盒主板和路由器，确保PC和机顶盒能互通，配置IP信息（使用网络传输进行烧录，原因是串口传输速率过慢） 在板端配置下，网关一般设置为路由器内网IP，一般路由器默认为192.168.1.1，子网掩码需要按照路由器进行配置，家用路由器默认是255.255.255.0，IP随意配置保证与路由器在同一子网，如192.168.1.200，物理地址随意设置 注：以上配置的目的是让机顶盒固定IP地址，以便将固件通过以太网传输到机顶盒，机顶盒会按照配置自动设置对应IP和PC建立通信 加载分区配置和固件：依次选择HiBurn->烧写EMMC->浏览（选择分区配置文件emmc_paritions.xml） 打开机顶盒开关，先是打印bootloader等启动信息后，直到一串绿色的OK字样显示出来，表示Ubuntu已经成功刷入！ 本文来自博客园，作者：学习新知识的小白白，转载请注明原文链接：https://www.cnblogs.com/onecainiao/p/18353586",https://www.cnblogs.com/onecainiao/p/18353586,2025-03-13 10:54:14
hez2010,未知作者,Brainfuck 是由 Urban Müller 在 1993 年创造的一门非常精简的图灵完备的编程语言。 正所谓大道至简，这门编程语言简单到语法只有 8 个字符，每一个字符对应一个指令，用 C 语言来描述的话就是： 然后只需要提供一个已经初始化为 0 的字节数组作为内存、一个指向数组的指针、以及用于输入输出的两个字节流就能够让程序运行了。 既然要用 C# 类型系统来构建 Brainfuck 的编译器，我们需要首先对 C# 类型系统有一些认知。 C# 的类型系统构建在 .NET 的类型系统之上，而众所周知 .NET 是一个有具现化泛型的类型系统的平台，意味着泛型参数不仅不会被擦除，还会根据泛型参数来分发甚至特化代码。 对于上面的代码，调用 new Foo ().Print() 会输出 0，调用 new Foo ().Print() 会输出 0001-01-01T00:00:00，而调用 new Foo ().Print() 则会输出 null。 你可能好奇为什么上面的 Calculator 里 left 和 right 可以直接加，这是因为 .NET 支持接口的虚静态成员。上面的 IAdditionOperators 接口其实定义长这个样子： 我们对 T 进行泛型约束 where T : IAdditionOperators 之后，就使得泛型代码中可以通过类型 T 直接调用接口中的静态抽象方法 operator+。 有了上面的知识，我想知道在这套类型系统之上，.NET 的编译器到底能生成多优化的代码，那接下来我们进行一些小的测试。 首先让我们用类型表达一下具有 int 范围的数字，毕竟之后构建 Brainfuck 编译器的时候肯定会用到。众所周知 int 有 32 位，用 16 进制表示那就是 8 位。我们可以给 16 进制的每一个数位设计一个类型，然后将 8 位十六进制数位组合起来就是数字。 这里我们想把数字和数位区分开，因此我们定义一个跟 IHex 长得差不多但是泛型的接口 INum 用来给数字 Int 实现，之所以是泛型的是因为给万一没准以后想要扩展点浮点数之类的做考虑： 这里我们给 Value 加了 [MethodImpl(MethodImplOptions.AggressiveInlining)] 确保这个方法会被编译器 inline。 如此一来，如果我们想表达一个 0x1234abcd，我们就可以用 Int 来表达。 对于 Brainfuck 源代码的解析，可以说是非常的简单，从左到右扫描一遍源代码就可以，这里就不详细说了。问题是怎么产生编译结果呢？ 我们 Brainfuck 不需要什么内存边界检查，因此这里我用了一个 UnsafeAt 扩展方法跳过边界检查： 然后，Brainfuck 是支持循环的，这要怎么处理呢？其实也很简单，模拟 while (*ptr) { 这个操作就行了，也就是反复执行当前操作更新指针，直到指针指向的数据变成 0，然后跳到下一个操作去。 这段程序很粗暴的分别把内存从左到右写成 Hello World! 的每一位，然后把指针移回到开头后逐位输出。 不过这么看 Hello World! 还是太长了，不适合用来一上来就展示，我们换个简单点的输出 123： 这里为了简洁，我把数字全都带入了数字类型，不然会变得很长。例如实际上 49 应该表达为 Int 。 如果我们想以 JIT 的形式运行 Brainfuck 代码，那如何在运行时生成类型然后运行代码呢？我们在 .NET 中有完善的反射支持，因此完全可以做到运行时创建类型。 你会发现，因为编译出的东西是类型，因此我们不仅可以在 JIT 环境下跑，还能直接把类型当作程序 AOT 编译出可执行文件！只需要编写一个入口点方法调用 Run 即可： 上面的 /p:IlcInstructionSet=native 即 C++ 世界里的 -march=native，OptimizationPreference=Speed 则是 -O2。 这里我们采用一段用 Brainfuck 编写的 Mandelbrot 程序进行性能测试，代码见 Pastebin。 最后 .NET AOT 在这个项目里取得了最好的成绩，当然，这离不开 .NET 类型系统层面的零开销抽象。,https://www.cnblogs.com/hez2010/p/18696074/brainfly-brainfuck-compiler-built-with-csharp,2025-03-13 10:54:11
"Alfred Zhao


                从Oracle起航，领略精彩的IT技术。",未知作者,本文目标：部署 DeepSeek 本地模型，并通过 Ollama 提供 API 支持，Chatbox 提供 UI 界面。 原则：不搞那些高深的玩法，让小白也能理解并真正的上手实践。 目前deepseek-r1模型大小提供了7个选择：1.5b、7b、8b、14b、32b、70b、671b。 因为我笔记本的显卡配置较低，所以这里只能选择最小的1.5b模型来做演示： 你可以根据你的硬件情况选择，通常模型大小（参数量）越大，模型的理解和生成能力越强，但也会消耗更多的计算资源。 我这里选择macOS，点击下载。 下载文件大小不到200M，文件名为：Ollama-darwin.zip。 这里Ollama默认给出的例子是下载/运行llama3.2大模型， 我们这里不使用这个llama3.2模型，直接下载/运行deepseek，参数选择最小的1.5b，在终端窗口运行下面命令： 下载的Chatbox-1.9.7.dmg，大小100M多点，点击安装，按下面提示拖到Applications内： 注意，这里我故意选错成DeepSeek API，这也是初学者经常会选错的，实际上，若选择这个你就找不到你本地的DeepSeek模型。 实际正确应该选择OLLAMA API，然后就可以看到我们上一步安装好的deepseek-r1:1.5b。 配置好DeepSeek本地模型之后，就可以实现在断网情况下自由问答了，比如，此刻我正在写这篇文章，于是就问他帮我想几个备选的标题： 他真的迅速给我起了10个吸引眼球的标题，还提供了它思考的过程，而且在我这4年前的电脑上跑起来都很迅速。 嗯，真的很赞！不过我还是决定用自己最初想的朴实标题，不做标题党了。 Copyright © 2025 AlfredZhao Powered by .NET 9.0 on Kubernetes,https://www.cnblogs.com/jyzhao/p/18700202/shou-ba-shou-jiao-ni-bu-shu-deepseek-ben-de-mo-xin,2025-03-13 10:54:05
探秘Transformer系列之（1）：注意力机制,未知作者,"因为各种事情，好久没有写博客了，之前写得一些草稿也没有时间整理（都没有时间登录博客和微信，导致最近才发现好多未读消息和私信，在这里和各位朋友说下万分抱歉）。现在恢复更新，是因为最近有些从非AI领域转过来的新同学来找我询问是否有比较好的学习资料，他们希望在短期内迅速上手 Transformer。我在网上找了下，但是没有找到非常合适的系统的学习资料，于是就萌发了自己写一个系列的想法，遂有此系列。在整理过程中，我也发现了自己很多似是而非的错误理解，因此这个系列也是自己一个整理、学习和提高的过程。 本文为系列第一篇，主要目的是引入Transformer概念和其相关背景。在2017年，Google Brain的Vaswani等人在论文”Attention is All You Need“中发布了Transformer。原始论文中给出Transformer的定义如下： Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution。 其中提到了sequence，RNN，convolution，self-attention等概念，所以我们接下来就从这些概念入手进行分析。我们先开始从Seq2Seq介绍，然后逐渐切换到注意力机制，最后再导出Transformer模型架构。 seq2seq（Sequence to Sequence/序列到序列）概念最早由Bengio在2014年的论文“Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation”中提出，其代表从一个源序列生成一个目标序列的操作。因为机器翻译是大家较熟悉且容易理解的领域，因此后续我们主要使用机器翻译来进行讲解，避免引入过多概念。 机器翻译其实就是文本生成。语言模型将文本看作是时间序列。在此视角下，每个单词都和它之前的单词相关，通过学习前面单词序列的统计规律就可以预测下一个单词。因此，机器翻译会从概率角度对语言建模，让新预测的单词和之前单词连成整个句子后最合理，即原有句子加上新预测单词后，成为整个句子的概率最大。这就涉及到自回归模型。 自回归（Autoregressive）模型是一种生成模型，其语言建模目标是根据给定的上下文来预测下一个单词。遵循因果原则（当前单词只受到其前面单词的影响），自回归模型的核心思想是利用一个变量的历史值来预测其未来的值，其将""序列数据的生成""建模为一个逐步预测每个新元素的条件概率的过程。在每个时间步，模型根据之前生成的元素预测当前元素的概率分布。 下图给出了自回归模型的示例。模型每次推理只会预测输出一个 token，当前轮输出token 与历史输入 token 拼接，作为下一轮的输入 token，这样逐次生成后面的预测token，直到新输出一个结束符号或者句子长度达到预设的最大阈值。就下图来说，模型执行序列如下： 将预测出的第一个单词“wear”结合原输入一起提供给模型，即第二次模型的输入是“You should wear”。 将预测出的第二个单词“shoes”结合原输入一起提供给模型，即第三次模型的输入是“You should wear shoes”。 该过程中的每一步预测都需要依赖上一步预测的结果，且从第二轮开始，前后两轮的输入只相差一个 token。 隐变量模型是一种引入隐变量来表示过去信息的模型。自回归模型在预测时会把过去观测到的信息总结起来记作\(h_t\)，并且更新预测\(x_t\)。即总结\(h_t = g(h_{t-1}, x_{t-1})\)，然后基于\(x_t = P(x_t \mid h_{t})\)来估计\(x_t\)。由于\(h_t\)从未被观测到，因此\(h_t\)就是隐变量，这类模型也被称为隐变量自回归模型（latent autoregressive models）。 有了\(ℎ_t\)之后，其实预测变换为两个子问题。一个问题是如何根据之前的隐变量 \(ℎ_{t−1}\) 和之前的输入信息\(x_{t-1}\)得到现在的隐变量\(ℎ_t\)，另一个问题是如何根据当前的隐变量\(ℎ_t\)和之前的输入\(x_{t-1}\)得到当前的\(x_t\) 。其实，这就是编码器-解码器模型要面对的问题。 目前，处理序列转换的神经网络模型大多是编码器-解码器（Encoder-Decoder）模型。传统的RNN架构仅适用于输入和输出等长的任务。然而，大多数情况下，机器翻译的输出和输入都不是等长的，因此，对于输入输出都是变长的序列，研究人员决定使用一个定长的状态机来作为输入和输出之间的桥梁。于是人们使用了一种新的架构：前半部分的RNN只有输入，后半部分的RNN只有输出（上一轮的输出会当作下一轮的输入以补充信息），两个部分通过一个隐状态（hidden state）来传递信息。把隐状态看成对输入信息的一种编码的话，前半部分可以叫做编码器（Encoder），后半部分可以叫做解码器（Decoder）。这种架构因而被称为编码器-解码器架构，所用到的模型就是编码器-解码器模型，具体如下图所示，图中编码器和解码器通过一个中间隐状态C来完成信息交互。 如何压缩？人们很容易想到马尔可夫假设，即系统的未来状态只与当前状态有关。这也被称为近因效应：从文本生成角度看，就是当前词只与距离它比较近的个词更加相关。如果考虑前面n个单词，这就得到了N-gram模型，即当前单词的概率取决于前n个单词。但是基于马尔科夫假设的模型难以处理句子中的长距离依赖关系（某个单词依赖句子中很早的单词），也没有考虑深层语义关系。而且N-gram模型的大小几乎是n的指数倍，如果n数目太大，则n个单词组合数可能性太多，对资源消耗过大，因此需要新的模型。新模型应该不仅仅简单的关注单词的频率和顺序，还可以考虑更长距离的依赖关系，却又不必显式考虑那么多单词组合可能性，于是人们想到使用神经网络来拟合。 MLP 是最基本的神经网络模型之一，它可以将词向量序列映射到一个固定长度的向量表示，然后将这个向量输入到一个softmax层中，计算出下一个词的概率分布。MLP虽然理论上没有距离长依赖问题，但是很难训练好。CNN/RNN/Transformer等网络结构都可以看作是在MLP上添加的约束条件。通过这些先验约束，同等参数量下的优化难度被降低，模型更容易找到最优解。前面提到的“使用神经网络来拟合”就是使用CNN，RNN或者Transformer来实现编码器和解码器。 因为我们本系列介绍的主角是Transformer，就说明Transformer在实现编码器和解码器上是存在优势的，因此我们先来看看CNN和RNN方案的问题所在。 注：本节只是从普遍意义或者说是在典型问题上进行阐释，并非定论。因为CNN和RNN方案也是在不停的发展，其某一个阶段的方案可能解决（或者缓解）了下面提到的问题。 当面临冗长且信息密集的输入序列时，编码器-解码器模型在整个解码过程中保持相关性的能力可能会减弱。为了更好的说明，我们先看看序列转换面对的主要技术挑战：对齐问题和长依赖问题（或者说是遗忘问题）。 我们来看看为什么要对齐。首先，在某些领域（比如语音识别）中，虽然输入与输出的顺序相同，但是没有一一对应的关系。其次，在某些领域（比如机器翻译）中，在处理完整个输入序列后，模型的输出序列可能和输入序列的顺序不一致。以机器翻译为例，假如我们要让模型将英语 ""how are you"" 翻译为中文 ""你好吗""，或者将“Where are you”翻译成“你在哪里”，我们会发现翻译结果中的语序和原来句子的语序并不相同，同时，一些翻译结果并不能与英语中的词汇一一对应到。 不对齐问题带来的最大困境是：在时间序列的 t 时刻，我们并不能确定此时的模型是否已经获得了输出正确结果所需要的所有信息。因此，人们往往先把所有输入编码到一个隐状态，然后逐步对这个隐状态进行解码，这样才能确保在解码过程中模型一定收到了所需的全部信息。虽然此方案可以保证输入信息的完整性，但却有一个明显缺陷，即在解码过程中，无法确定贡献度。比如，当把”I love you""翻译成“我爱你”时，“我”应该与“I”对齐（因为其贡献最大），但是该方案中，""I"",""love"",""you""这三个词对“我”的贡献都是一致的。 我们以下面句子为例来进行分析。“秋蝉的衰弱的残声，更是北国的特产，因为北平处处全长着树，屋子又低，所以无论在什么地方，都听得见它们的啼唱。” 将例句从英文翻译成中文时，英文和中文明显是有对齐关系的，因此需要知道哪个英文单词对应到哪个中文。比如将”它们“翻译成”They“，但是“它们”代表什么呢？是“树”？“屋子“？还是”秋蝉“？通过”啼唱“和知识，我们知道“秋蝉”和“它们”指代是同一个对象，但是如果把”听得见它们的啼唱“ 修改为“看见它们的树荫”，则“它们”指代的就是“树”了。 人类可以很容易的同时看到“秋蝉”和“它们”这两个词然后把这两个词关联起来，即人们知道“它们”和“秋蝉”有长距离的依赖关系，从而理解整个句子。但是对于计算机或者对于模型来说，”秋蝉“和”它们“在例句中的距离太长了，很容易被两个词中间的其它词干扰。为了准确给出最终的答案，神经网络需要对前面”秋蝉的衰弱的残声“和”它们“之间的交互关系进行建模。然而，某些神经网络很难处理长距离依赖关系，因为处理这种依赖关系的关键因素之一是信号在网络中穿越路径的长度，两个位置之间路径越短，神经网络就越容易学习到这种长距离依赖关系，两个位置之间距离越远，建模难度就越大。如果模型无法处理长距离依赖，则会出现长时信息丢失，也就是产生了遗忘问题。 CNN的本质是学习空间数据中的局部依赖关系。CNN的卷积操作可以提取重要特征，但是因为单个卷积核的长度一般比较小，所以CNN的卷积感受视野是局部的，提取的是局部特征，进行局部信息计算。即CNN对相对位置敏感，对绝对位置不敏感，难以提取序列的长距离依赖关系。 为了让CNN可以处理长序列，人们一般会堆叠更多的卷积，通过叠加多层卷积区去把局部感受野进行扩大，让卷积网络依靠深度来弥补全局信息的缺失，从而捕捉长距离依赖。在这种方案中，不同卷积层会提供不同层次的特征，进而在上层完成长距离信息依赖计算，把长序列的信息压缩到一个卷积窗口内部，以此来让模型有机会捕捉到长距离依赖和复杂的结构关系。 比如下图所示，最下层CNN使用滑动窗口作用于这个文本序列，每个窗口都对其中的数据进行处理。A1窗口获取到了“秋蝉”这个信息，A3窗口获取到了“它们”这个信息。但是，因为“秋蝉”和“它们”距离太远，所以没有一个单一窗口可以把这两个词建立起依赖关系，即没有单一窗口可以同时看到这两个词。因此，模型只能持续堆叠卷积网络，不断加深整个网络，这样C1这个窗口才可以同时包含“秋蝉”和“它们”的信息。 但是深度就代表间接，间接就代表着流失。在CNN方案中，因为信息被“逐级加工，层层抽象”，而且信息传递过程不够透明，信息会在过深的网络传播过程之后只有部分保留，导致模型性能下降。所以CNN一般在长依赖关系建模场景使用较少，更适合短文本计算。 从表象上看，RNN是时序结构，后面的时刻天然就依赖于前面时刻的输出。从本质上来说，RNN是一种连接主义模型，具有选择性地在序列步骤间传递信息的能力，可以对序列上下文信息以及不同范围元素间的局部依赖关系进行捕捉。RNN的独到之处在于引入了“记忆”功能，让网络能记住之前输入的信息。随着数据在RNN中的流动，之前时间步的记忆会作为输入参与到当前数据的处理中，让模型能够动态地融合时间上下文和序列的历史信息。因为可以有效地处理变长的序列数据，理论上，RNN可以预测无限长句子，利用前面所有信息，所以非常适合翻译场景。 事实上，在Transformer出现之前，编码器和解码器通常是由RNN或其变体（如 LSTM 或 GRU）组成的。我们先看看如何用RNN实现编码器。以下图为例，编码器需要把“北国的特产”这个句子编码成隐状态。图中的每个方块都是一个简单的RNN单元。每个RNN单元接收两个输入（输入词和隐状态），输出一个隐状态。 模型在第一步得到输入h0和“北”，调用函数f()进行计算，得到输出h1=f(h0, 北)。h0是第一个隐状态（通常内容是数值0或者随机值）。模型在第二步得到输入h1和“国”，同样调用函数f()进行计算，得到h2。以此类推，模型最终输出h5。在计算过程的每一步t，前面所有节点得到的信息都保存在上一步计算出来的中间隐状态ht中，因此，后一个词的计算都会用到前面所有词的输出结果。 可以把隐藏状态ht看作是信息循环的载体，其承载的信息可以跨时间步在RNN中进行传递。随着数据在RNN中的流动，之前时间步的激活状态会作为输入参与到当前数据的处理中，让模型能够动态地融合时间上下文和序列的历史信息。因此从理论上来说，RNN可以通过隐状态得到任意两个词的依赖，无论这两个词距离多远，它们的信息一定会在某一步计算之中汇聚。 我们再给出编码器-解码器的结构图，其中si是解码器在时刻\(i\)的隐状态。编码器读入输入的token xi，并对每个token xi生成一个隐状态hi。从第一个h1到最后一个hm，这些隐状态不断将前面信息积累。最后的携带整个输入序列信息的hm会作为解码器的输入s0。 RNN的缺点同样鲜明。在RNN方案中，在每个时间步，RNN都会将序列中之前的所有信息压缩到一个固定长度的隐向量，最终编码器和解码器之间只通过这个固定长度的隐状态来传递信息。固定长度的隐状态或者说有限的记忆能力在处理长序列时会导致接下来的几个问题，比如信息遗失、信息瓶颈等。 RNN需要对序列内容进行逐步处理， 每一步的输出取决于先前的隐藏状态和当前的输入。RNN这种串行计算在本质上是一种递归，其阻碍了训练时的并行计算，导致训练效率较低，训练时间过长。 RNN的网络结构特点导致难以训练。RNN 用于信息传输通路只有一条，并且该通路上的计算包含多次非线性激活操作，当 RNN 处理长序列时，因为时间步增加带来的多层激活函数的嵌套会导致梯度反传时指数级地衰减（消失）或增长（爆炸），这就是梯度消失问题或者梯度爆炸问题。当梯度消失时，前面的梯度信息无法有效地传递 到后面，也就导致词之间距离越远，前面对后面的影响越弱，所以RNN网络难以学习远距离依赖关系。当梯度爆炸时，网络的权重会变得极大，导致网络不稳定。而且当面对长序列时，RNN需要大量内存来维持长序列的隐状态，比如需要完整理解整个句子乃至整篇文章才能做出判断，这些内存负担对训练也造成了很大挑战。 我们再仔细看看”关系距离问题“。对于CNN方案，序列的第一个词和最后一个词要通过多层卷积后才可以建立联系。下图中的粗线为CNN结构中两个词建立关系所需的最长距离。 而RNN方案需要对序列“从头看到尾”才能确定给这两个词建立关系所需的距离。下图中的粗线是RNN结构中两个词建立关系所需的最长距离。 因此我们要面对的问题是：如何把大量的token压缩到一个隐藏状态中，而且该状态还可以有效的捕捉它们的底层结构和关系。如果想从根本上解决问题，我们有如下几步可以选择： 注意力（Attention）机制由Bengio团队2015年在论文“NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE”中提出，其主要思路为通过模仿人类观察事物的行为来降低算法复杂度、提高性能。人类在感知、认知和行为决策过程中会选择性地关注和处理相关信息，从而提高认知效率和精度，比如人类可以依据兴趣和需求选择关注某些信息而忽略或抑制其它信息，并且可以在多任务上分配注意力从而达到同时处理多个信息的目的。大家最熟悉的例子就是，当一个人看图片时，他会先快速通览，然后对重点区域进行特殊关注。而在文本生成的每个阶段，并非输入上下文的所有片段都同样重要。比如在机器翻译中，句子“A boy is eating the banana”中的“boy”一词并不需要了解整个句子的上下文之后，再来进行准确翻译。 注意力机制的本质可以用一句话来总结：上下文决定一切（context is everything）。一个字 / 词在一篇文本中表达的意思通常与它的上下文有关。比如下面两个句子中都有“transformer”这个单词，但是第一个“transformer”应该翻译成“变压器”，第二个“Transformer”最好不做翻译。 如何才能对“transformer”这个多义词进行语义区分？我们必须考虑单词的上下文才能更好的把单词的语义识别出来，即不仅仅要考虑到词本身，还要考虑其他词对这个词的影响，也就是语境的影响。比如第一个句子的“pole”、”fallen”和“wires”这几个邻近单词暗示了此处的“transformer”和真实物理环境相关。第二个句子的“model”和“natural language processing and image classification”则直接告诉我们此处的“Transformer”是深度学习相关概念。最终我们通过上下文语境可以推断出“Transformer”的准确含义，从而把这两个英文句子翻译成如下： 这就是注意力机制的作用：将每个词与序列中的其他词联系起来，通过句子中的其它词对我们关注词的语义进行推断。 注意力机制也是一种资源分配方式。目前我们知道了上下文的重要性，但是这还不够，因为一个单词的上下文包括很多其它单词，不同单词对目标单词的影响力往往不同。以翻译为例，因为输入语句是一个连贯的整体，所以每个输入词\(X_i\)都会给每个输出词\(Y_i\)造成影响。因此在考虑单词上下文时候，也需要考虑出上下文中每一个元素应该考虑多少。比如第二个英文句子中，“model”对“Transformer”的影响力必然是最大的。所以我们需要一种机制来依据不同的上下文聚焦于不同的信息，这样，一来可以使得序列中的重要元素被投射以较高注意力，重要信息不会被淹没；二来可以将有限的计算资源用来处理更重要的信息，从而做到提纲挈领。 注意力机制就是这种资源分配机制。其在学习的过程中自适应的给予输入的不同词赋予不同的注意力权重，从而区分输入的不同部分对输出的影响，自适应的学习应该关注的重点位置，做出准确判断，即注意力赋予了模型分辨的能力。 其实，论文“Recurrent Models of Visual Attention”中有一段话就深刻的印证了资源分配这个角度。具体如下：人类感知的一个重要特性是，人们不会一次处理整个场景。相反，人类有选择地将注意力集中在视觉空间的某些部分上，以在需要的时间和地点获取信息，并随着时间的推移将不同注视点的信息结合起来，建立场景的内部表示，指导未来的眼球运动和决策。将计算资源集中在场景的各个部分可以节省“带宽”，因为需要处理的“像素”更少。但它也大大降低了任务的复杂性，因为感兴趣的对象可以放置在注视的中心，而注视区域外的视觉环境的无关特征（“杂乱”）自然会被忽略。其英文原文如下： 确定了资源分配的原则之后就可以进行信息交换。注意力机制计算过程就是序列中元素交换信息的过程。注意力机制的输入是一个序列或者集合，注意力机制会从输入中选择性提取信息，据此计算出一组权重。这组权重代表了各个信息的重要程度，使用这种权重与原始信息相乘就得到了注意力处理后的加权信息。 信息交换在一定程度上起到了RNN中记忆力模块的作用，使得注意力编码器也能像RNN一样对一个复杂的语句或者场景进行理解和解析，比如： 将注意力机制应用在一个序列内部（这就是人们经常提到的自注意力机制），可以让序列中每个词都和序列中其他词关联起来，这样序列中每一个元素都有机会根据自身特征，有选择性地依据词之间的相关性来吸取整个序列中每一个其他元素的信息，对本身进行动态调整。这使得模型能够捕捉长距离依赖关系，不受距离的影响。考虑以下两个句子，第一句中的“it”就指代猫，因此“it”更多吸收了“cat”的信息。第二个句子的“it”指代牛奶，因此“it”更多吸收了“milk”的信息。 自注意力机制的目的是为当前单词创建抽象的、丰富的表征。这个表征是本单词被相同序列中的其它单词所影响的结果。经过自注意力机制处理之后，现在每个新单词都融入了其它单词的部分信息，这是一种数据依赖的加权平均，这种表征更加丰富。如果非要将自注意力机制和之前的注意力机制进行对比的话： 通过上面的原理分析，我们可以知道，注意力的核心思想就是帮助模型为输入的不同部分分配不同的权重，这样可以提取关键信息，让模型判断更精准，更加节省算力和存储。但是如何实现注意力机制？这就要解决两个问题： 论文”A General Survey on Attention Mechanisms in Deep Learning“用下图总结了注意力模型的通用结构，论文作者把这种通用架构叫做任务模型。任务模型包含四个部分： 我们用一个句子“北国的特产”为例来解释下图。图上X就是“北国的特产”这个完整的句子，F是从“北国的特产”提取到的特征向量列表，Z是“特”，q是从“特”提取出来的特征向量，我们目的就是要获取特征向量列表中哪个特征向量包含对于”特“最重要的信息。 因为此处提到了Q、K、V这三个术语，我们先用文本翻译为例来做初步介绍，后续章节会对Q、K、V进行深入分析。从上文的分析可知，序列中的每个单词都需要了解序列中其它单词的信息，从而才能确定彼此间的相互关系。因此每个单词都要去询问其它单词：咱俩之间是否密切？其它单词回复关系密切与否。拿到相互关系后，每个单词会据此把其它单词的信息吸收进来做信息融合。上述操作其实是一种搜索+合并的运算，我们需要找到一个适合的机制将这种运算进行落地。而上图注意力模型中，有两个输入：q（正在处理的序列）和F（被关注的序列），F又分别转换为K和V，这三个变量综合起来使用就可以满足我们的需求。 从词典的角度来看也许可以促进理解。query是你要找的内容，key是字典的索引（字典里面有什么样的信息），value是对应的信息。普通的字典查找是精确匹配，即依据匹配的键来返回其对应的值。而注意力机制是向量化+模糊匹配+信息合并。注意力机制不仅查找最佳匹配，还要依据匹配程度做加权求和。源序列每个元素转化为 对，这就构成了源序列的字典。目标序列每个元素提出了query，这就是要查询的内容。在查找中，目标序列中每个元素会用自己的query去和目标序列每个元素的key计算得到对齐系数。这个对齐系数就是元素之间的相似度或者相关性。query和key越相似就代表value对query的影响力越大，query越需要吸收value的信息。随后query会根据两个词之间的亲密关系来决定从V中提取出多少信息出来融入到自身。 通过 query、key、value 这三个向量的相互作用，模型得以衡量每个单词对其他单词的关注度。最终，源序列的每个元素会把自己融合其它单词提供的信息之后得到的真实数据放到一个向量中。 我们假设源序列和目标序列为同一个序列，下图中给出了序列中”一个“和其它单词之间的相似度。虚线代表key与query的相关度，线条的粗细分布就叫做”注意力分布“。或者说，线条的粗细就是权重，线条越粗，表示key与query越相关，对理解query越重要，value的权重就越大。 我们首先看看总体思路。自注意力层是一种和循环层和卷积层等效的计算单元。它们的目的都是把一个向量序列映射成另一个向量序列，比如说编码器把x映射成中间表示z。我们先回想翻译场景。如果是RNN方案，则最终编码器会生成一个隐向量，然后把这个隐向量传递给解码器进行解码。前面已经分析了这种方案的弊端，比如隐向量是固定的。为了克服这种弊端，我们应该在每一个时间步t都会产生一个隐向量\(h_t\)，把这些\(h_t\) 保存起来。当产生新输出时，我们让模型回顾之前保存的所有隐状态，发现隐状态中的关键信息就用上，这样就摆脱了RNN中隐向量是固定长度的弊端。但是如何判断某个隐状态对当前生成词是否重要？这就需要模型采用某种机制进行学习，从而才能了解需要给这个隐状态多少注意力。总而言之，注意力机制的任务应该是找到解码器当前隐向量和编码器所有隐向量之间的相互关系。按照上述思路，注意力机制的计算总体可以分为两步： 得到的注意力权重为 $$w_{3,1},w_{3,2},w_{3,3},w_{3,4}=softmax(g(Y2, C1, C2, C3, C4))$$。 当模型需要决定给予序列中某个单词以多大的“注意力”时，它会计算该单词与其他单词之间的注意力分数。注意力分数是衡量序列中不同单词对当前单词的重要性程度的指标，或者说是目标单词和输入中某单词对齐的可能性大小。可能性越大就应该赋予更大的权重。大权重代表在生成输出时，当下预测单词应该更加关注源文本中其对应单词的信息。 注意力分数通过相似度计算函数得到。该函数一般接受key和value向量作为输入，输出key和value向量之间的相关性，即注意力分数。下图提供了这些函数的概述，其中\(q\)是查询向量，\(k_l\)是矩阵\(K\)的第\(l\)列。如果对应到seq2seq，则q可以认为是解码器输出的隐向量，k可以认为是编码器内部的隐向量。现实中，相似度计算函数是通过矩阵形式来计算，而非单独计算某一列。 得到注意力分数之后，模型会使用softmax操作将注意力分数进行归一化得到注意力权重，这样可以使得所有权重总和为1，确保所有源元素贡献的特征总量保持一定，也可以更加突出重要的权重。 得到注意力权重之后，每个query都能从对应的key上获取相应的信息，此时，就需要由输出函数（Output Function）来将各个部分组合成最终的高级特征向量进行输出。在本阶段，注意力机制会以加权求和的模式对数据进行加工。这意味着每个单词在新的表示中不只是自身的信息，还包含了其他单词的信息，这帮助模型捕捉输入序列中的依赖关系。 回忆下之前提到的RNN方案和CNN方案遇到的问题：对齐问题、信息遗失问题和长依赖问题。以及为了从根本上解决这些问题，我们提出的几种改进思路： 我们接下来就以自注意力机制为例，看看其如何通过自己的优点来解决之前提到的问题，顺便也和这两个方案做下对比分析。 与RNN、CNN方案相比，自注意力机制可以增大信息含量，从而有效的解决RNN中的信息遗失问题。从某种程度上来看，所有的序列建模都在做如下操作： RNN方案中，解码器把过去所有的上下文信息压缩到一个固定大小低维向量（隐状态）中。解码器的不同阶段都使用这个隐状态。该方案的优势是在长上下文中的线性（相对于二次）复杂性。但是在长上下文时，RNN受限于固定大小的隐状态（表达能力有限），很难利用额外的条件信息。 TTT（Test-Time Training）则把上下文压缩到模型的权重中，其优势是既能在时间上保持固定大小，又能大大增强表达能力。 自注意力机制使用一个列表（从后续文章中我们会知道这其实是KV Cache）作为隐状态。所有的上下文都存储在列表中，没有被压缩。列表中所有的上下文共同构成了统一的隐状态（每个阶段的隐状态是列表的一项），这样编码器可以将更多数据传递给解码器。 因此，我们可以看到自注意力机制（Transformer）的优势：不只像RNN那样只传递一个编码器最终的隐状态，而是传入所有的隐藏状态（对应处理过的所有token）到解码器。这样新的token就可以和过去所有上下文进行交互。当然，随着上下文长度的增加，使用列表的成本也会越来越高，其处理时间随上下文长度急剧增长，列表的内存也急剧增加。 自注意力机制中，当某单词获取其它单词信息时，其它单词和它的位置距离都是固定常数。这样，两个词之间的紧密程度仅取决于它们的真实相关性，而非取决于距离。或者说对绝对位置不敏感，任意两个词都可以直接建模。该特点可以解决长距离依赖问题。我们具体分析下。 首先，CNN虽然可以借助增加卷积层数来扩大视野，融合信息，但是信息在过深的网络传播过程之中容易丢失，导致模型性能下降。自注意力机制摒弃了 CNN 的局部假设，把感知域（perceptive field）调整到了整个序列。RNN方案因为无法弥补长距离依赖问题会导致梯度消失和梯度爆炸问题。在处理任何一个单词时，自注意力可以注意到整个句子的所有单词，这样可以将序列中的任意两个位置之间的距离缩小为一个常量，能在常数时间内有效地捕捉到它们之间的依赖关系，直接建立联系，从而消除了距离概念。因为此特点，自注意力机制的信息传播损失小。下图为三个方案构建词语之间关系所需距离的对比。 另外，常数距离特点也让自注意力机制没有“有序假设”。让自注意力机制在处理每个位置时，并非像RNN那样更容易注意到靠后的内容，忽略靠前的输入。而是平等看待序列中每个单词的顺序，这样能够有效得考虑到输入序列中的其他位置，可以更好的将对其他单词的“理解”融入到当前处理的单词中，信息融合效率高。 加权求和可以分为加权和求和两方面来看，前者是对数据进行有差别的对待，而后者是做数据融合，综合在一起就是用提纲挈领（对于不同信息量的元素给予不同的关注度）的方式对数据进行加工。我们接下来从不同角度对加权求和与CNN、全连接层进行比对分析。 首先，注意力机制是动态产生权重。CNN或者全连接层的权重是静态权重，这些权重在训练时候固定下来，在推理时使用相同权重进行推理，并没有建立权重和输入实体本身的关系。注意力机制是动态权重，用输入的query、key通过相似度计算来得到注意力权重。注意力机制根据输入来计算出输出应该更关注哪些输入，权重的产生随着输入变化，这是一种自适应操作。 其次，从另一个角度来看，注意力机制是站在某个输入对象的立场上看待上下文和它之间的关系，并以此关系为权重来确定如何吸取其它对象的信息。因此，注意力机制是一种相对关系建模。而这种操作大多采用全局注意力以建模更广范围的依赖关系，是一种全局操作。 将注意力应用在序列转换的源序列和目标序列之间就是对齐机制。在机器翻译中，源序列中的每个单词都可能会对输出造成影响，不仅仅是待翻译词之前的输入，其后续的输入也可能对输出产生影响，因此需要结合源序列和目标序列进行综合考虑。比如下面中译英的例子，当翻译到”一个“时，我们无法确定英文应该翻译成”a“还是”an“，必须往后看到”苹果“才能确定，因此需要在翻译时把”a or an“的操作和”苹果“进行对齐。 注意力允许依据输入序列和输出序列的不同部分进行相关度计算，这样就可以决定在输出某个单词时，模型将重点关注输入数据的哪些部分。从而在输入序列和输出序列进行对齐，更准确地学习序列之间的关系，提高模型的泛化能力和性能。下图中虚线代表key与query的相关度，线条越粗，表示越相关。我们可以看出来，“一个”和“苹果”等词对“an”的编码更重要（或更相关或更相似），应该承担更多的对“an”的预测，需要赋予更多的权重。因此，在模型生成“an“的时候，不但要从”一个“提取语义，也要从”苹果“提取语义，才能判断出来应该是”an“还是”a“。最终”一个“和”苹果“共同决定了对应的英文单词应该是""an""。 编码器可以同时从左到右和从右到左读取输入序列，并将每个时间步的隐状态拼接起来作为输出。这样做的好处是，可以让编码器同时考虑输入序列中每个单词的前后上下文信息，从而生成更丰富和完整的表示。 综上所述，注意力机制确保每个解码步骤都由最相关的上下文片段提供信息，为长期存在的长距离依赖问题提供了稳健的解决方案，从而重新定义了序列建模的格局。而且，注意力机制的提出不仅是技术上的突破，也为机器学习领域开辟了新的视野，更反映了人类认知的深刻影响。正如 Andrej Karpathy 所评价的：“注意力是一种重大解锁，是神经网络架构设计的一次飞跃。” 注意力机制只是一种思想，可以用到很多任务上。我们接下来看看几个经典的使用注意力机制对Encoder-Decoder进行改进的案例，也梳理一下历史上的重要节点。用范伟的话说：我们要知道Transformer是怎么来的。当然，我们也希望可以看到Transformer是怎么没的，毕竟如果有新方案取代了Transformer，那就说明AI领域将迎来新的历史性突破。 我们首先用一个图来展示下注意力发展历史。大家可以看到， Transformer是建立在很多巨人肩膀之上的集大成者。 论文Recurrent Continuous Translation Models 被认为是神经网络机器翻译(NMT)的开篇之作，其特点如下： 论文""Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation""这篇是Bengio 团队的成果，其特点如下： 没有把编码器-解码器做成一个端到端的模型，只是把解码器的输出概率作为特征喂给了统计机器翻译（SMT）模型。 论文“Sequence to Sequence Learning with Neural Networks”也起到了重要作用，其特点如下： 论文“Neural Machine Translation by Learning to Jointly Align and Translate”提出的Bahdanau Attention是Attention的开创者之一。其特点如下： 作者在论文中使用注意力机制解决了如何建模“与距离无关的依赖”，如何把编码器产生的信息更有效的传递给解码器的问题。这样注意力机制就和RNN一起配合，解决了遗忘和对齐问题。 !Andrej Karpathy分享了Dzmitry Bahdanau 的私人邮件，从中我们可以看到，为了克服编码器和解码器之间的瓶颈，Dzmitry Bahdanau的思路发展历程。 可以看到，突破性的想法往往来自那些在实践中寻求解决问题的创新者，而非空想理论家。Dzmitry Bahdanau在邮件中说得非常好： 我的AI宏愿是启动更多像机器翻译项目那样的惊人应用项目。优秀的研发工作对基础技术的进步贡献远胜过那些我们常认为是“真正”AI研究的复杂理论。 论文“Effective Approaches to Attention-based Neural Machine Translation”基于Bahdanau Attention对注意力机制的计算方式进行多样性探索。 ResNet是何凯明大神的经典之作，其可以有效解决梯度消失/爆炸问题，让神经网络层数的瓶颈得以突破。后面有文章详细讲解，这里不再赘述。 之前的注意力机制都关注不同序列之间的注意力，即交叉注意力。而论文Long Short-Term Memory-Networks for Machine Reading在交叉注意力的基础上提出了self-attenton（自注意力机制），或者叫 intra-attention。作者认为我们在顺序地阅读每个单词的时候，每个单词和前面单词的关联度是不一样的，因此使用了注意力机制来完成这个分析工作。 论文的动机如下。传统的LSTM在处理输入时把句子视为单词序列，并递归的将每个词与之前的记忆结合起来，直至得到整个句子的语义表示。这种方案面临两个问题： 论文QKV-Attention: FRUSTRATINGLY SHORT ATTENTION SPANS IN NEURAL LANGUAGE MODELING这可能是最早提出QKV概念的论文。 作者认为目前注意力机制的隐向量承担了太多功能，这样会分身乏术，影响模型效果。因为同一个词，它在背景向量中的意义和它作为查询词时候代表的意义不同，所以要区分开来。因此该论文对每个时间步产生的输出向量进行分割成三个向量key, value, predict，各自有不同的分工。以下是修改逻辑和历程。 原始模型结构如下图所示，神经网络语言模型只有一个输出向量，同时用来计算注意力向量，编码历史上下文信息和预测下一个词的分布。 论文作者将原始模型的输出划分成两部分：key和value，用key来计算注意力向量，用value来编码下一个词的分布和上下文信息。 但是value仍然同时被用来编码上下文信息和下一个词的分布表示（既要对查询的结果负责，又要对聚合的结果负责），一心二用容易出错。因此作者做了进一步改进。 新模型结构如下图所示，作者对原始模型的输出做进一步划分为三部分：key，value和predict。用key来计算注意力向量，用value来编码上下文信息，用predict来编码下一个词的分布。 作者又继续进行改进，将原始模型的输出划分成N－1部分，用前N－1个时间步的输出向量的一部分来计算下一个词的分布表示。模型结构如下图所示。 论文Self Attention & MultiHead Attention : A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING 把注意力机制用到了文本表示学习中，让注意力机制完成多项工作。论文的注意力计算过程如下： 论文[Convolutional Sequence to Sequence Learning](Convolutional Sequence to Sequence Learning) 使用CNN模型结合注意力机制进行操作，从而解决RNN不能并行的问题。 与RNN相比，CNN存在一定的优势。一来CNN可以并行处理，因此训练速度快，二则RNN并不能很好地处理句子中的结构化信息。因此，作者使用CNN来做编码器和解码器，利用层次结构来捕获词之间远程的依赖关系，也可以更好地捕捉更复杂的关系。在解码过程中，每一个卷积层都会进行注意力操作，称为multi-step attention。 我们可以看到，在上述的演进过程中，Transformer所包括或者说所需要的组件一步步的被实现出来。然而上述注意力方案中依然有RNN、CNN的存在，这拖了后腿。比如RNN无法并行训练，不利于大规模快速训练和部署，也不利于整个算法领域发展。 因此，Transformer作者彻底摒弃了RNN和CNN，构建了一个全新的序列转换架构。Transformer整个网络结构完全是由注意力机制组成，通过把序列元素两两直接比较，Transformer可以学习到输入序列里所有词的相关性，一步到位捕捉到全局的联系。同时，因为Transformer不是按顺序分析，这样它可以并行操作，也使得它比 RNN 具有更高的计算效率和可扩展性。最终，Transformer就是这样一个融汇贯通的集大成者。 当然我们也要看到，RNN和CNN一直没有放弃努力，各自的发展也是有声有色。尤其在强化学习领域，RNN更是在发挥巨大的作用。我们也期望未来会有更多创新的模型和方法来让Transformer在强化学习领域发挥更大的作用。 A General Survey on Attention Mechanisms in Deep Learning Gianni Brauwers and Flavius Frasincar Andrej Karpathy 首次公开私人邮件：揭秘Transformer 注意力机制真相 AI寒武纪 Attention is All your Need Bahdanau 和 Luong Attention 直观介绍 Honoria [Convolutional Sequence to Sequence Learning](Convolutional Sequence to Sequence Learning) Effective Approaches to Attention-based Neural Machine Translation Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation Long Short-Term Memory-Networks for Machine Reading NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE QKV-Attention: FRUSTRATINGLY SHORT ATTENTION SPANS IN NEURAL LANGUAGE MODELING Recurrent Continuous Translation Models Recurrent Continuous Translation Models Self Attention & MultiHead Attention : A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING Self Attention & MultiHead Attention : A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING Self Attention 1: Long Short-Term Memory-Networks for Machine Reading seq2seq中的两种attention机制（图+公式） 胡文星 Sequence to Sequence Learning with Neural Networks Thang Luong's Thesis on Neural Machine Translation Minh-Thang Luong Transformer自下而上理解(4) Attention without RNN marsggbo 《FRUSTRATINGLY SHORT ATTENTION SPANS IN NEURAL LANGUAGE MODELING》阅读笔记 Simple 一文看懂 Bahdanau 和 Luong 两种 Attention 机制的区别 Flitter 注意力机制 OnlyInfo 理解Attention:从起源到MHA,MQA和GQA Linsight 看图学大模型：Transformers 的前生今世(上） 看图学 第四篇：一文搞懂Transformer架构的三种注意力机制 AIwithGary 综述：图像处理中的注意力机制 极市平台 非Transformer架构的进展与猜想 StormBlafe",https://www.cnblogs.com/rossiXYZ/p/18705809,2025-03-13 10:54:00
朝野布告的后院,未知作者,我们有个海外的项目，一共70个服务，前前后后花了超过一年时间完成了云服务迁移和架构调整。主要是架构调整了，原来的docker swarm托管服务，几台云服务器将n个服务堆在一起，只会对服务器资源做整体监控，服务器没事没人管单个服务的内存情况。新架构对单个服务改动不大，但外部相关架构则改头换面了，最大的改动是容器改为Kubernetes托管，放在AWS的EKS上。新架构在新的云服务上线后，暴露了很多内存问题。在此拿某个服务根据解决过程，给个排查思路，希望能给到大家一点启示。 服务为一个普通的ASP.NET Core gRPC工程，平常没什么流量。HPA设置的最大副本数为5，生产环境服务启动后，Pod内存达到或超过K8s内存请求值(512Mi)，自动触发扩展到5个实例，即副本数达到最大。这与QA环境表现并不一样，也就没有在测试阶段发现。需要想办法复现、排查解决高副本和高内存占用问题。 部署里面对容器的资源，容器资源只做了对CPU和内存的预留设置(request)，没有做上限设置(llimit)。从内存曲线上看，很多副本的内存甚至超过了请求的512Mi。不过有一点很奇怪，服务的接口并没有出现性能下降的迹象。 尝试在QA环境对相关接口进行压测，问题能复现，表现为HPA副本扩展后各个POD的内存居高不下，维持在500~600Mi长时间不释放，有时候压测甚至能冲到800Mi。即使没有什么接口请求，也需要超过20个小时才缓慢下降到350Mi左右。但尝试本地VS诊断工具则并没有发现什么内存不释放问题，除了一些个大对象驻留问题。 因为其他类似的服务并不会这样，所以第一时间怀疑是代码问题，但这么想是错的，下面交代。怀疑代码问题后，想着是不是有什么内存泄漏，找了个服务的接口在QA压测后问题能复现（即内存长时间不释放）。看了好几遍代码，除了一些个ToList用的太过频繁并没什么问题(也与内存问题不相关)，用VS诊断工具检查内存有运行又没内存泄露问题。于是在QA环境用dotnet-dump把内存快照下载回来分析，找到了个大对象堆LOH驻留类型的类，而且VS诊断工具找到的类是同一个，接着定位到了接口调用这个类的地方。业务调用很单纯，这个类就从数据库用Dapper查出来得到列表，然后分组计算下数据，不会有什么内存泄露的机会；但注释掉此部分查询则内存不再上升到500~600Mi，只在300Mi左右，而且内存使用率下降也变快了。继续二分法+注释大法调试，最后只保留数据库查询语句而不做后续业务处理，连引用都不做，内存还是会达到500~600Mi。这就让人摸不着头脑了，代码肯定是相关的，数据库查询几下一列表数据都能让内存达到预留临界值(request)，列表也才约11000条数据，虽然确实是LOH对象，但不至于造成这么严重的内存不释放现象。 添加环境变量DOTNET_GCHeapHardLimit=0x20000000 # 512Mi的十六进制值，能限制内存的使用，但并不能让GC能敏感地进行回收，方案失败 原默认为Server GC，指定为Workstation GC后，内存占用不到180Mi，扩容缩容正常，这方案看起来也可以 调试结束，方案一和方案三似乎可行，查了相关资料后，两个方案其实都有问题。方案一是代码主动强制执行了垃圾回收，但大多数情况下并不被建议在代码里面去执行，因为执行GC.Collect多多少少会影响服务性能，GC自己能感知内存使用率然后自动进行执行回收。至于方案三，不同的模式本来就对应着不同的服务场景，服务本身就是后端接口，切换为工作站模式也许可行，但ASP.NET Core默认就是Server GC，Server GC模式本身为了支持高性能并不会频繁执行垃圾回收(从.NET 9开始不一样，.NET 9的ASP.NET Core默认是第三种模式，.NET 8也支持这种模式，只不过不是默认的)。 查资料过程中才了解K8s的资源设置是有预留设置(request，又称请求设置)和上限设置(limit)，服务只设置了请求request部分，没有limit部分，那有没有可能是服务容器因为没有被设置内存limit，导致GC如脱缰野马般豪气地使用内存呢？那为啥内存不释放？就是Server GC感觉内存还是够用的，具体文章参考：工作站和服务器垃圾回收和动态适应应用程序大小 (DATAS)。先查询下可用内存吧，于是加个下面接口查询： 可用内存居然高达4Gi，真相很接近了。接着为服务设置内存limit为512Mi，再次查询得到可用内存为512Mi。没错！就是少设置了内存上限，没有这个，此时可用内存为节点内存（4GB）；加了limit重新压测，曲线： 服务内存曲线高居不下是因为容器没有被限制内存，K8s没有指定内存limit，可用内存就是节点/宿主机的物理内存，高达4GB。没有设置内存limit，但是设置了HPA，于是服务一启动经过一些时间内存超过HPA阈值造成副本数增加；GC默认是Server GC，其感知的内存足够所以不释放（包括小对象和大对象）。虽然主动调用GC.Collect则可以释放，但一般不会这样做，因为GC有自己的一套逻辑。限定内存为0.5Gi后，内存释放曲线正常，HPA扩缩正常，响应时间正常，问题得到解决，也能解释服务的接口并没有出现性能下降的的现象。 如果遇到类似内存居高不下问题，先确定.NET版本极其GC是Server GC还是Workstation GC。然后再确定其分配的可用内存是多少，K8s下要检查其资源limit有没有被设置。如果被设置之后依然有内存不释放/泄露问题，再怀疑代码问题。,https://www.cnblogs.com/huangsheng/p/18731382,2025-03-13 10:53:54
.NET研究社,未知作者,"目前国产化系统浪潮下，适配鸿蒙是中国软件大势所趋，.NET作为最适合开发客户端语言之一，适配鸿蒙系统(HarmonyOS Next)是目前.NET开发者最关心的事情。我目前业余时间正在移植Avalonia到HarmonyOS，去年在.NET Conf CN上分享过，目前又取得一点进展，所以本文把所有问题进行整合与大家进行分享。 自HarmonyOS 5.0.0(12)起，禁止匿名内存申请可执行权限，除系统内置的JavaScript引擎外，其他虚拟机不能使用Jit功能，所以无法将CoreCLR接入到鸿蒙系统中，而最新版的Mono虽然支持解释执行，但是由于性能问题也不会接入Mono到鸿蒙系统，最终只能选择接入NativeAOT运行时。 支撑鸿蒙可以接入NativeAOT的原理是鸿蒙系统兼容libc是musl的Linux系统的动态库(.so)。而.NET的RID支持linux-musl-arm64/linux-musl-x64，所以理论上可以将.NET程序编译为原生的Linux动态库(.so)，然后在鸿蒙的原生项目中，通过dlopen以及dlsym等函数调用C#中的入口函数。 而C#调用鸿蒙api则通过P/Invoke调用鸿蒙的NDK，而ArkUI的TypeScript api则通过NDK中的napi调用。 具体做法可以参考我正在做的Avalonia移植项目: https://github.com/OpenHarmony-NET/OpenHarmony.Avalonia 鸿蒙系统使用了seccomp限制危险的syscall调用。标准posix下，如果系统不支持某个syscall则返回错误码，而seccomp非常激进，如果调用了非法的sycall则直接杀掉进程。.NET的运行时初始化时，会调用__NR_get_mempolicy系统调用对numa支持进行检查，而这个系统调用不在鸿蒙的seccomp白名单中，所以导致直接宕机。 鸿蒙系统中seccomp的系统调用白名单如下：https://gitee.com/openharmony/startup_init/blob/master/services/modules/seccomp/seccomp_policy/app.seccomp.policy 其实安卓中也有类似的限制，.NET的NativeAOT之所以能在安卓平台下运行是因为.NET中对安卓进行了特殊处理，而在鸿蒙平台我们使用的是Linux平台的代码，所以没有对这些系统调用进行处理。 解决上个问题后，.NET运行时初始化依然不能成功，导致程序崩溃，经过排查发现是GC初始化时会申请256G左右的虚拟内存，导致mmap返回Out Of Memory错误。 解决办法1：设置环境变量“DOTNET_GCHeapHardLimit”，将虚拟内存申请控制在约180G以下即可。 解决方案1：从Alpine上偷包 ，因为Alpine的libc是musl，所以理论上Alpine的库在鸿蒙上大部分都能使用。 阿里云Alpine软件包镜像地址: arm64架构：https://mirrors.aliyun.com/alpine/edge/main/aarch64/ amd64架构：https://mirrors.aliyun.com/alpine/edge/main/x86_64/ 鸿蒙的ICU配置文件路径与默认路径不同，需要调用修改环境变量API，将ICU_DATA修改为/system/usr/ohos_icu 且鸿蒙平台上libICU的大版本是72，要使用这个版本的库。 NativeAOT众所周知不支持跨平台编译，而我的方案需要发布到linux-musl平台，所以无法在Windows上发布，影响开发效率。 解决方案：在项目中引入项目https://github.com/OpenHarmony-NET/PublishAotCross Marshal.GetDelegateForFunctionPointer的实现依赖动态生成汇编，而HarmonyOS不支持动态生成汇编代码执行(Jit)，使用该函数会导致崩溃。 前文中提到部分问题的解决方案是修改源码，具体操作步骤如下： 修改完代码，执行以下命令进行编译(linux平台下，需要有编译环境)： 编译成功后，打开目录 运行时/artifacts/bin/coreclr/linux.arm64.Release/aotsdk，将这里所有的替换到自己电脑nuget的缓存目录, 例如C:\Users\用户名\.nuget\packages\runtime.linux-musl-arm64.microsoft.dotnet.ilcompiler\dotnet版本\sdk https://github.com/dotnet/runtime/issues/110074 https://github.com/dotnet/runtime/issues/111649",https://www.cnblogs.com/CeSun/p/18706813,2025-03-13 10:53:58
软件开发众包平台，园子商业化的未来,未知作者,众包平台是园子正在探索的与开发者共赢的商业模式，目前一边在召集早期合作开发者，一边在基于企业微信简单运营。 如果您有软件开发外包需求，如果您想找高手帮您解决棘手的软件开发问题，欢迎加企业微信联系我们发单，加好友时请备注【众包需求方】。 如果您想成为众包平台的合作开发者，欢迎加企业微信申请，加好友时请备注【擅长的技术栈与业务方向-工作年限-所在城市】，至少需要3年工作经验。,https://www.cnblogs.com/cmt/p/18500368,2025-03-13 10:53:46
.NET研究社,未知作者,"目前国产化系统浪潮下，适配鸿蒙是中国软件大势所趋，.NET作为最适合开发客户端语言之一，适配鸿蒙系统(HarmonyOS Next)是目前.NET开发者最关心的事情。我目前业余时间正在移植Avalonia到HarmonyOS，去年在.NET Conf CN上分享过，目前又取得一点进展，所以本文把所有问题进行整合与大家进行分享。 自HarmonyOS 5.0.0(12)起，禁止匿名内存申请可执行权限，除系统内置的JavaScript引擎外，其他虚拟机不能使用Jit功能，所以无法将CoreCLR接入到鸿蒙系统中，而最新版的Mono虽然支持解释执行，但是由于性能问题也不会接入Mono到鸿蒙系统，最终只能选择接入NativeAOT运行时。 支撑鸿蒙可以接入NativeAOT的原理是鸿蒙系统兼容libc是musl的Linux系统的动态库(.so)。而.NET的RID支持linux-musl-arm64/linux-musl-x64，所以理论上可以将.NET程序编译为原生的Linux动态库(.so)，然后在鸿蒙的原生项目中，通过dlopen以及dlsym等函数调用C#中的入口函数。 而C#调用鸿蒙api则通过P/Invoke调用鸿蒙的NDK，而ArkUI的TypeScript api则通过NDK中的napi调用。 具体做法可以参考我正在做的Avalonia移植项目: https://github.com/OpenHarmony-NET/OpenHarmony.Avalonia 鸿蒙系统使用了seccomp限制危险的syscall调用。标准posix下，如果系统不支持某个syscall则返回错误码，而seccomp非常激进，如果调用了非法的sycall则直接杀掉进程。.NET的运行时初始化时，会调用__NR_get_mempolicy系统调用对numa支持进行检查，而这个系统调用不在鸿蒙的seccomp白名单中，所以导致直接宕机。 鸿蒙系统中seccomp的系统调用白名单如下：https://gitee.com/openharmony/startup_init/blob/master/services/modules/seccomp/seccomp_policy/app.seccomp.policy 其实安卓中也有类似的限制，.NET的NativeAOT之所以能在安卓平台下运行是因为.NET中对安卓进行了特殊处理，而在鸿蒙平台我们使用的是Linux平台的代码，所以没有对这些系统调用进行处理。 解决上个问题后，.NET运行时初始化依然不能成功，导致程序崩溃，经过排查发现是GC初始化时会申请256G左右的虚拟内存，导致mmap返回Out Of Memory错误。 解决办法1：设置环境变量“DOTNET_GCHeapHardLimit”，将虚拟内存申请控制在约180G以下即可。 解决方案1：从Alpine上偷包 ，因为Alpine的libc是musl，所以理论上Alpine的库在鸿蒙上大部分都能使用。 阿里云Alpine软件包镜像地址: arm64架构：https://mirrors.aliyun.com/alpine/edge/main/aarch64/ amd64架构：https://mirrors.aliyun.com/alpine/edge/main/x86_64/ 鸿蒙的ICU配置文件路径与默认路径不同，需要调用修改环境变量API，将ICU_DATA修改为/system/usr/ohos_icu 且鸿蒙平台上libICU的大版本是72，要使用这个版本的库。 NativeAOT众所周知不支持跨平台编译，而我的方案需要发布到linux-musl平台，所以无法在Windows上发布，影响开发效率。 解决方案：在项目中引入项目https://github.com/OpenHarmony-NET/PublishAotCross Marshal.GetDelegateForFunctionPointer的实现依赖动态生成汇编，而HarmonyOS不支持动态生成汇编代码执行(Jit)，使用该函数会导致崩溃。 前文中提到部分问题的解决方案是修改源码，具体操作步骤如下： 修改完代码，执行以下命令进行编译(linux平台下，需要有编译环境)： 编译成功后，打开目录 运行时/artifacts/bin/coreclr/linux.arm64.Release/aotsdk，将这里所有的替换到自己电脑nuget的缓存目录, 例如C:\Users\用户名\.nuget\packages\runtime.linux-musl-arm64.microsoft.dotnet.ilcompiler\dotnet版本\sdk https://github.com/dotnet/runtime/issues/110074 https://github.com/dotnet/runtime/issues/111649",https://www.cnblogs.com/CeSun/p/18706813,2025-03-13 10:54:40
liyongqiang-cc,未知作者,在构建高性能的 .NET Core 应用时，缓存是提升系统响应速度、减轻数据库压力的利器。然而，缓存并非一蹴而就，它也需要“热身”才能发挥最佳性能。这就是缓存预热的意义所在。 缓存预热，顾名思义，就是在系统启动或缓存失效后，主动将热点数据加载到缓存中，避免用户首次请求时直接访问数据库，造成性能瓶颈。 .NET Core 提供了多种缓存机制，例如内存缓存、分布式缓存等。我们可以根据不同的缓存类型，选择不同的预热方式。 缓存预热是提升 .NET Core 应用性能的重要手段。通过合理的预热策略，可以有效提升用户体验、降低数据库压力、提高系统稳定性。希望本文能帮助你更好地理解和应用缓存预热技术。,https://www.cnblogs.com/liyongqiang-cc/p/18737994,2025-03-13 10:53:49
老周,未知作者,"由于两个月的奋战，导致很久没更新了。就是上回老周说的那个产线和机械手搬货的项目，好不容易等到工厂放假了，我就偷偷乐了。当然也过年了，老周先给大伙伴们拜年了，P话不多讲，就祝大家身体健康、生活愉快。其实生活和健康是密不可分的，想活得好，就得健康。包括身体健康、思想健康、心理健康、精神健康。不能以为我无病无痛就很健康，你起码要全方位健康。 不管你的工作是什么，忙或者不忙，报酬高或低，但是，人，总得活，总得过日子。咱们最好多给自己点福利，多整点可以自娱自乐的东西，这就是生活。下棋、打游戏、绘画、书法、钓鱼、飙车、唢呐……不管玩点啥，只要积极正向的就好，可以大大降低得抑郁症、高血压的机率；可以减少70%无意义的烦恼；可以降低跳楼风险；在这个礼崩乐坏的社会环境中，可以抵御精神污染……总之，益处是大大的有。 然后老周再说一件事，一月份的时候常去工厂调试，也认识了机械臂厂商派的技术支持——吴大工程师。由于工厂所处地段非常繁华，因此每次出差，午饭只能在附近一家四川小吃店解决。毕竟这方圆百十里也仅此一家。不去那里吃饭除非自带面包蹲马路边啃，工厂不供食也不供午休场所。刚开始几次出差还真的像个傻子似的蹲马路边午休。后来去多了，直接钻进工厂的会议室睡午觉。 4、做过许多项目的一线开发者。我强调的项目数量多，而不是长年只维护一个项目的。只有数量多你学到的才多； 5、社区贡献较多者，这个和3差不多。不过，老周认为的社区贡献就是不仅提供代码，还提供文档、思路、技巧等； ========================================================================================================== 下面进入主题，咱们今天聊聊 IChangeToken。它的主要功能是提供更改通知。比如你的配置源发生改变了，要通知配置的使用者重新加载。你可能会疑惑，这货跟使用事件有啥区别？这个老周也不好下结论，应该是为异步代码准备的吧。 这个 Change Token 思路很清奇，实际功能类似事件，就是更改通知。咱们可以了解一下其原理，但如果你觉得太绕，不想了解也没关系的。在自定义配置源时，咱们是不需要自己写 Change Token 的，框架已有现成的。我们只要知道要触发更改通知时调用相关成员就行。 1、runtime-main\src\libraries\Common\src\Extensions\ChangeCallbackRegistrar.cs：这个主要是 UnsafeRegisterChangeCallback 方法，用于注册回调委托； 2、runtime-main\src\libraries\Microsoft.Extensions.Primitives\src\ChangeToken.cs：这个类主要是提供静态的辅助方法，用于注册回调委托。它的好处是可以循环——注册回调后，触发后委托被调用；调用完又自动重新注册，使得 Change Token 可以多次触发； 3、runtime-main\src\libraries\Microsoft.Extensions.Primitives\src\CancellationChangeToken.cs：这个类是真正实现 IChangeToken 接口的； 4、runtime-main\src\libraries\Microsoft.Extensions.Configuration\src\ConfigurationReloadToken.cs：这个也是实现 IChangeToken 接口，而且它才是咱们今天的主角，该类就是为重新加载配置数据而提供的。调用它的 OnReload 方法可以触发更改通知。 CancellationTokenSource 类表示一个取消任务的标记，访问它的 Token 属性可以获得一个 CancellationToken 结构体实例，可以检索它的 IsCancellationRequested 属性以明确是否有取消请求（有则true，无则false）。 还有更重要的，CancellationToken 结构体的 Register 方法可以注册一个委托作为回调，当收到取消请求后会触发这个委托。对的，这个就是 Change Token 灵魂所在了。一旦回调被触发后，CancellationTokenSource 就处于取消状态了，你无法再次触发，除非重置或重新实例化。这就是回调只能触发一次的原因。 下面，咱们完成一个简单的演示——用数据库做配置源。在 SQL Server 里面随便建个数据库，然后添加一个表，名为 tb_configdata。它有四个字段： ID和config_key设为主键，config_value 是配置的值，remark 是备注。备注字段其实可以不用，但实际应用的时候，可以用来给配置项写点注释。 然后，在程序里面咱们用到 EF Core，故要先生成与表对应的实体类。这里老周就不用工具了，直接手写更有效率。 上述代码的情况特殊，实体类的名称和成员名称与数据表并不一致，所以在重写 OnModelCreating 方法时，需要进行映射。 1、ToTable(""tb_configdata"") 告诉 EF 实体类对应的数据表是 tb_configdata； 2、HasKey(c => new { c.ID, c.ConfigKey })：表明该实体有两个主键——ID和ConfigKey。这里指定的是实体类的属性，而不是数据表的字段名，因为后面咱们会进行列映射； 3、HasColumnName(""config_key"")：告诉 EF，实体的 ConfigKey 属性对应的是数据表中 config_key。后面的几个属性的道理一样，都是列映射。 做映射就类似于填坑，如果你不想挖坑，那就直接让实体类名与表名一样，属性名与表字段（列）一样，这样就省事多了。不过，在实际使用中真没有那么美好。很多时候数据库是小李负责的，人家早就建好了，存储过程都写了几万个了。后面前台程序是老张来开发，对老张来说，要么把实体的命名与数据库的一致，要么就做一下映射。多数情况下是要映射的，毕竟很多时候数据库对象的命名都比较奇葩。尤其有上千个表的时候，为了看得顺眼，很多人喜欢这样给数据表命名：ta_XXX、ta_YYY、tb_ZZZ、tc_FFF、tx_PPP、ty_EEE、tz_WWW。还有这样命名的：m1_Report、m2_ReportDetails…… m105_TMD、m106_WNM、m107_DOUBI。 此处咱们不用直接实现 IConfigurationProvider 接口，而是从 ConfigurationProvider 类派生就行了。自定义配置源的东东老周以前写过，只是当时没有实现更改通知。 由于老周不知道怎么监控数据库更新，最简单的办法就是用定时器循环检查。重点是重写 Load 方法，完成加载配置的逻辑。Load 方法覆写后不需要调用 base 的 Load 方法，因为基类的方法是空的，调用了也没毛用。 在 Timer 对象调用的方法（OnTimer）中，先调用 Load 方法，再调用 OnReload 方法。这样就可以在加载数据后触发更改通知。 然后实现 IConfigurationSource 接口，提供 MyConfigurationProvider 实例。 默认的配置源有JSON文件、命令行、环境变量等，为了排除干扰，便于查看效果，在 Main 方法中咱们先把配置源列表清空，再添加咱们自定义的配置源。 这里你可能会有个疑问：连接字符串硬编码了不太好，要不写在配置文件中，可是，写在JSON文件中咱们怎么获取呢？毕竟 ConfigurationProvider 不使用依赖注入。 IConfigurationSource 不是有个 Build 方法吗？Build 方法不是有个参数是 IConfigurationBuilder 吗？用它，用它，狠狠地用它。 ----------------------------------------------------------------------------------------------------- 接下来，咱们弄个一对多的例子。逻辑是这样的：启动程序显示主窗口，接着创建五个子窗口。主窗口上有个大大的按钮，点击后，五个子窗口会收到通知。大概就这个样子： 窗口上只放了一个文本框。上面代码中，使用了 ChangeToken.OnChange 静态方法，为 Change Token 注册回调委托，本例中回调委托绑定的是 OnCallback 方法，也就是说：当 Change Token 触发后会在文本框中追加文本。OnChange 静态方法有两个重载： 上述例子用的是第一个，其实里面调用的也是第二个重载，只是把咱们传递的 OnCallback 方法当作 TState 传进去了。 请大伙伴暂时记住 changeTokenProducer 和 changeTokenConsumer 这两参数。changeTokenProducer 也是一个委托，返回 IChangeToken。用的时候一定要注意，每次触发之前，Change Token 要先创建新实例。注意是先创建新实例再触发，否则会导致无限。尽管内部会判断 HasChanged 属性，可问题是这个判断是在注册回调之后的。这个是跟 Change Token 的清奇逻辑有关，咱们看看 OnChage 的源代码就明白了。 简单来说，就是返回一个 ChangeTokenRegistration 实例，这是个私有类，咱们是访问不到的，以 IDisposable 接口公开。其中，它有两个方法是递归调用的： 在 ChangeTokenRegistration 类的构造函数中，先调用 RegisterChangeTokenCallback 方法，开始了整个递归套娃的过程。在 RegisterChangeTokenCallback 方法中，为 token 注册的回调就是调用 OnChangeTokenFired 方法。 而 OnChangeTokenFired 方法中，是先获取新的 Change Token，再触发旧 token。最后，又调用 RegisterChangeTokenCallback 方法，实现了无限套娃的逻辑。 因此，咱们在用的时候，必须先创建新的 Change Token 实例，然后再调用 RegisterChangeTokenCallback 实例的 Cancel 方法。不然这无限套娃会一直进行到栈溢出，除非你提前把 ChangeTokenRegistration 实例 Dispose 掉（由 OnChange 静态方法返回）。可是那样的话，你就不能多次接收更改了。 下面就是主窗口部分，也是最危险的部分——必须按照咱们上面分析的顺序进行，不然会 Stack Overflow。 按钮1的单击事件处理方法就是触发点，所以，CancellationTokenSource、CancellationChangeToken 要先换成新的实例，然后再用旧的实例去 Cancel。这里用 Interlocked 类会好一些，毕竟要考虑异步的情况，虽然咱这里都是在UI线程上传递的，但还是遵守这个习惯好一些。 这样处理就能避免栈溢出了。运行后，先打开五个子窗口（多点击一次就能创建十个子窗口）。接着点击大大按钮，五个子窗口就能收到通知了。",https://www.cnblogs.com/tcjiaan/p/18012397,2025-03-13 10:56:52
老周,未知作者,"由于两个月的奋战，导致很久没更新了。就是上回老周说的那个产线和机械手搬货的项目，好不容易等到工厂放假了，我就偷偷乐了。当然也过年了，老周先给大伙伴们拜年了，P话不多讲，就祝大家身体健康、生活愉快。其实生活和健康是密不可分的，想活得好，就得健康。包括身体健康、思想健康、心理健康、精神健康。不能以为我无病无痛就很健康，你起码要全方位健康。 不管你的工作是什么，忙或者不忙，报酬高或低，但是，人，总得活，总得过日子。咱们最好多给自己点福利，多整点可以自娱自乐的东西，这就是生活。下棋、打游戏、绘画、书法、钓鱼、飙车、唢呐……不管玩点啥，只要积极正向的就好，可以大大降低得抑郁症、高血压的机率；可以减少70%无意义的烦恼；可以降低跳楼风险；在这个礼崩乐坏的社会环境中，可以抵御精神污染……总之，益处是大大的有。 然后老周再说一件事，一月份的时候常去工厂调试，也认识了机械臂厂商派的技术支持——吴大工程师。由于工厂所处地段非常繁华，因此每次出差，午饭只能在附近一家四川小吃店解决。毕竟这方圆百十里也仅此一家。不去那里吃饭除非自带面包蹲马路边啃，工厂不供食也不供午休场所。刚开始几次出差还真的像个傻子似的蹲马路边午休。后来去多了，直接钻进工厂的会议室睡午觉。 4、做过许多项目的一线开发者。我强调的项目数量多，而不是长年只维护一个项目的。只有数量多你学到的才多； 5、社区贡献较多者，这个和3差不多。不过，老周认为的社区贡献就是不仅提供代码，还提供文档、思路、技巧等； ========================================================================================================== 下面进入主题，咱们今天聊聊 IChangeToken。它的主要功能是提供更改通知。比如你的配置源发生改变了，要通知配置的使用者重新加载。你可能会疑惑，这货跟使用事件有啥区别？这个老周也不好下结论，应该是为异步代码准备的吧。 这个 Change Token 思路很清奇，实际功能类似事件，就是更改通知。咱们可以了解一下其原理，但如果你觉得太绕，不想了解也没关系的。在自定义配置源时，咱们是不需要自己写 Change Token 的，框架已有现成的。我们只要知道要触发更改通知时调用相关成员就行。 1、runtime-main\src\libraries\Common\src\Extensions\ChangeCallbackRegistrar.cs：这个主要是 UnsafeRegisterChangeCallback 方法，用于注册回调委托； 2、runtime-main\src\libraries\Microsoft.Extensions.Primitives\src\ChangeToken.cs：这个类主要是提供静态的辅助方法，用于注册回调委托。它的好处是可以循环——注册回调后，触发后委托被调用；调用完又自动重新注册，使得 Change Token 可以多次触发； 3、runtime-main\src\libraries\Microsoft.Extensions.Primitives\src\CancellationChangeToken.cs：这个类是真正实现 IChangeToken 接口的； 4、runtime-main\src\libraries\Microsoft.Extensions.Configuration\src\ConfigurationReloadToken.cs：这个也是实现 IChangeToken 接口，而且它才是咱们今天的主角，该类就是为重新加载配置数据而提供的。调用它的 OnReload 方法可以触发更改通知。 CancellationTokenSource 类表示一个取消任务的标记，访问它的 Token 属性可以获得一个 CancellationToken 结构体实例，可以检索它的 IsCancellationRequested 属性以明确是否有取消请求（有则true，无则false）。 还有更重要的，CancellationToken 结构体的 Register 方法可以注册一个委托作为回调，当收到取消请求后会触发这个委托。对的，这个就是 Change Token 灵魂所在了。一旦回调被触发后，CancellationTokenSource 就处于取消状态了，你无法再次触发，除非重置或重新实例化。这就是回调只能触发一次的原因。 下面，咱们完成一个简单的演示——用数据库做配置源。在 SQL Server 里面随便建个数据库，然后添加一个表，名为 tb_configdata。它有四个字段： ID和config_key设为主键，config_value 是配置的值，remark 是备注。备注字段其实可以不用，但实际应用的时候，可以用来给配置项写点注释。 然后，在程序里面咱们用到 EF Core，故要先生成与表对应的实体类。这里老周就不用工具了，直接手写更有效率。 上述代码的情况特殊，实体类的名称和成员名称与数据表并不一致，所以在重写 OnModelCreating 方法时，需要进行映射。 1、ToTable(""tb_configdata"") 告诉 EF 实体类对应的数据表是 tb_configdata； 2、HasKey(c => new { c.ID, c.ConfigKey })：表明该实体有两个主键——ID和ConfigKey。这里指定的是实体类的属性，而不是数据表的字段名，因为后面咱们会进行列映射； 3、HasColumnName(""config_key"")：告诉 EF，实体的 ConfigKey 属性对应的是数据表中 config_key。后面的几个属性的道理一样，都是列映射。 做映射就类似于填坑，如果你不想挖坑，那就直接让实体类名与表名一样，属性名与表字段（列）一样，这样就省事多了。不过，在实际使用中真没有那么美好。很多时候数据库是小李负责的，人家早就建好了，存储过程都写了几万个了。后面前台程序是老张来开发，对老张来说，要么把实体的命名与数据库的一致，要么就做一下映射。多数情况下是要映射的，毕竟很多时候数据库对象的命名都比较奇葩。尤其有上千个表的时候，为了看得顺眼，很多人喜欢这样给数据表命名：ta_XXX、ta_YYY、tb_ZZZ、tc_FFF、tx_PPP、ty_EEE、tz_WWW。还有这样命名的：m1_Report、m2_ReportDetails…… m105_TMD、m106_WNM、m107_DOUBI。 此处咱们不用直接实现 IConfigurationProvider 接口，而是从 ConfigurationProvider 类派生就行了。自定义配置源的东东老周以前写过，只是当时没有实现更改通知。 由于老周不知道怎么监控数据库更新，最简单的办法就是用定时器循环检查。重点是重写 Load 方法，完成加载配置的逻辑。Load 方法覆写后不需要调用 base 的 Load 方法，因为基类的方法是空的，调用了也没毛用。 在 Timer 对象调用的方法（OnTimer）中，先调用 Load 方法，再调用 OnReload 方法。这样就可以在加载数据后触发更改通知。 然后实现 IConfigurationSource 接口，提供 MyConfigurationProvider 实例。 默认的配置源有JSON文件、命令行、环境变量等，为了排除干扰，便于查看效果，在 Main 方法中咱们先把配置源列表清空，再添加咱们自定义的配置源。 这里你可能会有个疑问：连接字符串硬编码了不太好，要不写在配置文件中，可是，写在JSON文件中咱们怎么获取呢？毕竟 ConfigurationProvider 不使用依赖注入。 IConfigurationSource 不是有个 Build 方法吗？Build 方法不是有个参数是 IConfigurationBuilder 吗？用它，用它，狠狠地用它。 ----------------------------------------------------------------------------------------------------- 接下来，咱们弄个一对多的例子。逻辑是这样的：启动程序显示主窗口，接着创建五个子窗口。主窗口上有个大大的按钮，点击后，五个子窗口会收到通知。大概就这个样子： 窗口上只放了一个文本框。上面代码中，使用了 ChangeToken.OnChange 静态方法，为 Change Token 注册回调委托，本例中回调委托绑定的是 OnCallback 方法，也就是说：当 Change Token 触发后会在文本框中追加文本。OnChange 静态方法有两个重载： 上述例子用的是第一个，其实里面调用的也是第二个重载，只是把咱们传递的 OnCallback 方法当作 TState 传进去了。 请大伙伴暂时记住 changeTokenProducer 和 changeTokenConsumer 这两参数。changeTokenProducer 也是一个委托，返回 IChangeToken。用的时候一定要注意，每次触发之前，Change Token 要先创建新实例。注意是先创建新实例再触发，否则会导致无限。尽管内部会判断 HasChanged 属性，可问题是这个判断是在注册回调之后的。这个是跟 Change Token 的清奇逻辑有关，咱们看看 OnChage 的源代码就明白了。 简单来说，就是返回一个 ChangeTokenRegistration 实例，这是个私有类，咱们是访问不到的，以 IDisposable 接口公开。其中，它有两个方法是递归调用的： 在 ChangeTokenRegistration 类的构造函数中，先调用 RegisterChangeTokenCallback 方法，开始了整个递归套娃的过程。在 RegisterChangeTokenCallback 方法中，为 token 注册的回调就是调用 OnChangeTokenFired 方法。 而 OnChangeTokenFired 方法中，是先获取新的 Change Token，再触发旧 token。最后，又调用 RegisterChangeTokenCallback 方法，实现了无限套娃的逻辑。 因此，咱们在用的时候，必须先创建新的 Change Token 实例，然后再调用 RegisterChangeTokenCallback 实例的 Cancel 方法。不然这无限套娃会一直进行到栈溢出，除非你提前把 ChangeTokenRegistration 实例 Dispose 掉（由 OnChange 静态方法返回）。可是那样的话，你就不能多次接收更改了。 下面就是主窗口部分，也是最危险的部分——必须按照咱们上面分析的顺序进行，不然会 Stack Overflow。 按钮1的单击事件处理方法就是触发点，所以，CancellationTokenSource、CancellationChangeToken 要先换成新的实例，然后再用旧的实例去 Cancel。这里用 Interlocked 类会好一些，毕竟要考虑异步的情况，虽然咱这里都是在UI线程上传递的，但还是遵守这个习惯好一些。 这样处理就能避免栈溢出了。运行后，先打开五个子窗口（多点击一次就能创建十个子窗口）。接着点击大大按钮，五个子窗口就能收到通知了。",https://www.cnblogs.com/tcjiaan/p/18012397,2025-03-13 10:56:53
CodeBlogMan,未知作者,写在前面的是关于自己的一点介绍，21年本科毕业，学的是计算机科学与技术专业。自己本身对编程和开发是有兴趣的，同时也对项目管理、团队建设方面有兴趣。 目前有两段工作经历，都是在一线当 Java 开发大头兵，但职级倒是逐渐有所上升。做过从 0-1 的 SaaS、PaaS 的自研项目，也做过基于多租户的标准化服务项目，还有过半年的 PM 的经历...这些都让我有了一些关于工作上的感悟。现在简单总结一下，拿出来和大家做分享交流。 下面主要从工作的方式方法、构建知识体系、尽早规划职业这三个大的方面展开。文章有点长，但都是自己一步步实践总结得来的，有写得不好的地方，还希望大家多包涵。 拿出积极的态度：首先需要自己努力去解决问题，其次包括调动资源、团队协助，最好要让领导/同事知晓你的情况。 摆烂并不可取，劳动是用来换取报酬的。如果平时的工作表现/绩效过不去，那么将可能会直接影响到我们的收入。 对于工作量的估算要留有余地。学会平衡别人的预期，如果 3 天内能完成，那最好回复一个星期：总有别的事情会打扰你。 遵守基本的职业道德：如果你负责的服务出现故障或发现一个严重的 bug ，作为开发我们应该马上对其进行修复/解决。 工作里对事不对人：我们需要推进的是项目，是一件件具体的事情，聚焦于如何解决问题，而不是和人做无效拉扯。 做好上面这些，假如是对于刚毕业的我来说可能并不容易，但过了两年自己开始负责项目了以后，我就能慢慢理解并适应了。 所以我们可以有意识地提炼日常工作成果，形成自己的个人源码库和工具库、解决某类问题的通用系统体系结构、甚至进化为框架。 众所周知对软件开发人员而言，有、无经验的一个显著区别是：新手无论完成什么开发任务都从头开始，而有经验的开发往往通过重组自己的可复用模块、类库来解决问题。 这并不是说所有可复用的东西都必须自己实现，别人成熟的、通过测试的成果也可以收集、整理到自己的知识库中。 实际程序员们可能很难做到：总有一些会议拉上你去 battle，总有一些线上的 bug 困扰着你，总会有领导安排的其它事情让你分身乏力...还有很多。工作是为了更好的生活这个主旨不会变，所以我也有自己的一些想法： 知识体系是系统化的知识，具有连续性的特点，反映了一个人经过学习后所拥有的知识构成情况与结合方式。如果一个开发人员在1、2年内都没有更新过自己的知识，那么他可能已经不再属于这个行业了。 抽象思维能够超越感官直接感知信息，通过对事物的本质属性和规律的把握，使人们在认识活动中获得更深层次的知识。 多阅读优秀的代码，多思考本质和原因，多通过实践来检验 写出可阅读的、简洁易懂、可维护的代码，应该是程序员在工作中都需要追求的。在实现业务的时候，通过借助模仿优秀的开源实现，学习他人在提高代码效率、逻辑思维、处理高并发等方面是怎么做的。 学会总结复盘，从中抽取有价值的经验，将成长记录下来 复盘的内容，可以是一次线上 bug 的处理过程，可以是一次代码重构优化的过程，也可以是对一个中间件的新认识等等。复盘不是问责也不是甩锅，而是聚焦于好和不好，知其然再知其所以然，同时避免重蹈覆辙。 不是所有人都愿意直面自己的缺点和不足，我自己最开始也不明白复盘有什么用处，觉得对领导和业务能有交代就行。但随着自己做过的项目多了起来，受到身边优秀同事的影响也在日益增多，我才明白复盘是可以让人迅速成长的不二法宝。 有意识地站在更高的角度去看待问题，可能会得到不一样的答案 作为一线开发我们更多关注的，可能是接口的参数和实现逻辑，数据库字段的设计，中间件的使用，异常的处理等实际编码问题； 而你的 TeamLeader 可能关注的是项目的高可用，处理高并发的性能，硬件资源的成本，服务挂了怎么降低对业务的影响等； 部门的总监则可能在考虑这个项目要不要做二期三期？对业务方有什么价值？能带来多少新用户？让谁来负责后续的运营等。 初/中级可以先从广度出发，多见识一些主流技术或者解决方案，这样在面对业务的时候可以更从容地进行设计，也能更清楚自己 CURD 的目的是什么。 高级/资深可能就要精通某一或者某几方面的通用架构/解决方案了，同时对于负责的业务有深刻的认识，可能也要负责团队的建设，从更广阔的视角去思考问题。 我是在工作一年半左右慢慢有了对未来进行职业规划的想法，因为时间会向前走，人也会渐渐地成长。对于未来想要什么，可以慢慢给自己多找找方向去尝试。 2010-2020 这十年，互联网发展如火如荼，各种公司如雨后春笋般冒出来，那时候招聘需求量大，互联网就业市场一片欣欣向荣。 而现在没有了当年的辉煌，很多互联网公司业务发展停滞，同时经过前面多年的发展，产品架构已经成熟稳定，不再需要这么多的程序员，势必要进行结构优化，那优化的对象自然是性价比低的人群。 其次，IT互联网行业高薪的宣传又吸引了一大批年轻人跑步进场，需求变少的情况下人又变多，内卷就这么产生了。高学历的人才多了很多，所谓学历贬值的论调在这几年一直都有出现。 让我们透过现象看本质：IT互联网行业的特殊性在于产出是可迭代的。这个行业程序员的产出就是软件&代码，而这个东西就是可以不断的迭代的。 分析了上面的原因，就有针对性了。软件代码这些东西是可迭代的，但有些东西是没法复制和迭代的：像是沟通协调能力、解决问题能力、规划组织能力、管理能力和人脉资源等等。这些其实就是我们经常说的软实力，这些能力才是越久越吃香。 这些能力不仅可以在日常工作中有意识的积累，也可以通过考取相关的证书来掌握理论知识，比如：软考高项（信息系统项目管理师、系统架构师）、PMP认证、阿里云/华为云高级认证等。 打开认知的局限，不只着眼于手里掌握的技术，拓展自己职业的综合能力，明白不是只有靠技术才能活下去，才能更好地“破局”。 文章最后，我还有一些想说的。的确，技术逐渐会变得越来越“廉价”，不再是必须要工作多年的大佬才能驾驭的高级东西。 这里面有个很重要的原因，就是开源的力量。大家可以想一想自己参与的一些项目，用到了多少开源的东西？以一个典型的后端项目为例，可能涉及到了 MySQL、Redis、ElasticSearch、RocketMQ、Nginx、SprintBoot、Jenkins、CI/CD、Docker...... 程序员要做的就是根据业务需求，把上面那一堆组件按照规则合理地堆在一起。一般就是 Nginx 负载均衡，Spring Boot/Cloud 写后端业务逻辑，Redis 做缓存，MySQL、ES、MongoDB 做数据存储，OSS/COS 做云对象存储，RocketMQ/Kafka 做消息队列，最后进行 CI/CD 操作塞到 Docker 里面部署运行。现在绝大多数的功能，开源组件都能提供，再加上现在是云的时代，各种服务都上云了，那么问题就更简单了。 既然我们身处在这个行业，那么就要做好持续学习的准备，不断打怪升级，锻炼综合能力，做到让经验随着年龄的增长成真正的正比。,https://www.cnblogs.com/CodeBlogMan/p/17983370,2025-03-13 10:56:53
CodeBlogMan,未知作者,写在前面的是关于自己的一点介绍，21年本科毕业，学的是计算机科学与技术专业。自己本身对编程和开发是有兴趣的，同时也对项目管理、团队建设方面有兴趣。 目前有两段工作经历，都是在一线当 Java 开发大头兵，但职级倒是逐渐有所上升。做过从 0-1 的 SaaS、PaaS 的自研项目，也做过基于多租户的标准化服务项目，还有过半年的 PM 的经历...这些都让我有了一些关于工作上的感悟。现在简单总结一下，拿出来和大家做分享交流。 下面主要从工作的方式方法、构建知识体系、尽早规划职业这三个大的方面展开。文章有点长，但都是自己一步步实践总结得来的，有写得不好的地方，还希望大家多包涵。 拿出积极的态度：首先需要自己努力去解决问题，其次包括调动资源、团队协助，最好要让领导/同事知晓你的情况。 摆烂并不可取，劳动是用来换取报酬的。如果平时的工作表现/绩效过不去，那么将可能会直接影响到我们的收入。 对于工作量的估算要留有余地。学会平衡别人的预期，如果 3 天内能完成，那最好回复一个星期：总有别的事情会打扰你。 遵守基本的职业道德：如果你负责的服务出现故障或发现一个严重的 bug ，作为开发我们应该马上对其进行修复/解决。 工作里对事不对人：我们需要推进的是项目，是一件件具体的事情，聚焦于如何解决问题，而不是和人做无效拉扯。 做好上面这些，假如是对于刚毕业的我来说可能并不容易，但过了两年自己开始负责项目了以后，我就能慢慢理解并适应了。 所以我们可以有意识地提炼日常工作成果，形成自己的个人源码库和工具库、解决某类问题的通用系统体系结构、甚至进化为框架。 众所周知对软件开发人员而言，有、无经验的一个显著区别是：新手无论完成什么开发任务都从头开始，而有经验的开发往往通过重组自己的可复用模块、类库来解决问题。 这并不是说所有可复用的东西都必须自己实现，别人成熟的、通过测试的成果也可以收集、整理到自己的知识库中。 实际程序员们可能很难做到：总有一些会议拉上你去 battle，总有一些线上的 bug 困扰着你，总会有领导安排的其它事情让你分身乏力...还有很多。工作是为了更好的生活这个主旨不会变，所以我也有自己的一些想法： 知识体系是系统化的知识，具有连续性的特点，反映了一个人经过学习后所拥有的知识构成情况与结合方式。如果一个开发人员在1、2年内都没有更新过自己的知识，那么他可能已经不再属于这个行业了。 抽象思维能够超越感官直接感知信息，通过对事物的本质属性和规律的把握，使人们在认识活动中获得更深层次的知识。 多阅读优秀的代码，多思考本质和原因，多通过实践来检验 写出可阅读的、简洁易懂、可维护的代码，应该是程序员在工作中都需要追求的。在实现业务的时候，通过借助模仿优秀的开源实现，学习他人在提高代码效率、逻辑思维、处理高并发等方面是怎么做的。 学会总结复盘，从中抽取有价值的经验，将成长记录下来 复盘的内容，可以是一次线上 bug 的处理过程，可以是一次代码重构优化的过程，也可以是对一个中间件的新认识等等。复盘不是问责也不是甩锅，而是聚焦于好和不好，知其然再知其所以然，同时避免重蹈覆辙。 不是所有人都愿意直面自己的缺点和不足，我自己最开始也不明白复盘有什么用处，觉得对领导和业务能有交代就行。但随着自己做过的项目多了起来，受到身边优秀同事的影响也在日益增多，我才明白复盘是可以让人迅速成长的不二法宝。 有意识地站在更高的角度去看待问题，可能会得到不一样的答案 作为一线开发我们更多关注的，可能是接口的参数和实现逻辑，数据库字段的设计，中间件的使用，异常的处理等实际编码问题； 而你的 TeamLeader 可能关注的是项目的高可用，处理高并发的性能，硬件资源的成本，服务挂了怎么降低对业务的影响等； 部门的总监则可能在考虑这个项目要不要做二期三期？对业务方有什么价值？能带来多少新用户？让谁来负责后续的运营等。 初/中级可以先从广度出发，多见识一些主流技术或者解决方案，这样在面对业务的时候可以更从容地进行设计，也能更清楚自己 CURD 的目的是什么。 高级/资深可能就要精通某一或者某几方面的通用架构/解决方案了，同时对于负责的业务有深刻的认识，可能也要负责团队的建设，从更广阔的视角去思考问题。 我是在工作一年半左右慢慢有了对未来进行职业规划的想法，因为时间会向前走，人也会渐渐地成长。对于未来想要什么，可以慢慢给自己多找找方向去尝试。 2010-2020 这十年，互联网发展如火如荼，各种公司如雨后春笋般冒出来，那时候招聘需求量大，互联网就业市场一片欣欣向荣。 而现在没有了当年的辉煌，很多互联网公司业务发展停滞，同时经过前面多年的发展，产品架构已经成熟稳定，不再需要这么多的程序员，势必要进行结构优化，那优化的对象自然是性价比低的人群。 其次，IT互联网行业高薪的宣传又吸引了一大批年轻人跑步进场，需求变少的情况下人又变多，内卷就这么产生了。高学历的人才多了很多，所谓学历贬值的论调在这几年一直都有出现。 让我们透过现象看本质：IT互联网行业的特殊性在于产出是可迭代的。这个行业程序员的产出就是软件&代码，而这个东西就是可以不断的迭代的。 分析了上面的原因，就有针对性了。软件代码这些东西是可迭代的，但有些东西是没法复制和迭代的：像是沟通协调能力、解决问题能力、规划组织能力、管理能力和人脉资源等等。这些其实就是我们经常说的软实力，这些能力才是越久越吃香。 这些能力不仅可以在日常工作中有意识的积累，也可以通过考取相关的证书来掌握理论知识，比如：软考高项（信息系统项目管理师、系统架构师）、PMP认证、阿里云/华为云高级认证等。 打开认知的局限，不只着眼于手里掌握的技术，拓展自己职业的综合能力，明白不是只有靠技术才能活下去，才能更好地“破局”。 文章最后，我还有一些想说的。的确，技术逐渐会变得越来越“廉价”，不再是必须要工作多年的大佬才能驾驭的高级东西。 这里面有个很重要的原因，就是开源的力量。大家可以想一想自己参与的一些项目，用到了多少开源的东西？以一个典型的后端项目为例，可能涉及到了 MySQL、Redis、ElasticSearch、RocketMQ、Nginx、SprintBoot、Jenkins、CI/CD、Docker...... 程序员要做的就是根据业务需求，把上面那一堆组件按照规则合理地堆在一起。一般就是 Nginx 负载均衡，Spring Boot/Cloud 写后端业务逻辑，Redis 做缓存，MySQL、ES、MongoDB 做数据存储，OSS/COS 做云对象存储，RocketMQ/Kafka 做消息队列，最后进行 CI/CD 操作塞到 Docker 里面部署运行。现在绝大多数的功能，开源组件都能提供，再加上现在是云的时代，各种服务都上云了，那么问题就更简单了。 既然我们身处在这个行业，那么就要做好持续学习的准备，不断打怪升级，锻炼综合能力，做到让经验随着年龄的增长成真正的正比。,https://www.cnblogs.com/CodeBlogMan/p/17983370,2025-03-13 10:56:54
救园行动——赞助商计划,未知作者,在求救的同时，我们也在想办法奋力自救，会采取一系列的救园行动，这一次一定要把园子救下来，因为没有下一次了。 有人可能会笑话这个救园计划是一个幼稚的想法，没有直接的收益，哪个企业会愿意花钱支持一个困境中的技术社区。 但我们还是想试一试，万一找到呢，哪怕只找到一家，也会帮助加快救园，也会让大家见证的确有这样非同寻常的企业。,https://www.cnblogs.com/cmt/p/18333440,2025-03-13 10:56:54
